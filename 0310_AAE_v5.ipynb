{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data가 기존에 1.3GB => 400MB 까지 축소 가능\n",
    "\n",
    "* [pandas handling](https://www.dataquest.io/blog/pandas-big-data/)\n",
    "\n",
    "* Adversarial AutoEncoder 적용!!\n",
    "* target에 따라서 latent space 를 잘 학습시킬수 있게하기.\n",
    "* 시각화 하는 부분이나 2차원 맵핑해서 보여주는 것 만들어야 함.\n",
    "* zdim 이 2차원으로 하다보니 잘 안된다 그래서 20차원으로 줄이고 또 이것을 2차원으로 줄인다면?...\n",
    "    * 이게 사실 의미 있는 행위 인지는 모르겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import numpy as np , os , pandas as pd , pandas_profiling\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint , seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from moving_free_batch_normalization import moving_free_batch_norm\n",
    "from stochastic_weight_averaging import StochasticWeightAveraging\n",
    "\n",
    "from math import sin,cos,sqrt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./credit44_sc.csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fac_var = []\n",
    "num_var = []\n",
    "for i in list(data) : \n",
    "    if data[i].nunique() < 15 : \n",
    "        print(\"factor variable : {}, count : {} \".format(i, data[i].nunique()))\n",
    "        fac_var.append(i)\n",
    "        if data[i].nunique() == 2 :\n",
    "            print(\"binary\" , i)\n",
    "    else : \n",
    "        print(\"연속형 variable : {} , count : {} \".format(i, data[i].nunique()))\n",
    "        num_var.append(i)\n",
    "        \n",
    "data = data.loc[:,fac_var + num_var]\n",
    "\n",
    "col = list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fac_var = list(set(fac_var) - set([\"target\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_usage(pandas_obj):\n",
    "    if isinstance(pandas_obj,pd.DataFrame):\n",
    "        usage_b = pandas_obj.memory_usage(deep=True).sum()\n",
    "    else: # we assume if not a df it's a series\n",
    "        usage_b = pandas_obj.memory_usage(deep=True)\n",
    "    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n",
    "    return \"{:03.2f} MB\".format(usage_mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_usage(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dtype in ['float','int','object']:\n",
    "    selected_dtype = data.select_dtypes(include=[dtype])\n",
    "    mean_usage_b = selected_dtype.memory_usage(deep=True).mean()\n",
    "    mean_usage_mb = mean_usage_b / 1024 ** 2\n",
    "    print(\"Average memory usage for {} columns: {:03.2f} MB\".format(dtype,mean_usage_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to be calculating memory usage a lot,\n",
    "# so we'll create a function to save us some time!\n",
    "\n",
    "data_int = data.select_dtypes(include=['int'])\n",
    "converted_int = data_int.apply(pd.to_numeric,downcast='unsigned')\n",
    "\n",
    "print(mem_usage(data_int))\n",
    "print(mem_usage(converted_int))\n",
    "\n",
    "compare_ints = pd.concat([data_int.dtypes,converted_int.dtypes],axis=1)\n",
    "compare_ints.columns = ['before','after']\n",
    "compare_ints.apply(pd.Series.value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_float = data.select_dtypes(include=['float'])\n",
    "converted_float = data_float.apply(pd.to_numeric,downcast='float')\n",
    "\n",
    "print(mem_usage(data_float))\n",
    "print(mem_usage(converted_float))\n",
    "\n",
    "compare_floats = pd.concat([data_float.dtypes,converted_float.dtypes],axis=1)\n",
    "compare_floats.columns = ['before','after']\n",
    "compare_floats.apply(pd.Series.value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_obj = data.select_dtypes(include=['object']).copy()\n",
    "\n",
    "converted_obj = pd.DataFrame()\n",
    "\n",
    "for col in gl_obj.columns:\n",
    "    num_unique_values = len(gl_obj[col].unique())\n",
    "    num_total_values = len(gl_obj[col])\n",
    "    if num_unique_values / num_total_values < 0.5:\n",
    "        converted_obj.loc[:,col] = gl_obj[col].astype('category')\n",
    "    else:\n",
    "        converted_obj.loc[:,col] = gl_obj[col]\n",
    "        \n",
    "        \n",
    "print(mem_usage(gl_obj))\n",
    "print(mem_usage(converted_obj))\n",
    "\n",
    "compare_obj = pd.concat([gl_obj.dtypes,converted_obj.dtypes],axis=1)\n",
    "compare_obj.columns = ['before','after']\n",
    "compare_obj.apply(pd.Series.value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimized_gl = data.copy()\n",
    "\n",
    "data[converted_int.columns] = converted_int\n",
    "data[converted_float.columns] = converted_float\n",
    "data[converted_obj.columns] = converted_obj\n",
    "\n",
    "print(\"Change Memory size : {}\".format(mem_usage(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "msno.heatmap( data , figsize=(20,20))\n",
    "plt.savefig(\"./IITP_EDA.png\")\n",
    "msno.dendrogram(data , figsize = (15,15))\n",
    "plt.savefig(\"./IITP_EDA_Dandro.png\")\n",
    "msno.matrix(data)\n",
    "plt.savefig(\"./IITP_MISSING_MATRIX.png\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_label = data.target\n",
    "data = data.drop(\"target\" , axis = 1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_label = pd.get_dummies(data_label , drop_first= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_label = data_label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col =list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "z_id_ = np.random.randint(0, 2, size=[batch_size])\n",
    "\n",
    "def gaussian_mixture(batch_size, n_dim=2, n_labels=2, x_var=0.5, y_var=0.1, label_indices=None):\n",
    "    if n_dim != 2:\n",
    "        raise Exception(\"n_dim must be 2.\")\n",
    "\n",
    "    def sample(x, y, label, n_labels):\n",
    "        shift = 1.4\n",
    "        r = 2.0 * np.pi / float(n_labels) * float(label)\n",
    "        new_x = x * cos(r) - y * sin(r)\n",
    "        new_y = x * sin(r) + y * cos(r)\n",
    "        new_x += shift * cos(r)\n",
    "        new_y += shift * sin(r)\n",
    "        return np.array([new_x, new_y]).reshape((2,))\n",
    "\n",
    "    x = np.random.normal(0, x_var, (batch_size, (int)(n_dim/2)))\n",
    "    y = np.random.normal(0, y_var, (batch_size, (int)(n_dim/2)))\n",
    "    z = np.empty((batch_size, n_dim), dtype=np.float32)\n",
    "    for batch in range(batch_size):\n",
    "        for zi in range((int)(n_dim/2)):\n",
    "            if label_indices is not None:\n",
    "                z[batch, zi*2:zi*2+2] = sample(x[batch, zi], y[batch, zi], label_indices[batch], n_labels)\n",
    "            else:\n",
    "                z[batch, zi*2:zi*2+2] = sample(x[batch, zi], y[batch, zi], np.random.randint(0, n_labels), n_labels)\n",
    "\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = gaussian_mixture(batch_size , n_labels = 2 , label_indices= z_id_)\n",
    "plt.scatter(value[:, 0], value[:, 1], c = z_id_ , s=40, cmap='viridis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform(batch_size, n_dim, n_labels=10, minv=-1, maxv=1, label_indices=None):\n",
    "    if label_indices is not None:\n",
    "\n",
    "        def sample(label, n_labels):\n",
    "            num = int(np.ceil(np.sqrt(n_labels)))\n",
    "            size = (maxv-minv)*1.0/num\n",
    "            x, y = np.random.uniform(-size/2, size/2, (2,))\n",
    "            i = label / num\n",
    "            j = label % num\n",
    "            x += j*size+minv+0.5*size\n",
    "            #y += i*size+minv+0.5*size\n",
    "            return np.array([x, y]).reshape((2,))\n",
    "\n",
    "        z = np.empty((batch_size, n_dim), dtype=np.float32)\n",
    "        for batch in range(batch_size):\n",
    "            for zi in range((int)(n_dim/2)):\n",
    "                    z[batch, zi*2:zi*2+2] = sample(label_indices[batch], n_labels)\n",
    "    else:\n",
    "        z = np.random.uniform(minv, maxv, (batch_size, n_dim)).astype(np.float32)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = uniform(batch_size , n_dim =2 ,  n_labels = 2 , label_indices= z_id_)\n",
    "plt.scatter(value[:, 0], value[:, 1], c = z_id_ , s=40, cmap='viridis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "z_id_one_hot_vector = np.zeros((batch_size, 2))\n",
    "z_id_one_hot_vector[np.arange(batch_size), z_id_] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mb_size = 1500\n",
    "X_dim = np.shape(data)[1]\n",
    "label_n = data_label.shape[1]\n",
    "z_dim = 2\n",
    "lr = 0.0001\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.9\n",
    "d_steps = 5\n",
    "g_steps = 3\n",
    "EPOCHS = 50000\n",
    "Margin = 14\n",
    "Margin_LIMIT = Margin - 1 \n",
    "ROW , COL = 7,7\n",
    "data_len = np.shape(data)[0]\n",
    "batch_iter = int(data_len / mb_size)\n",
    "LAMDA = 10 \n",
    "Recon_weight = 2\n",
    "\n",
    "seed_n = np.random.randint(1, 1000, size=1 )[0]\n",
    "\n",
    "tf.random.set_random_seed(seed_n)\n",
    "\n",
    "\n",
    "remark = \"SAMPLE_Z를 Uniform 에서 Gaussian으로 변경? 근데 Uniform으로 하면 더 차이를 볼 수 있지 않을까?\"\n",
    "title = \"0310_AAE_v5\"\n",
    "\n",
    "path_v = title\n",
    "\n",
    "loss_func = \"rasgan_gp\"\n",
    "\n",
    "model_dir = \"./Model_Save/{}\".format(path_v)\n",
    "vis_dir =  model_dir + \"/vis\"\n",
    "\n",
    "if not os.path.exists(model_dir) :\n",
    "    os.makedirs(model_dir)\n",
    "    os.makedirs(vis_dir)\n",
    "\n",
    "if loss_func == \"ragan_gp\" :\n",
    "    Gradient_type = [\"GP\",\"LP\"][np.random.randint(0, 2, size=1 )[0]]\n",
    "else :\n",
    "    Gradient_type = \"None\" \n",
    "    \n",
    "DATA_RANGE = (0,1)\n",
    "\n",
    "\n",
    "select1 = np.random.randint(0, 2, size=1 )[0]\n",
    "select2 = np.random.randint(0, 2, size=1 )[0]\n",
    "\n",
    "relu_w_init = [tf.keras.initializers.he_uniform(seed = seed_n) ,\n",
    "               tf.keras.initializers.he_normal(seed = seed_n)][select1]\n",
    "tanh_w_init = [tf.keras.initializers.glorot_normal(seed = seed_n) ,\n",
    "               tf.keras.initializers.glorot_uniform(seed = seed_n)][select2]\n",
    "\n",
    "\n",
    "\n",
    "relu_w_say = [\"he uniform\" , \"he normal\"][select1]\n",
    "\n",
    "\n",
    "if DATA_RANGE == (0,1) :\n",
    "    g_act = tf.nn.sigmoid\n",
    "    tanh_w_say = \"None\"\n",
    "if DATA_RANGE == (-1,1) :\n",
    "    g_act = tf.nn.tanh\n",
    "    tanh_w_say = [\"glorot normal\" , \"glorot uniform\"][select2]\n",
    "\n",
    "\n",
    "spec_iter = np.random.randint(1,4, size = 1)[0]\n",
    "\n",
    "P_layer_count = np.random.randint(5, 7, size=1 )[0]\n",
    "Q_layer_count = np.random.randint(5, 7, size=1 )[0]\n",
    "D_layer_count = P_layer_count \n",
    "\n",
    "P_layer = list(np.sort(np.random.randint(5 , z_dim + label_n + 10, size=P_layer_count ))[::-1]) ## Discriminator\n",
    "D_layer = list(np.sort(np.random.randint(z_dim + 2 , X_dim - 2 , size=D_layer_count ))) ## Decoder\n",
    "Q_layer = list(np.sort(np.random.randint(z_dim + 2 , 55 , size=Q_layer_count ))[::-1]) ## Encoder\n",
    "    \n",
    "P_H = [ z_dim + label_n ] + P_layer + [ 1 ]\n",
    "D_H = [ z_dim ] + D_layer + [ X_dim ]\n",
    "Q_H = [ X_dim ] + Q_layer + [ z_dim ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=[None, X_dim], name=\"X\")\n",
    "Z = tf.placeholder(tf.float32, shape=[None, z_dim], name=\"z\")\n",
    "Z_ID = tf.placeholder(tf.float32, shape=[None, label_n], name='prior_sample_label')\n",
    "X_ID = tf.placeholder(tf.float32, shape=[None, label_n], name='target_label')\n",
    "Z_IN = tf.placeholder(tf.float32, shape=[None, z_dim], name='latent_variable')\n",
    "\n",
    "batch_size = tf.placeholder(tf.int64)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, Z , Z_ID , X_ID, Z_IN )).batch(batch_size ,drop_remainder= True ).repeat().shuffle(1000)\n",
    "\n",
    "#is_training_bn = tf.placeholder(shape=[], dtype=tf.bool)\n",
    "#use_moving_statistics = tf.placeholder(shape=[], dtype=tf.bool)\n",
    "learning_rate = tf.placeholder(shape=[], dtype=tf.float32)\n",
    "\n",
    "\n",
    "iter = dataset.make_initializable_iterator()\n",
    "x , z , z_id , x_id , z_in = iter.get_next()\n",
    "print(\"=\"*30)\n",
    "print(\"Title : {}\".format(title))\n",
    "print(\"Loss func : {}\".format(loss_func))\n",
    "print(\"Model Directory : {}\".format(model_dir))\n",
    "print(\"Gradient Type : {} , Lambda : {}\".format(Gradient_type , LAMDA ))\n",
    "print(\"Data RANGE : {} , G activation : {}\".format(DATA_RANGE , g_act))\n",
    "print(\"Q Hidden : {}\".format(P_H))\n",
    "print(\"D Hidden : {}\".format(D_H))\n",
    "print(\"H Hidden : {}\".format(Q_H))\n",
    "print(\"Dim : {} , Z dim : {}, Margin : {}\".format(np.shape(data) , z_dim , Margin))\n",
    "print(\"Epoch : {} , Batch SIZE : {} , Batch Iter : {}\".format(EPOCHS , mb_size , batch_iter))\n",
    "print(\" Beta1 : {} , Beta2 : {}\".format( beta_1 , beta_2))\n",
    "print(\"D Step : {} , G Step : {}\".format(d_steps , g_steps))\n",
    "print(\"=\"*30)\n",
    "\n",
    "inner = [[remark , title , loss_func , model_dir , Gradient_type , LAMDA , spec_iter , \n",
    "          Recon_weight , seed_n , DATA_RANGE , g_act , P_H  , D_H , relu_w_say , tanh_w_say , \n",
    " np.shape(data) , z_dim , Margin , EPOCHS , mb_size , batch_iter , beta_1 , beta_2, d_steps , g_steps]]\n",
    "\n",
    "colname = [\"Remark : \" ,  \"Title : \" , \"Loss function : \" , \"Model 경로 : \" , \"Gradient 타입 : \" , \"LAMDA : \" ,  \"spec iter\" , \n",
    "           \"Reconstrunction weight : \" ,  \"seed\" ,\n",
    "           'G 생성 범위 : ' , 'G 생성 함수 : ' ,\"G Hidden SIZE : \" , \"D Hidden SIZE : \" , \n",
    "           \"Relu W init\" , \"Tanh W init\" , \n",
    "           \"Data Shape : \" , \" Z dim : \" ,\n",
    "           \"Margin : \", \"Epochs : \" , \"Batch size  : \" , \n",
    "           \" Batch Iteration : \" , \"Beta 1 : \" , \"Beta 2 : \" ,\n",
    "           \"D steps : \" , \"G steps  : \" ]\n",
    "\n",
    "pd.DataFrame( inner , columns = colname).T.to_csv(model_dir + \"/summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z(m, n_dim , n_labels=2, minv=-1, maxv=1, label_indices= None ):\n",
    "    if label_indices is not None:\n",
    "\n",
    "        def sample(label, n_labels):\n",
    "            num = int(np.ceil(np.sqrt(n_labels)))\n",
    "            size = (maxv-minv)*1.0/num\n",
    "            x, y = np.random.uniform(-size/2, size/2, (2,))\n",
    "            i = label / num\n",
    "            j = label % num\n",
    "            x += j*size+minv+0.5*size\n",
    "            #y += i*size+minv+0.5*size\n",
    "            return np.array([x, y]).reshape((2,))\n",
    "\n",
    "        z = np.empty((m, n_dim), dtype=np.float32)\n",
    "        for batch in range(m):\n",
    "            for zi in range((int)(n_dim/2)):\n",
    "                    z[batch, zi*2:zi*2+2] = sample(label_indices[batch], n_labels)\n",
    "    else:\n",
    "        z = np.random.uniform(minv, maxv, (m, n_dim)).astype(np.float32)\n",
    "    return z\n",
    "\n",
    "\n",
    "def sample_z(batch_size, n_dim=2, n_labels=2, x_var=0.5, y_var=0.1, label_indices=None):\n",
    "\n",
    "    def sample(x, y, label, n_labels):\n",
    "        shift = 1.4\n",
    "        r = 2.0 * np.pi / float(n_labels) * float(label)\n",
    "        new_x = x * cos(r) - y * sin(r)\n",
    "        new_y = x * sin(r) + y * cos(r)\n",
    "        new_x += shift * cos(r)\n",
    "        new_y += shift * sin(r)\n",
    "        return np.array([new_x, new_y]).reshape((2,))\n",
    "\n",
    "    x = np.random.normal(0, x_var, (batch_size, (int)(n_dim/2)))\n",
    "    y = np.random.normal(0, y_var, (batch_size, (int)(n_dim/2)))\n",
    "    z = np.empty((batch_size, n_dim), dtype=np.float32)\n",
    "    for batch in range(batch_size):\n",
    "        for zi in range((int)(n_dim/2)):\n",
    "            if label_indices is not None:\n",
    "                z[batch, zi*2:zi*2+2] = sample(x[batch, zi], y[batch, zi], label_indices[batch], n_labels)\n",
    "            else:\n",
    "                z[batch, zi*2:zi*2+2] = sample(x[batch, zi], y[batch, zi], np.random.randint(0, n_labels), n_labels)\n",
    "\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(prev = None , shape1 = None , shape2 = None , \n",
    "          name = None , activation = tf.nn.leaky_relu ,\n",
    "          final = False , SN = True , Type = None) :\n",
    "    \n",
    "    if activation == tf.nn.leaky_relu or activation == tf.nn.relu : \n",
    "        init = relu_w_init\n",
    "    \n",
    "    elif activation == tf.nn.tanh :\n",
    "        init = tanh_w_init\n",
    "    \n",
    "    W1 = tf.get_variable(\"Weight\" + str(name) , shape = [shape1 , shape2] , dtype = tf.float32 ,\n",
    "                        initializer = init)\n",
    "    b1 = tf.get_variable(\"Bias\" + str(name) , shape = [shape2] , dtype = tf.float32 ,\n",
    "                        initializer = tf.constant_initializer(0.001))\n",
    "    W2 = spectral_norm(W1 , name = \"SN\" + str(name))\n",
    "    if final == True :\n",
    "        layer = tf.matmul( prev , W1 )\n",
    "        layer = activation(layer)\n",
    "    else :\n",
    "        if SN == True :\n",
    "            layer = tf.matmul( prev , W2) + b1\n",
    "            layer = activation(layer)\n",
    "        else : \n",
    "            layer = tf.matmul( prev , W1) + b1\n",
    "            if Type == \"SWA\" :\n",
    "                layer = moving_free_batch_norm(layer, axis=-1, training=is_training_bn,\n",
    "                                               use_moving_statistics=use_moving_statistics, momentum=0.99)\n",
    "                layer = activation(layer)\n",
    "            elif Type == \"Self_Normal\" :\n",
    "                layer = activation(layer)\n",
    "                layer = tf.contrib.nn.alpha_dropout(layer , 0.5)\n",
    "            elif Type == \"Instance_Norm\" :\n",
    "                layer = tf.contrib.layers.instance_norm(layer)\n",
    "                layer = activation(layer)\n",
    "            else :\n",
    "                ## Nothing\n",
    "                layer = layer\n",
    "    return layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "MLP ENCODER\n",
    "\"\"\"     \n",
    "def Q(X = None , reuse = tf.AUTO_REUSE) :\n",
    "    \n",
    "    with tf.variable_scope(\"Q\" , reuse = reuse) as scope: \n",
    "        h = layer( prev = X , shape1 = Q_H[0], shape2 = Q_H[1] ,\n",
    "                  name = \"Q_first\", SN = False ,Type = \"N\" )\n",
    "        for i in range( len(Q_H) ) :\n",
    "            if (i + 2) >= (len(Q_H) -1 ) :\n",
    "                continue\n",
    "            else :\n",
    "                h = layer(prev= h , shape1 = Q_H[i+1], shape2 = Q_H[i+2] ,\n",
    "                          name = \"Q\" + str(i) , SN = False ,Type = \"N\")\n",
    "        Out = layer(prev = h , shape1 = Q_H[-2] , shape2 = Q_H[-1] ,\n",
    "                   name = \"Encoder_final\", final = True)\n",
    "        \n",
    "        return Out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Discriminator\n",
    "\"\"\"  \n",
    "\n",
    "def P(Z = None , reuse = tf.AUTO_REUSE) :\n",
    "    with tf.variable_scope(\"P\" , reuse = reuse) as scope: \n",
    "        h = layer(prev = Z , shape1 = P_H[0], shape2 = P_H[1] ,\n",
    "                  name = \"P_first\", SN = True ,  Type = \"N\")\n",
    "        for i in range( len(P_H) ) :\n",
    "            if (i + 2) >= (len(P_H) -1 ) :\n",
    "                continue\n",
    "            else :\n",
    "                h = layer(prev= h , shape1 = P_H[i+1], shape2 = P_H[i+2] ,\n",
    "                          name = \"P\" + str(i) , SN = True, Type = \"N\")\n",
    "        Out = layer(prev = h , shape1 = P_H[-2] , shape2 = P_H[-1] ,\n",
    "                   name = \"W_P_final\", final = True)\n",
    "        \n",
    "        if g_act == tf.nn.sigmoid :\n",
    "            G_final_b = tf.get_variable(\"Discriminator_Bias_final\" , shape = [P_H[-1]],\n",
    "                                       dtype = tf.float32 , initializer = tf.constant_initializer(0.001))\n",
    "            Out  = Out + G_final_b\n",
    "            prob = g_act(Out)\n",
    "        else :\n",
    "            prob = g_act( Out )\n",
    "        return prob , Out \n",
    "      \n",
    "\"\"\"\n",
    "MLP DECODER 나중에 확률 값 내뱉기  (0~1로 만들기)\n",
    "\"\"\"      \n",
    "def D(Z = None , reuse = tf.AUTO_REUSE) :\n",
    "    with tf.variable_scope(\"D\" , reuse = reuse) as scope: \n",
    "        h = layer(prev = Z , shape1 = D_H[0], shape2 = D_H[1] ,\n",
    "                  name = \"D_first\", SN = False ,Type = \"N\")\n",
    "        for i in range( len(D_H) ) :\n",
    "            if (i + 2) >= (len(D_H) -1 ) :\n",
    "                continue\n",
    "            else :\n",
    "                h = layer(prev= h , shape1 = D_H[i+1], shape2 = D_H[i+2] ,\n",
    "                          name = \"D\" + str(i) , SN = False, Type = \"N\")\n",
    "        Out = layer(prev = h , shape1 = D_H[-2] , shape2 = D_H[-1] ,\n",
    "                   name = \"W_D_final\", final = True)\n",
    "        \n",
    "        if g_act == tf.nn.sigmoid :\n",
    "            G_final_b = tf.get_variable(\"Decoder_Bias_final\" , shape = [D_H[-1]],\n",
    "                                       dtype = tf.float32 , initializer = tf.constant_initializer(0.001))\n",
    "            Out  = Out \n",
    "            #+ G_final_b\n",
    "            prob = g_act(Out)\n",
    "        else :\n",
    "            prob = g_act( Out )\n",
    "            \n",
    "        return prob\n",
    "\n",
    "    \n",
    "def Discriminator_loss( Ra = None, loss_func = None, real = None, fake = None) :\n",
    "    loss = 0\n",
    "    if Ra :\n",
    "        fake_logit = (fake - tf.reduce_mean(real))\n",
    "        real_logit = (real - tf.reduce_mean(fake))\n",
    "        \n",
    "        if loss_func == \"ralsgan\" :\n",
    "            fake_loss = tf.reduce_mean( tf.square( fake_logit + 1 ))\n",
    "            real_loss = tf.reduce_mean( tf.square( real_logit - 1 ))\n",
    "            loss = real_loss + fake_loss\n",
    "        if loss_func == \"rasgan_gp\" or loss_func == \"rasgan\" :\n",
    "            real_logit = tf.sigmoid( real_logit )\n",
    "            fake_logit = tf.sigmoid( fake_logit )\n",
    "            loss = - tf.reduce_mean( log(real_logit)) - tf.reduce_mean( log(1-fake_logit ))\n",
    "        if loss_func == \"rsgan_gp\" or loss_func == \"rsgan\" :\n",
    "            loss = - tf.reduce_mean( log(tf.nn.sigmoid( real - fake)))\n",
    "        if loss_func == \"rahinge\" :\n",
    "            fake_loss = tf.reduce_mean( tf.nn.relu( 1 + fake_logit ))\n",
    "            real_loss = tf.reduce_mean( tf.nn.relu( 1 - real_logit ))\n",
    "            loss = real_loss + fake_loss\n",
    "        if loss_func == \"Boundary\" : \n",
    "            #real_logit = tf.sigmoid( real - fake)\n",
    "            #fake_logit = tf.sigmoid( fake - real )\n",
    "            real_logit = tf.sigmoid( real_logit )\n",
    "            fake_logit = tf.sigmoid( fake_logit )\n",
    "            loss  =  -tf.reduce_mean(log( real_logit ) + log( 1 - fake_logit ))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def Generator_loss(Ra  = None , loss_func  = None , real = None ,fake = None ) :\n",
    "    loss = 0\n",
    "    if Ra :\n",
    "        fake_logit = (fake - tf.reduce_mean(real))\n",
    "        real_logit = (real - tf.reduce_mean(fake))\n",
    "        \n",
    "        if loss_func == \"ralsgan\" :\n",
    "            fake_loss = tf.reduce_mean( tf.square( fake_logit - 1 ))\n",
    "            real_loss = tf.reduce_mean( tf.square( real_logit + 1 ))\n",
    "            loss = real_loss + fake_loss\n",
    "        if loss_func == \"rasgan_gp\" or loss_func == \"rasgan\" :\n",
    "            real_logit = tf.sigmoid( real_logit )\n",
    "            fake_logit = tf.sigmoid( fake_logit )\n",
    "            loss = - tf.reduce_mean( log( fake_logit )) - tf.reduce_mean( log(1- real_logit ))\n",
    "        if loss_func == \"rsgan_gp\" or loss_func == \"rsgan\" :\n",
    "            loss = - tf.reduce_mean( log(tf.nn.sigmoid( fake - real)))\n",
    "        if loss_func == \"rahinge\" :\n",
    "            fake_loss = tf.reduce_mean( tf.nn.relu( 1 - fake_logit ))\n",
    "            real_loss = tf.reduce_mean( tf.nn.relu( 1 + real_logit ))\n",
    "            loss = real_loss + fake_loss\n",
    "        if loss_func == \"Boundary\" : \n",
    "            fake_logit = tf.sigmoid( fake_logit )\n",
    "            #fake_logit = tf.sigmoid( fake - real  )\n",
    "            loss  =  0.5 * tf.reduce_mean( ( log( fake_logit ) - log( 1 - fake_logit ) )**2 )\n",
    "    return loss\n",
    "\n",
    "    \n",
    "\n",
    "# def sample_z(m, n) :\n",
    "#     output = np.random.normal(loc = 0 ,scale= 1 , size=[m, n])\n",
    "#     #output = np.random.uniform(-1., 1., size=[m, n])\n",
    "#     return output\n",
    "\n",
    "def log(x):\n",
    "    return tf.log( tf.maximum( x , 1e-8) )\n",
    "\n",
    "\n",
    "\n",
    "def spectral_norm(w, iteration= spec_iter , name = None):\n",
    "    w_shape = w.shape.as_list()\n",
    "    w = tf.reshape(w, [-1, w_shape[-1]])\n",
    "    \n",
    "    u = tf.get_variable(name , [1, w_shape[-1]], initializer=tf.random_normal_initializer(), trainable=False)\n",
    "\n",
    "    u_hat = u\n",
    "    v_hat = None\n",
    "    for i in range(iteration):\n",
    "       \n",
    "        \"\"\"\n",
    "       power iteration\n",
    "       Usually iteration = 1 will be enough\n",
    "       \"\"\"\n",
    "        \n",
    "        v_ = tf.matmul(u_hat, tf.transpose(w))\n",
    "        v_hat = tf.nn.l2_normalize(v_)\n",
    "\n",
    "        u_ = tf.matmul(v_hat, w)\n",
    "        u_hat = tf.nn.l2_normalize(u_)\n",
    "\n",
    "    u_hat = tf.stop_gradient(u_hat)\n",
    "    v_hat = tf.stop_gradient(v_hat)\n",
    "\n",
    "    sigma = tf.matmul(tf.matmul(v_hat, w), tf.transpose(u_hat))\n",
    "\n",
    "    with tf.control_dependencies([u.assign(u_hat)]):\n",
    "        w_norm = w / sigma\n",
    "        w_norm = tf.reshape(w_norm, w_shape)\n",
    "\n",
    "    return w_norm \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_sample                  = Q(x) ## ENCODER\n",
    "X_samples                 = D(z_sample) ## DECODER\n",
    "\n",
    "z_real = tf.concat([z, z_id],1)\n",
    "z_fake = tf.concat([z_sample, x_id],1)\n",
    "D_fake, D_fake_logits     = P(z_fake) ## DISCRIMI\n",
    "D_real, D_real_logits     = P(z_real)\n",
    "\n",
    "\n",
    "Test_Samples = D( z_in )\n",
    "\"\"\"\n",
    "z : encoder 값 -> 이것이 fake 가 되야 함\n",
    "z_sample :  임의의 분포 값 f\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "recon_loss = tf.reduce_mean(tf.reduce_sum(tf.squared_difference(x , X_samples ) , axis = 1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#D_loss = -tf.reduce_mean( log(D_real) +  log(1. - D_fake))\n",
    "#G_loss = -tf.reduce_mean( log(D_fake) ) \n",
    "\n",
    "e = tf.random_uniform([batch_size , 1] , 0 , 1)\n",
    "x_hat = e * z_real + (1-e) * z_fake\n",
    "grad  = tf.gradients( P(x_hat), x_hat)[0]\n",
    "grad_norm = tf.norm( tf.layers.flatten(grad) , axis = 1)\n",
    "\n",
    "if Gradient_type == \"GP\" :\n",
    "    GP = LAMDA * tf.reduce_mean( tf.square( grad_norm - 1.0))\n",
    "elif Gradient_type == \"LP\" :    \n",
    "    GP = LAMDA * tf.reduce_mean( tf.square( tf.maximum(0.0 ,grad_norm - 1.0)))\n",
    "else :\n",
    "    GP = 0\n",
    "    \n",
    "\n",
    "D_loss = Discriminator_loss(True , \"Boundary\" , D_real_logits , D_fake_logits) + GP\n",
    "\n",
    "# D_loss_real = tf.reduce_mean(\n",
    "#         tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, labels=tf.ones_like(D_real_logits)))\n",
    "# D_loss_fake = tf.reduce_mean(\n",
    "#     tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.zeros_like(D_fake_logits)))\n",
    "# D_loss = D_loss_real+D_loss_fake + GP\n",
    "\n",
    "    \n",
    "G_loss = Generator_loss(True , \"Boundary\" , D_real_logits , D_fake_logits)\n",
    "\n",
    "## 함수가 점점 감소하는 것은 볼 수 있지만 결과가 좋아지는 것을 볼 수가 없다. \n",
    "\n",
    "# G_loss = tf.reduce_mean(\n",
    "#     tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits,\n",
    "#                                             labels=tf.ones_like(D_fake_logits)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print(GP)\n",
    "# D_loss = Discriminator_loss(True , loss_func , D_real , D_fake) + GP\n",
    "# G_loss = Generator_loss(True , loss_func , D_real , D_fake)\n",
    "\n",
    "# tf.contrib.framework.sort()\n",
    "#gene_17 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_sample), 17))\n",
    "#true_17 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(x), 17))\n",
    "# gene_17 = gene_17 / tf.reduce_sum(gene_17)\n",
    "# true_17 = true_17 / tf.reduce_sum(true_17)\n",
    "\n",
    "\n",
    "# def kl_divergence(p, q): \n",
    "#     return tf.reduce_sum(p * log( p/q  ))\n",
    "\n",
    "\n",
    "\n",
    "# recon_loss_17 = kl_divergence(true_17 , gene_17 )\n",
    "\n",
    "# recon_loss = Recon_weight * recon_loss_17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_vars = tf.trainable_variables()\n",
    "\n",
    "d_vars = [var for var in t_vars if \"P\" in var.name]\n",
    "g_vars = [var for var in t_vars if \"D\" in var.name]\n",
    "recon_vars = [var for var in t_vars if \"D\" or \"Q\" in var.name]\n",
    "#pp.pprint(g_vars)\n",
    "#print(\"\\n\")\n",
    "#pp.pprint(d_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "#update_bn_ops = tf.get_collection('UPDATE_BN_OPS')\n",
    "#reset_bn_ops = tf.get_collection('RESET_BN_OPS')\n",
    "#update_ops = tf.group(*update_ops)\n",
    "#update_bn_ops = tf.group(*update_bn_ops)\n",
    "#reset_bn_ops = tf.group(*reset_bn_ops)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "    G_solver = tf.train.AdamOptimizer(learning_rate=learning_rate , beta1=beta_1 , beta2 = beta_2).minimize(G_loss, var_list=g_vars)\n",
    "    D_solver = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta_1 , beta2 = beta_2).minimize(D_loss, var_list=d_vars)\n",
    "    AE_solver = tf.train.AdamOptimizer().minimize(recon_loss, var_list=recon_vars )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.name_scope('SWA'):\n",
    "#     swa = StochasticWeightAveraging()\n",
    "#     swa_op = swa.apply(var_list=g_vars)\n",
    "#     # Make backup variables\n",
    "#     with tf.variable_scope('BackupVariables'):\n",
    "#         backup_vars = [tf.get_variable(var.op.name, dtype=var.value().dtype, trainable=False,\n",
    "#                                        initializer=var.initialized_value())\n",
    "#                        for var in g_vars ]\n",
    "#     # operation to assign SWA weights to model\n",
    "#     swa_to_weights = tf.group(*(tf.assign(var, swa.average(var).read_value()) for var in g_vars))\n",
    "#     # operation to store model into backup variables\n",
    "#     save_weight_backups = tf.group(*(tf.assign(bck, var.read_value()) for var, bck in zip(g_vars, backup_vars)))\n",
    "#     # operation to get back values from backup variables to model\n",
    "#     restore_weight_backups = tf.group(*(tf.assign(var, bck.read_value()) for var, bck in zip(g_vars, backup_vars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def sample_data(data = None , n= len(data)) : \n",
    "    output = data[list(np.random.choice(len(data) , n))]\n",
    "    \n",
    "    return output\n",
    "\n",
    "scaler = MinMaxScaler(feature_range= DATA_RANGE )\n",
    "\n",
    "x_plot = pd.DataFrame(sample_data(data = data.values , n=mb_size) , columns = col)\n",
    "data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Check_KS(ks_output , Total_ks_graph , Margin , ROW , COL ) : \n",
    "    clear_output(wait= True)\n",
    "    print(\"KS Plot\")\n",
    "    total = sess.run(Test_Samples )\n",
    "    total = scaler.inverse_transform(total)\n",
    "    g_plot = pd.DataFrame(total , columns = col )\n",
    "    g_plot[fac_var] = g_plot[fac_var].round(0)\n",
    "    Value = []\n",
    "    for label in col :\n",
    "        sample = g_plot[label]\n",
    "        real   = x_plot[label]\n",
    "        ks , p = stats.ks_2samp(real.values , sample.values)\n",
    "        Value.append(ks)\n",
    "    Total_KS = round( np.sum(Value) , 2)\n",
    "    ks_2 = [iteration] + Value\n",
    "    ks_3 = pd.DataFrame([ks_2], columns = [\"iter\"] + col)\n",
    "    ks_output = ks_output.append(ks_3)\n",
    "    fig , ax = plt.subplots(figsize=(26,13))\n",
    "    fig.subplots_adjust(top = 0.95 , left = 0.03 , bottom = 0.05 , right = 0.99)\n",
    "    updown = 0\n",
    "    for name in col : \n",
    "        if updown % 2 == 0 :\n",
    "            param , space=\"bottom\" , \"  \"\n",
    "        else : \n",
    "            param , space =\"top\" , \"   \"\n",
    "        ax.plot(ks_output.iter , ks_output[[name]], label = name)\n",
    "        ax.text(iteration , ks_output.loc[ks_output[\"iter\"]==iteration , [name]].values , space + name ,\n",
    "                verticalalignment = param)\n",
    "        updown +=1\n",
    "    ax.set_title(\"KS [{}]\".format(Total_KS) , fontsize = 30 )\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"KS\")    \n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0 + box.height * 0.1, box.width, box.height * 0.9])\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True, shadow=True, ncol=15 , fontsize= 10)\n",
    "    ax.text(iteration+1 , 0.05 , \"   0.05\", verticalalignment = param)\n",
    "    ax.axhline(0.05, linewidth=4, color='r')\n",
    "    ax.set_title(\"[{}] , EPOCH : {} , KS : {}[{}][{}]\".format(title , iteration-1 , Total_KS , Total_ks_graph.ks.min() ,len(col) ) , fontsize = 30)\n",
    "    plt.savefig(model_dir +\"/IND_KS_Log_{}.png\".format(path_v))\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    Total Graph\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Total KS Plot\")\n",
    "    Total_ks_graph_ap = pd.DataFrame({\"iter\":[iteration] , \"ks\" :[Total_KS]})\n",
    "    Total_ks_graph    = Total_ks_graph.append(Total_ks_graph_ap)\n",
    "    #Total_ks_graph    = Total_ks_graph[Total_ks_graph.iter>0]\n",
    "    fig , ax = plt.subplots(figsize = (26,13))\n",
    "    fig.subplots_adjust(top = 0.95 , left = 0.03 , bottom = 0.04 , right = 0.99)\n",
    "    ax.plot(Total_ks_graph.iter , Total_ks_graph.ks , linestyle =\"-\" , marker =\".\" , linewidth = 3, markersize = 12)\n",
    "    ax.axhline(0.05, linewidth=4, color='r')\n",
    "    ax.set_title(\"[{}] , EPOCH : {} , KS : {}[{}][{}]\".format(title , iteration-1 , Total_KS , Total_ks_graph.ks.min() ,len(col) ) , fontsize = 30)\n",
    "    plt.savefig(model_dir +\"/Total_KS_Log{}.png\".format(path_v))\n",
    "    plt.show()\n",
    "    \n",
    "    fig , ax = plt.subplots(figsize = (26,13))\n",
    "    fig.subplots_adjust(top = 0.95 , left = 0.03 , bottom = 0.04 , right = 0.99)\n",
    "\n",
    "    ax.plot(output.iter , output.dloss , label =\"dloss\" , linestyle =\"-\" , marker =\".\" , linewidth = 4, markersize = 12)\n",
    "    ax.plot(output.iter , output.gloss , label =\"gloss\" , linestyle =\"-\" , marker =\".\" , linewidth = 4, markersize = 12)\n",
    "    ax.set_title(\"[{}] , EPOCH : {} , Dloss : {} , Gloss : {}\".format(title , iteration-1 ,  dloss, gloss ), fontsize= 30)\n",
    "    #ax.set_ylim(-5, 15)\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0 + box.height * 0.1, box.width, box.height * 0.9])\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True, shadow=True, ncol=4 , fontsize= 20)\n",
    "    plt.savefig(model_dir +\"/LOSS_Log_{}.png\".format(path_v))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    if Margin > Total_KS : \n",
    "        show_plot(row = ROW , ncol= COL )\n",
    "        print(\"=======UPDate============\")\n",
    "        with open(model_dir + \"/Change_Margin.txt\", \"a\") as f:\n",
    "            f.write(\"Epoch : {} , Margin : {} ===> {} \\n\".format(iteration , Margin , Total_KS))\n",
    "        Margin = Total_KS\n",
    "        ## 여기선 특정 샘플 저장해야 하므로. 원하는 개수 만큼.\n",
    "        generate = {}\n",
    "        sample_n = 50000\n",
    "        z_id_ = np.random.randint(0, 2, size=[sample_n])\n",
    "        z_id_one_hot_vector = np.zeros((sample_n, 2))\n",
    "        z_id_one_hot_vector[np.arange(sample_n), z_id_] = 1\n",
    "        z_batch = sample_z(sample_n , z_dim , label_indices= z_id_)\n",
    "        generate[z_in] = z_batch\n",
    "        total = sess.run(Test_Samples, feed_dict= generate )\n",
    "        total = scaler.inverse_transform(total)\n",
    "        g_plot = pd.DataFrame(total , columns = col )\n",
    "        g_plot[fac_var] = g_plot[fac_var].round(0)\n",
    "        g_plot.to_csv(model_dir + \"/Generated_{}.csv\".format(path_v) , index = False)\n",
    "        \n",
    "        saver.save(sess , model_dir + \"/MODEL_{}\".format(path_v))\n",
    "    return ks_output , Total_ks_graph , Margin , Total_KS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def show_plot(row , ncol ) : \n",
    "    \n",
    "    fig , axes = plt.subplots(row , ncol , figsize = (26,13))\n",
    "    fig.subplots_adjust(hspace = 0.35 , wspace= 0.14 , top = 0.92 , left = 0.03 , bottom = 0.04 , right = 0.99)\n",
    "    total = sess.run( Test_Samples )\n",
    "    try : \n",
    "        total = total[~np.isnan(total).any(axis=1)]\n",
    "        total = scaler.inverse_transform(total)\n",
    "        g_plot = pd.DataFrame(total , columns = col )\n",
    "        g_plot[fac_var] = g_plot[fac_var].round(0)\n",
    "        \"\"\"\n",
    "        좀 더 쉬운 분포로 만들어서 학습시킨 후 다시 원래값으로 (factor 변수이기 때문에 가능하다 생각함.)\n",
    "        \"\"\"\n",
    "\n",
    "        col2 = 0\n",
    "        error = []\n",
    "        for j in range(row) :\n",
    "            for k in range(ncol) :\n",
    "                try :\n",
    "                    label = col[col2]\n",
    "                    sample = g_plot.loc[: , label]\n",
    "                    sample.name = \"Gene\"\n",
    "                    real_0 = x_plot.loc[: , label]\n",
    "                    real_0.name =\"Real\"\n",
    "                    ks , p = stats.ks_2samp(real_0.values , sample.values)\n",
    "                    error.append(ks)\n",
    "                    col2 += 1\n",
    "                    if label in fac_var : \n",
    "                        sns.distplot( sample , ax=axes[j , k], norm_hist =True , kde=False , hist_kws ={\"color\":\"r\" , \"label\" :\"Gene\", \"rwidth\":0.75})\n",
    "                        sns.distplot(real_0 , ax=axes[j , k],norm_hist =True, kde=False , hist_kws ={\"color\":\"g\" , \"label\" :\"Real\", \"rwidth\":0.75})\n",
    "                        axes[j , k].legend(fontsize = 10)\n",
    "                    elif label in num_var : \n",
    "                        sns.distplot(  sample , ax=axes[j , k] ,\n",
    "                                     kde_kws={\"color\": \"r\", \"lw\": 2, \"label\": \"Gene\" , \"shade\" : True} , hist =False , rug = False) #   \n",
    "                        sns.distplot(  real_0 , ax=axes[j , k] ,\n",
    "                                     kde_kws={\"color\": \"g\", \"lw\": 2, \"label\": \"Real\", \"shade\" : True } , hist =False , rug = False) # \n",
    "                        axes[j , k].legend(fontsize = 10 )\n",
    "                    axes[j , k].set_title( label , loc =\"center\" , fontsize= 10 )\n",
    "                    axes[j , k].set_xlabel(' ')\n",
    "                except IndexError as e : \n",
    "                    axes[j , k].axis(\"off\")\n",
    "        \n",
    "        KS_DIF = round(np.sum(error),2)\n",
    "        plt.suptitle('[{}] EPOCH {} , D_loss : {} , G_loss : {} KS : {}'.format(title , i , dloss,gloss,KS_DIF) , fontsize= 30)\n",
    "        if KS_DIF < 9 : \n",
    "            plt.savefig(vis_dir +\"/Vis_{}_{}.png\".format(path_v,KS_DIF))\n",
    "        else : \n",
    "            plt.savefig(model_dir +\"/Vis_{}.png\".format(path_v))\n",
    "        plt.show()\n",
    "        \n",
    "        return print(\"시각화\")\n",
    "\n",
    "    except Exception as e : \n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_BEFORE_SWA = 500\n",
    "ALPHA1_LR = 0.0009\n",
    "ALPHA2_LR = 0.000001\n",
    "\n",
    "def get_learning_rate(step, epoch, steps_per_epoch):\n",
    "    if epoch < EPOCHS_BEFORE_SWA:\n",
    "        return ALPHA1_LR\n",
    "\n",
    "    if step > int(0.9 * EPOCHS * steps_per_epoch):\n",
    "        return ALPHA2_LR\n",
    "\n",
    "    length_slope = int(0.9 * EPOCHS * steps_per_epoch) - EPOCHS_BEFORE_SWA * steps_per_epoch\n",
    "    return ALPHA1_LR - ((ALPHA1_LR - ALPHA2_LR) / length_slope) * (step - EPOCHS_BEFORE_SWA * steps_per_epoch)\n",
    "\n",
    "steps_per_epoch_train = EPOCHS// mb_size\n",
    "print(steps_per_epoch_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_steps = []\n",
    "all_lr = []\n",
    "\n",
    "step = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    for _ in range(steps_per_epoch_train):\n",
    "        all_steps.append(step)\n",
    "        all_lr.append(get_learning_rate(step, epoch, steps_per_epoch_train))\n",
    "        step += 1 \n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(all_steps, all_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from math import ceil\n",
    "\n",
    "# steps_per_epoch_train = int(ceil(EPOCHS/mb_size))\n",
    "# def fit_bn_statistics():\n",
    "#     sess.run(reset_bn_ops)\n",
    "    \n",
    "#     feed_dict = {is_training_bn: True, use_moving_statistics: True}\n",
    "#     for _ in range(steps_per_epoch_train):\n",
    "#         sess.run(update_bn_ops, feed_dict=feed_dict)\n",
    "\n",
    "# def inference(with_moving_statistics=True):\n",
    "#     feed_dict = {is_training_bn: False,\n",
    "#                  use_moving_statistics: with_moving_statistics}\n",
    "#     g_loss = []\n",
    "#     d_loss = []\n",
    "#     nb_steps = 10\n",
    "    \n",
    "#     for _ in range(nb_steps):\n",
    "#         _, Dloss = sess.run([D_solver, D_loss], feed_dict=feed_dict_train)\n",
    "#         _, Gloss = sess.run([G_solver, G_loss], feed_dict=feed_dict_train)\n",
    "#         g_loss.append(Gloss)\n",
    "#         d_loss.append(Dloss)\n",
    "    \n",
    "#     return np.mean(g_loss), np.mean(d_loss)\n",
    "\n",
    "# feed_dict_train = {is_training_bn: True, \n",
    "#                    use_moving_statistics:True,}\n",
    "\n",
    "feed_dict_train = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_cmap(N, base_cmap=None):\n",
    "    \"\"\"Create an N-bin discrete colormap from the specified input map\"\"\"\n",
    "\n",
    "    # Note that if base_cmap is a string or None, you can simply do\n",
    "    #    return plt.cm.get_cmap(base_cmap, N)\n",
    "    # The following works for string, None, or a colormap instance:\n",
    "\n",
    "    base = plt.cm.get_cmap(base_cmap)\n",
    "    color_list = base(np.linspace(0, 1, N))\n",
    "    cmap_name = base.name + str(N)\n",
    "    return base.from_list(cmap_name, color_list, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"START\")\n",
    "config=tf.ConfigProto( log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config = config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "ks_output = pd.DataFrame([[0]+[1.]*len(col)], columns = [\"iter\"]+col)\n",
    "\n",
    "iteration , dloss ,  gloss = 0 , 0, 0\n",
    "output = pd.DataFrame({\"iter\" : [iteration] , \"dloss\" : [dloss] , \"gloss\" : [gloss] })\n",
    "ks_init = len(col)\n",
    "Total_ks_graph = pd.DataFrame({\"iter\":[iteration] , \"ks\" :[ks_init]})\n",
    "\n",
    "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "\n",
    "\n",
    "step = 0\n",
    "\n",
    "for i in range(EPOCHS) : \n",
    "    z_id_ = np.random.randint(0, 2, size=[data_len])\n",
    "    z_id_one_hot_vector = np.zeros((data_len, 2))\n",
    "    z_id_one_hot_vector[np.arange(data_len), z_id_] = 1\n",
    "    z_batch = sample_z(data_len , z_dim , label_indices= z_id_)\n",
    "    sess.run(iter.initializer, feed_dict={ X : data , Z : z_batch  ,\n",
    "                                          batch_size: mb_size ,\n",
    "                                          Z_ID : z_id_one_hot_vector, X_ID : data_label , Z_IN : z_batch })\n",
    "    iteration +=1\n",
    "    dloss = []\n",
    "    gloss = []\n",
    "    for _ in range( batch_iter)  :\n",
    "        feed_dict_train[learning_rate] = get_learning_rate(step, i , steps_per_epoch_train)\n",
    "        feed_dict_train[batch_size] = mb_size\n",
    "        step += 1\n",
    "        for _ in range(5) :\n",
    "            _, Dloss = sess.run([D_solver, D_loss], feed_dict=feed_dict_train)\n",
    "            dloss.append(Dloss)\n",
    "        \n",
    "        for _ in range(2) :\n",
    "            _ , Gloss = sess.run([G_solver, G_loss], feed_dict=feed_dict_train)\n",
    "            gloss.append(Gloss)\n",
    "            _, recon = sess.run([AE_solver, recon_loss], feed_dict=feed_dict_train )\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    dloss = np.mean(dloss)\n",
    "    gloss = np.mean(gloss)\n",
    "    \n",
    "    \n",
    "#     sess.run(swa_op)\n",
    "#     sess.run(save_weight_backups)\n",
    "#     sess.run(swa_to_weights)\n",
    "#     sess.run(restore_weight_backups)\n",
    "    \n",
    "#     fit_bn_statistics()\n",
    "#     dloss, gloss = inference(with_moving_statistics=False)\n",
    "    \n",
    "    output1 = pd.DataFrame({\"iter\" : [iteration] , \"dloss\" : [dloss] , \"gloss\" : [gloss] })\n",
    "    output  = output.append(output1)\n",
    "    output  = output[output.iter > 0 ]\n",
    "    dloss = round( np.float64(dloss),4)\n",
    "    gloss = round( np.float64(gloss),4)\n",
    "    ks_output , Total_ks_graph , Margin  , Total_KS = Check_KS(ks_output , Total_ks_graph , Margin , ROW , COL)\n",
    "    print('EPOCH : {}; D_loss: {}; G_loss: {} , KS : {}'.format(i, dloss, gloss , Total_KS))\n",
    "    print(\"Recon Loss : {}\".format(recon))\n",
    "    \n",
    "    \n",
    "    \n",
    "    import random\n",
    "#     idx = np.random.choice(data_len, 1000 , replace=False )\n",
    "#     sample_manifold_label = data_label[idx]\n",
    "#     sample_manifold       = data[idx]\n",
    "    z_PMLR = sess.run(z_sample , feed_dict = {x : data})\n",
    "    if z_dim == 2 :\n",
    "        z_PMLR = z_PMLR\n",
    "    else :\n",
    "        z_PMLR = TSNE(n_components=2, perplexity=15, learning_rate=0.01 , n_jobs=10).fit_transform(z_PMLR)\n",
    "        \n",
    "    N = 2 \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(z_PMLR[:, 0], z_PMLR[:, 1], c=np.argmax(data_label, 1),\n",
    "                marker='o', edgecolor='none', cmap=discrete_cmap(N, 'jet'))\n",
    "    plt.colorbar(ticks=range(N))\n",
    "    axes = plt.gca()\n",
    "    #axes.set_xlim([-1, 1])\n",
    "    #axes.set_ylim([-1, 1])\n",
    "    plt.grid(True)\n",
    "    plt.savefig(vis_dir +\"/Manifold.png\")\n",
    "    plt.show()\n",
    "\n",
    "        \n",
    "    if (i % 10 == 0) &  (Margin < Margin_LIMIT ) :\n",
    "        clear_output(wait= True)\n",
    "        show_plot(row = ROW , ncol= COL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
