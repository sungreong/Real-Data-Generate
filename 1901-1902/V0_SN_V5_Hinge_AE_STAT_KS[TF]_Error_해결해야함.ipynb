{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 변종2\n",
    "\n",
    "* KS통계량 값을 가중치로 학습시키기 \n",
    "* 1  컬럼수만큼이 좋을지 \n",
    "* 2  1이라는 수가 좋을지 \n",
    "* magan 처럼 boundary 를 주는 방식으로\n",
    "\n",
    "## > 학습이 잘 안된다.....\n",
    "## > Hinge loss로 변경하기\n",
    "## SNGAN https://github.com/isr-wang/SNGAN/blob/master/generator.py\n",
    "\n",
    "# 必必必必必必必\n",
    "## KS 통계량을 Tensorflow 로 구현을 해서 minimize하는 형식으로 설정해둠\n",
    "### 과연 잘 될지.\n",
    "> 먼가 구현이 안되는 상태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import kurtosis\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "from tensorflow.contrib.distributions import percentile as tf_percent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "else:\n",
    "    print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./credit44_sc.csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연속형 variable : SCORE_ORG , count : 395 \n",
      "연속형 variable : SCORE_NK0200_000 , count : 445 \n",
      "연속형 variable : SCORE_RK0400_700 , count : 294 \n",
      "factor variable : CNT_CONTACT_POS, count : 8 \n",
      "factor variable : CNT_ENG, count : 7 \n",
      "factor variable : DAYS_CONTACT_POS, count : 11 \n",
      "factor variable : DAYS_CALL_PAYMENT, count : 11 \n",
      "factor variable : EWS_C_N_P27000100, count : 14 \n",
      "연속형 variable : EWS_A_K_D1M232000_OPR , count : 2384 \n",
      "factor variable : EWS_C_N_P42000200, count : 14 \n",
      "연속형 variable : EWS_C_N_P32003000 , count : 31 \n",
      "연속형 variable : AGE , count : 54 \n",
      "연속형 variable : CNT_Contact , count : 29 \n",
      "연속형 variable : EWS_A_K_D1M23200C_OPR , count : 2384 \n",
      "연속형 variable : EWS_C_K_D10310000_OPR , count : 52 \n",
      "연속형 variable : EWS_A_K_D90232200_OPR , count : 2490 \n",
      "연속형 variable : D_N_CA0000603 , count : 4894 \n",
      "연속형 variable : EWS_C_N_P32002600 , count : 22 \n",
      "연속형 variable : EWS_D_N_P43004000 , count : 399 \n",
      "factor variable : C_N_PS0001777, count : 11 \n",
      "factor variable : A_K_D10220000_OPR, count : 9 \n",
      "연속형 variable : SCORE_RK0400_000 , count : 424 \n",
      "연속형 variable : D_K_D10310000_OPR , count : 52 \n",
      "연속형 variable : D_K_L2Z000034 , count : 400 \n",
      "연속형 variable : D_K_L20283000 , count : 402 \n",
      "연속형 variable : EWS_D_K_L20283000 , count : 402 \n",
      "연속형 variable : EWS_D_N_P43004500 , count : 344 \n",
      "연속형 variable : D_K_L2Z000035 , count : 402 \n",
      "연속형 variable : EWS_A_K_D10231000_OPR , count : 2778 \n",
      "factor variable : DAYS_CONTACT, count : 7 \n",
      "연속형 variable : EWS_D_K_L2Z000035 , count : 402 \n",
      "연속형 variable : SC0000059 , count : 448 \n",
      "연속형 variable : D_N_L24003800 , count : 401 \n",
      "factor variable : MOB, count : 14 \n",
      "연속형 variable : SC0000063 , count : 445 \n",
      "연속형 variable : EWS_A_K_D10232000_OPR , count : 3270 \n",
      "연속형 variable : TF_N_CRT000021 , count : 5156 \n",
      "factor variable : EWS_C_K_D10220000_OPR, count : 9 \n",
      "연속형 variable : SC0000055 , count : 376 \n",
      "연속형 variable : SC0000049 , count : 308 \n",
      "factor variable : EWS_C_K_D10210D00_OPR, count : 11 \n",
      "연속형 variable : D_N_P21010500 , count : 170 \n",
      "연속형 variable : EWS_C_K_D10110000_OPR , count : 53 \n",
      "연속형 variable : EWS_A_N_L22002000 , count : 6369 \n",
      "factor variable : target, count : 2 \n",
      "binary target\n"
     ]
    }
   ],
   "source": [
    "fac_var = []\n",
    "num_var = []\n",
    "for i in list(data) : \n",
    "    if data[i].nunique() < 15 : \n",
    "        print(\"factor variable : {}, count : {} \".format(i, data[i].nunique()))\n",
    "        fac_var.append(i)\n",
    "        if data[i].nunique() == 2 :\n",
    "            print(\"binary\" , i)\n",
    "    else : \n",
    "        print(\"연속형 variable : {} , count : {} \".format(i, data[i].nunique()))\n",
    "        num_var.append(i)\n",
    "        \n",
    "data = data.loc[:,fac_var + num_var]\n",
    "\n",
    "col = list(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "def mse_mean( real , fake ) : \n",
    "    first = log(real) - log(tf.keras.backend.mean(fake , axis = 0 ))\n",
    "    second = tf.square(first)\n",
    "    third = tf.reduce_mean(second)\n",
    "    return third\n",
    "\n",
    "def mse_std( real , fake ) : \n",
    "    first = log(real) -  log(tf.keras.backend.std(fake , axis = 0 ))\n",
    "    second = tf.square(first)\n",
    "    third = tf.reduce_mean(second)\n",
    "    return third\n",
    "\n",
    "def percentile( real , fake , q=25.0) : \n",
    "    first  = log(real) - log(tf.expand_dims(tf_percent(fake , axis = 0 , q= q) , axis = 0))\n",
    "    second = tf.square(first)\n",
    "    third  = tf.reduce_mean(second)\n",
    "    return third\n",
    "\n",
    "\n",
    "def log(x):\n",
    "    return tf.log( tf.maximum(x , 1e-8) )\n",
    "\n",
    "\n",
    "def spectral_norm(w, iteration=1 , name = None):\n",
    "    w_shape = w.shape.as_list()\n",
    "    w = tf.reshape(w, [-1, w_shape[-1]])\n",
    "    \n",
    "    u = tf.get_variable(name , [1, w_shape[-1]], initializer=tf.random_normal_initializer(), trainable=False)\n",
    "\n",
    "    u_hat = u\n",
    "    v_hat = None\n",
    "    for i in range(iteration):\n",
    "       \n",
    "        \"\"\"\n",
    "       power iteration\n",
    "       Usually iteration = 1 will be enough\n",
    "       \"\"\"\n",
    "        \n",
    "        v_ = tf.matmul(u_hat, tf.transpose(w))\n",
    "        v_hat = tf.nn.l2_normalize(v_)\n",
    "\n",
    "        u_ = tf.matmul(v_hat, w)\n",
    "        u_hat = tf.nn.l2_normalize(u_)\n",
    "\n",
    "    u_hat = tf.stop_gradient(u_hat)\n",
    "    v_hat = tf.stop_gradient(v_hat)\n",
    "\n",
    "    sigma = tf.matmul(tf.matmul(v_hat, w), tf.transpose(u_hat))\n",
    "\n",
    "    with tf.control_dependencies([u.assign(u_hat)]):\n",
    "        w_norm = w / sigma\n",
    "        w_norm = tf.reshape(w_norm, w_shape)\n",
    "\n",
    "    return w_norm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =====================\n",
    "# Tuning Path , Hidden , etc\n",
    "# ====================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_size = 5000\n",
    "X_dim = np.shape(data)[1]\n",
    "z_dim = 64\n",
    "h_dim = 128\n",
    "lr = 1e-5\n",
    "d_steps = 3\n",
    "path_v = \"V5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mb_size = 32\n",
    "X_dim = np.shape(data)[1]\n",
    "z_dim = 64\n",
    "h_dim = 128\n",
    "lr = 1e-3\n",
    "d_steps = 3\n",
    "\n",
    "나쁘지 않게 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=[None, X_dim], name=\"X\")\n",
    "z = tf.placeholder(tf.float32, shape=[None, z_dim], name=\"z\")\n",
    "\n",
    "m = tf.placeholder(tf.float32 , name  = \"KS\")\n",
    "\n",
    "\n",
    "Hint_mean = tf.placeholder(tf.float32 , [None , X_dim] , name = \"mean\")\n",
    "Hint_std = tf.placeholder(tf.float32 , [None , X_dim] , name = \"std\")\n",
    "\n",
    "Hint_q25 = tf.placeholder(tf.float32 , [None , X_dim] , name = \"q25\")\n",
    "Hint_q75 = tf.placeholder(tf.float32 , [None , X_dim] , name = \"q75\")\n",
    "\n",
    "\n",
    "\n",
    "def sample_z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "\n",
    "device = ['/cpu:0' , '/cpu:0']\n",
    "\n",
    "\n",
    "def D(X,reuse=tf.AUTO_REUSE ):\n",
    "    with tf.device(device[0]) :\n",
    "        with tf.variable_scope( \"Discriminator\" ,reuse=reuse  ) as scope :\n",
    "            D_W1 = tf.get_variable(\"DW1\",[X_dim, h_dim], dtype=tf.float32)\n",
    "            D_b1 = tf.get_variable(\"DB1\", [h_dim], dtype=tf.float32 , initializer=tf.constant_initializer(0.0))\n",
    "            D_W2 = tf.get_variable(\"DW2\",[h_dim, h_dim/2], dtype=tf.float32)\n",
    "            D_b2 = tf.get_variable(\"DB2\",[h_dim/2], dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n",
    "            D_W3 = tf.get_variable(\"DW3\",[h_dim/2, h_dim/4], dtype=tf.float32)\n",
    "            D_b3 = tf.get_variable(\"DB3\",[h_dim/4], dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n",
    "            D_W4 = tf.get_variable(\"DW4\",[h_dim/4, h_dim/8], dtype=tf.float32)\n",
    "            D_b4 = tf.get_variable(\"DB4\",[h_dim/8], dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n",
    "            D_W5 = tf.get_variable(\"DW5\",[h_dim/8, 1], dtype=tf.float32)\n",
    "            D_b5 = tf.get_variable(\"DB5\",[1], dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n",
    "            \n",
    "            D_h1 = tf.nn.leaky_relu(tf.matmul(X, spectral_norm(D_W1 , name =\"sn1\")) + D_b1)\n",
    "            D_h2 = tf.nn.leaky_relu(tf.matmul(D_h1, spectral_norm(D_W2 , name =\"sn2\")) + D_b2)\n",
    "            D_h3 = tf.nn.leaky_relu(tf.matmul(D_h2, spectral_norm(D_W3 , name =\"sn3\")) + D_b3)\n",
    "            D_h4 = tf.nn.leaky_relu(tf.matmul(D_h3, spectral_norm(D_W4 , name =\"sn4\")) + D_b4)\n",
    "            out = tf.matmul(D_h4, D_W5) + D_b5\n",
    "        return out\n",
    "\n",
    "\n",
    "def G(X = None , Z = None , reuse = tf.AUTO_REUSE) : \n",
    "    with tf.device(device[1]) :\n",
    "        with tf.variable_scope(\"Generator\" , reuse = reuse) as scope: \n",
    "            G_W1 = tf.get_variable(\"GW1\",[z_dim, h_dim], dtype=tf.float32 , initializer=tf.contrib.layers.xavier_initializer())\n",
    "            G_b1 = tf.get_variable(\"GB1\",[h_dim],dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n",
    "            G_W2 = tf.get_variable(\"GW2\" ,[h_dim, h_dim/2], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "            G_b2 = tf.get_variable(\"GB2\",[h_dim/2], dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n",
    "            G_W3 = tf.get_variable(\"GW3\" ,[h_dim/2, h_dim/4], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "            G_b3 = tf.get_variable(\"GB3\",[h_dim/4], dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n",
    "            G_W4 = tf.get_variable(\"GW4\" ,[h_dim/4, X_dim], dtype=tf.float32)\n",
    "            G_b4 = tf.get_variable(\"GB4\",[X_dim], dtype=tf.float32, initializer=tf.constant_initializer(0.0))\n",
    "            \n",
    "            G_h1 = tf.matmul(Z, spectral_norm(G_W1 , name=\"Gsn1\") ) + G_b1\n",
    "            G_h1 = tf.layers.dropout(G_h1)\n",
    "            G_h1 = tf.nn.leaky_relu(G_h1)\n",
    "            G_h2 = tf.matmul(G_h1, spectral_norm(G_W2 , name=\"Gsn2\")) + G_b2\n",
    "            #G_h2 = tf.layers.dropout(G_h2)\n",
    "            G_h2 = tf.nn.leaky_relu(G_h2)\n",
    "            G_h3 = tf.matmul(G_h2, spectral_norm(G_W3, name=\"Gsn3\")) + G_b3\n",
    "            #G_h3 = tf.layers.dropout(G_h3)\n",
    "            G_h3 = tf.nn.leaky_relu(G_h3)\n",
    "            \n",
    "            G_log_prob = tf.matmul(G_h3, G_W4) # + G_b2 Bias는 일단 제외 \n",
    "            G_log_prob = tf.nn.sigmoid(G_log_prob)\n",
    "            \n",
    "#             one = tf.slice(G_log_prob , [0, 0] , [1,-1] )[0]\n",
    "#             two = tf.slice(G_log_prob , [0, 1] , [1,-1] )[0]\n",
    "#             thr = tf.slice(G_log_prob , [0, 2] , [1,-1] )[0]\n",
    "#             fou = tf.slice(G_log_prob , [0, 3] , [1,-1] )[0]\n",
    "#             fif = tf.slice(G_log_prob , [0, 4] , [1,-1] )[0]\n",
    "#             six = tf.slice(G_log_prob , [0, 5] , [1,-1] )[0]\n",
    "#             sev = tf.slice(G_log_prob , [0, 6] , [1,-1] )[0]\n",
    "#             eig = tf.slice(G_log_prob , [0, 7] , [1,-1] )[0]\n",
    "#             nin = tf.slice(G_log_prob , [0, 8] , [1,-1] )[0]\n",
    "#             ten = tf.slice(G_log_prob , [0, 9] , [1,-1] )[0]\n",
    "#             one1 = tf.slice(G_log_prob , [0, 10] , [1,-1] )[0]\n",
    "#             one2 = tf.slice(G_log_prob , [0, 11] , [1,-1] )[0]\n",
    "#             one3 = tf.slice(G_log_prob , [0, 12] , [1,-1] )[0]\n",
    "#             one4 = tf.slice(G_log_prob , [0, 13] , [1,-1] )[0]\n",
    "#             one5 = tf.slice(G_log_prob , [0, 14] , [1,-1] )[0]\n",
    "#             one6 = tf.slice(G_log_prob , [0, 15] , [1,-1] )[0]\n",
    "#             one7 = tf.slice(G_log_prob , [0, 16] , [1,-1] )[0]\n",
    "#             one8 = tf.slice(G_log_prob , [0, 17] , [1,-1] )[0]\n",
    "#             one9 = tf.slice(G_log_prob , [0, 18] , [1,-1] )[0]\n",
    "#             two0 = tf.slice(G_log_prob , [0, 19] , [1,-1] )[0]\n",
    "#             two1 = tf.slice(G_log_prob , [0, 20] , [1,-1] )[0]\n",
    "#             two2 = tf.slice(G_log_prob , [0, 21] , [1,-1] )[0]\n",
    "#             two3 = tf.slice(G_log_prob , [0, 22] , [1,-1] )[0]\n",
    "#             two4 = tf.slice(G_log_prob , [0, 23] , [1,-1] )[0]\n",
    "#             two5 = tf.slice(G_log_prob , [0, 24] , [1,-1] )[0]\n",
    "#             two6 = tf.slice(G_log_prob , [0, 25] , [1,-1] )[0]\n",
    "#             two7 = tf.slice(G_log_prob , [0, 26] , [1,-1] )[0]\n",
    "#             two8 = tf.slice(G_log_prob , [0, 27] , [1,-1] )[0]\n",
    "#             two9 = tf.slice(G_log_prob , [0, 28] , [1,-1] )[0]\n",
    "#             thr0 = tf.slice(G_log_prob , [0, 29] , [1,-1] )[0]\n",
    "#             thr1 = tf.slice(G_log_prob , [0, 30] , [1,-1] )[0]\n",
    "#             thr2 = tf.slice(G_log_prob , [0, 31] , [1,-1] )[0]\n",
    "#             thr3 = tf.slice(G_log_prob , [0, 32] , [1,-1] )[0]\n",
    "#             thr4 = tf.slice(G_log_prob , [0, 33] , [1,-1] )[0]\n",
    "#             thr5 = tf.slice(G_log_prob , [0, 34] , [1,-1] )[0]\n",
    "#             thr6 = tf.slice(G_log_prob , [0, 35] , [1,-1] )[0]\n",
    "#             thr7 = tf.slice(G_log_prob , [0, 36] , [1,-1] )[0]\n",
    "#             thr8 = tf.slice(G_log_prob , [0, 37] , [1,-1] )[0]\n",
    "#             thr9 = tf.slice(G_log_prob , [0, 38] , [1,-1] )[0]\n",
    "#             fou0 = tf.slice(G_log_prob , [0, 39] , [1,-1] )[0]\n",
    "#             fou1 = tf.slice(G_log_prob , [0, 40] , [1,-1] )[0]\n",
    "#             fou2 = tf.slice(G_log_prob , [0, 41] , [1,-1] )[0]\n",
    "#             fou3 = tf.slice(G_log_prob , [0, 42] , [1,-1] )[0]\n",
    "#             fou4 = tf.slice(G_log_prob , [0, 43] , [1,-1] )[0]\n",
    "#             fou5 = tf.slice(G_log_prob , [0, 44] , [1,-1] )[0]\n",
    "            \n",
    "            \n",
    "            one = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 0))\n",
    "            two = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 1))\n",
    "            thr = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 2))\n",
    "            fou = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 3))\n",
    "            fif = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 4))\n",
    "            six = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 5))\n",
    "            sev = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 6))\n",
    "            eig = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 7))\n",
    "            nin = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 8))\n",
    "            ten = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 9))\n",
    "            one1 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 10))\n",
    "            one2 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 11))\n",
    "            one3 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 12))\n",
    "            one4 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 13))\n",
    "            one5 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 14))\n",
    "            one6 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 15))\n",
    "            one7 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 16))\n",
    "            one8 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 17))\n",
    "            one9 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 18))\n",
    "            two0 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 19))\n",
    "            two1 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 20))\n",
    "            two2 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 21))\n",
    "            two3 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 22))\n",
    "            two4 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 23))\n",
    "            two5 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 24))\n",
    "            two6 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 25))\n",
    "            two7 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 26))\n",
    "            two8 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 27))\n",
    "            two9 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 28))\n",
    "            thr0 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 29))\n",
    "            thr1 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 30))\n",
    "            thr2 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 31))\n",
    "            thr3 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 32))\n",
    "            thr4 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 33))\n",
    "            thr5 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 34))\n",
    "            thr6 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 35))\n",
    "            thr7 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 36))\n",
    "            thr8 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 37))\n",
    "            thr9 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 38))\n",
    "            fou0 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 39))\n",
    "            fou1 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 40))\n",
    "            fou2 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 41))\n",
    "            fou3 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 42))\n",
    "            fou4 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 43))\n",
    "            fou5 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_log_prob), 44))\n",
    "            \n",
    "            \n",
    "            slice_collection = [one , two , thr , fou , fif , six , sev , eig ,nin , ten , \n",
    "                                one1 ,one2 ,one3 , one4 , one5 , one6 ,one7 , one8,one9 , \n",
    "                                two0 , two1 ,two2 ,two3 , two4 , two5 , two6 ,two7 , two8,two9 , \n",
    "                                thr0 , thr1 ,thr2 ,thr3 , thr4 , thr5 , thr6 ,thr7 , thr8,thr9 ,\n",
    "                                fou0 , fou1 , fou2 , fou3 , fou4 , fou5\n",
    "                               ]\n",
    "        return G_log_prob , slice_collection\n",
    "\n",
    "\n",
    "def gradient_penalty(real, fake, f ):\n",
    "    alpha = tf.random_uniform(\n",
    "        shape=[mb_size ,1], \n",
    "        minval=0.,\n",
    "        maxval=1.\n",
    "    )\n",
    "    differences = fake- real\n",
    "    interpolates = real + (alpha*differences)\n",
    "    gradients = tf.gradients(f(interpolates), [interpolates])[0]\n",
    "    slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1]))\n",
    "    gp = tf.reduce_mean((slopes-1.)**2)\n",
    "    return gp , gradients\n",
    "\n",
    "def ks_2samp_tf(G_sample , X , batch_size , index = 0 ):\n",
    "    \n",
    "    G_sample2 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_sample), index))\n",
    "    X2        = tf.transpose(tf.nn.embedding_lookup(tf.transpose(X), index))\n",
    "    G_sample2 = G_sample\n",
    "    X2        = tf.slice(X , [0, index] , [1,-1] )[0]\n",
    "    \n",
    "    n1        = batch_size\n",
    "    n2        = batch_size\n",
    "    data1     = tf.contrib.framework.sort(G_sample2, axis = 0 )\n",
    "    data2     = tf.contrib.framework.sort(X2 , axis = 0 )\n",
    "#     data1     =  tf.to_float( data1 ) \n",
    "#     data2     =  tf.to_float( data2 ) \n",
    "    data_all  = tf.concat([data1,data2] , axis = 0)\n",
    "    cdf1      = tf.searchsorted(data1,data_all,side='right' ) / ( mb_size)\n",
    "    cdf2      = tf.searchsorted(data2,data_all,side='right')/ ( mb_size)\n",
    "    d         = tf.reduce_max(tf.abs(cdf1-cdf2))\n",
    "    d         = tf.to_float(d)\n",
    "    return d\n",
    "\n",
    "def ks_2samp_tf(G_sample , X , batch_size , index = 0 ):\n",
    "    \n",
    "    G_sample2 = G_sample[index]\n",
    "    X2        = tf.transpose(tf.nn.embedding_lookup(tf.transpose(X), index))\n",
    "#     G_log_prob2 = G_log_prob\n",
    "#     X2        = tf.slice(X , [0, index] , [1,-1] )[0]\n",
    "    \n",
    "    n1        = batch_size\n",
    "    n2        = batch_size\n",
    "    data1     = tf.contrib.framework.sort(G_sample2, axis = 0 )\n",
    "    data2     = tf.contrib.framework.sort(X2 , axis = 0 )\n",
    "    data_all  = tf.concat([data1,data2] , axis = 0)\n",
    "    cdf1      = tf.searchsorted(data1,data_all,side='right' ) / ( mb_size)\n",
    "    cdf2      = tf.searchsorted(data2,data_all,side='right')/ ( mb_size)\n",
    "    d         = tf.reduce_max(tf.abs(cdf1-cdf2))\n",
    "    d         = tf.reduce_mean(tf.square(data1-data2))\n",
    "    d         = tf.to_float(d)\n",
    "    return d\n",
    "\n",
    "def sample_data(data = None , n= len(data)) : \n",
    "    output = data[list(np.random.choice(len(data) , n))]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  2  8  1 10]\n",
      "[0.  0.2 0.4 0.6 0.8 1.  0.2 1.  0.  1. ]\n",
      "[0.  0.  0.4 0.8 0.8 0.8 0.  0.4 0.  0.8]\n",
      "0.6\n"
     ]
    }
   ],
   "source": [
    "data1 = [1,2,3,4,5]\n",
    "data2 = [6,2,8,1,10]\n",
    "data_all = np.concatenate([data1 ,data2 ])\n",
    "\n",
    "print(data_all)\n",
    "cdf1 = np.searchsorted(data1 , data_all ) / len(data1)\n",
    "print(cdf1)\n",
    "cdf2 = np.searchsorted(data2 , data_all) / len(data2)\n",
    "print(cdf2)\n",
    "d         = np.max(np.abs(cdf1-cdf2)) \n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf , numpy as np\n",
    "# tf.reset_default_graph()\n",
    "# def loss_func(G , X , batch_size , index = 0 ):  \n",
    "#     G2 = G[index]\n",
    "#     X2        = tf.transpose(tf.nn.embedding_lookup(tf.transpose(X), index))    \n",
    "#     n1 ,n2    = batch_size , batch_size\n",
    "#     data1     = tf.contrib.framework.sort(G2, axis = 0 )\n",
    "#     data2     = tf.contrib.framework.sort(X2 , axis = 0 )\n",
    "#     data_all  = tf.concat([data1,data2] , axis = 0)\n",
    "#     cdf1      = tf.searchsorted(data1,data_all,side='right' ) / ( batch_size)\n",
    "#     cdf2      = tf.searchsorted(data2,data_all,side='right')/ ( batch_size)\n",
    "#     #d         = tf.reduce_max(tf.abs(cdf1-cdf2))\n",
    "#     d         = tf.reduce_mean(tf.square(data1-data2))\n",
    "#     return tf.to_float(d)\n",
    "\n",
    "# X_dim = 45\n",
    "# z_dim = 30\n",
    "# h_dim = 45\n",
    "# batch_size = 500\n",
    "# X = tf.placeholder(tf.float32, shape=[None, X_dim], name=\"X\")\n",
    "# z = tf.placeholder(tf.float32, shape=[None, z_dim], name=\"z\")\n",
    "# D_W1 = tf.get_variable(\"DW1\",[z_dim, h_dim], dtype=tf.float32)\n",
    "# slice_output = tf.matmul(z , D_W1)\n",
    "# print(slice_0)\n",
    "# slice_0 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(slice_output), 0))\n",
    "# slice_1 = tf.transpose(tf.nn.embedding_lookup(tf.transpose(slice_output), 1))\n",
    "# slice_output = [slice_0 , slice_1]\n",
    "\n",
    "# loss1 = loss_func(slice_output , X , batch_size , index = 0 )\n",
    "# loss2 = loss_func(slice_output , X , batch_size , index = 1 )\n",
    "# loss  = loss1 + loss2\n",
    "# # sess = tf.InteractiveSession()\n",
    "# X11 = np.random.normal(size= [1000,45])\n",
    "# Z11 = np.random.normal(size= [1000,30])\n",
    "# # H = np.random.normal(size= [1000,45])\n",
    "# # H =[tf.transpose(tf.nn.embedding_lookup(tf.transpose(H), 0))]\n",
    "# # index = 0\n",
    "# # mb_size = 500\n",
    "# # X11 = np.random.normal(size= [1000,45])\n",
    "# # H2 = loss_func(H ,X11 , mb_size , index)\n",
    "# # print(H2.eval())\n",
    "\n",
    "\n",
    "# opt = tf.train.GradientDescentOptimizer(0.0001).minimize(loss)\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     for i in range(10000):\n",
    "#         _ , lss = sess.run([opt, loss] , feed_dict={z: Z11 , X : X11})\n",
    "#         if i % 1000  == 0:\n",
    "#             print(lss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ks_2samp_tf_total(G_sample , X , mb_size ):\n",
    "    ks_tf0 = ks_2samp_tf(G_sample ,X , mb_size , 0)\n",
    "    ks_tf1 = ks_2samp_tf(G_sample ,X , mb_size , 1)\n",
    "    ks_tf2 = ks_2samp_tf(G_sample ,X , mb_size , 2)\n",
    "    ks_tf3 = ks_2samp_tf(G_sample ,X , mb_size , 3)\n",
    "    ks_tf4 = ks_2samp_tf(G_sample ,X , mb_size , 4)\n",
    "    ks_tf5 = ks_2samp_tf(G_sample ,X , mb_size , 5)\n",
    "    ks_tf6 = ks_2samp_tf(G_sample ,X , mb_size , 6)\n",
    "    ks_tf7 = ks_2samp_tf(G_sample ,X , mb_size , 7)\n",
    "    ks_tf8 = ks_2samp_tf(G_sample ,X , mb_size , 8)\n",
    "    ks_tf9 = ks_2samp_tf(G_sample ,X , mb_size , 9)\n",
    "    ks_tf10 = ks_2samp_tf(G_sample ,X , mb_size , 10)\n",
    "    ks_tf11 = ks_2samp_tf(G_sample ,X , mb_size , 11)\n",
    "    ks_tf12 = ks_2samp_tf(G_sample ,X , mb_size , 12)\n",
    "    ks_tf13 = ks_2samp_tf(G_sample ,X , mb_size , 13)\n",
    "    ks_tf14 = ks_2samp_tf(G_sample ,X , mb_size , 14)\n",
    "    ks_tf15 = ks_2samp_tf(G_sample ,X , mb_size , 15)\n",
    "    ks_tf16 = ks_2samp_tf(G_sample ,X , mb_size , 16)\n",
    "    ks_tf17 = ks_2samp_tf(G_sample ,X , mb_size , 17)\n",
    "    ks_tf18 = ks_2samp_tf(G_sample ,X , mb_size , 18)\n",
    "    ks_tf19 = ks_2samp_tf(G_sample ,X , mb_size , 19)\n",
    "    ks_tf20 = ks_2samp_tf(G_sample ,X , mb_size , 20)\n",
    "    ks_tf21 = ks_2samp_tf(G_sample ,X , mb_size , 21)\n",
    "    ks_tf22 = ks_2samp_tf(G_sample ,X , mb_size , 22)\n",
    "    ks_tf23 = ks_2samp_tf(G_sample ,X , mb_size , 23)\n",
    "    ks_tf24 = ks_2samp_tf(G_sample ,X , mb_size , 24)\n",
    "    ks_tf25 = ks_2samp_tf(G_sample ,X , mb_size , 25)\n",
    "    ks_tf26 = ks_2samp_tf(G_sample ,X , mb_size , 26)\n",
    "    ks_tf27 = ks_2samp_tf(G_sample ,X , mb_size , 27)\n",
    "    ks_tf28 = ks_2samp_tf(G_sample ,X , mb_size , 28)\n",
    "    ks_tf29 = ks_2samp_tf(G_sample ,X , mb_size , 29)\n",
    "    ks_tf30 = ks_2samp_tf(G_sample ,X , mb_size , 30)\n",
    "    ks_tf31 = ks_2samp_tf(G_sample ,X , mb_size , 31)\n",
    "    ks_tf32 = ks_2samp_tf(G_sample ,X , mb_size , 32)\n",
    "    ks_tf33 = ks_2samp_tf(G_sample ,X , mb_size , 33)\n",
    "    ks_tf34 = ks_2samp_tf(G_sample ,X , mb_size , 34)\n",
    "    ks_tf35 = ks_2samp_tf(G_sample ,X , mb_size , 35)\n",
    "    ks_tf36 = ks_2samp_tf(G_sample ,X , mb_size , 36)\n",
    "    ks_tf37 = ks_2samp_tf(G_sample ,X , mb_size , 37)\n",
    "    ks_tf38 = ks_2samp_tf(G_sample ,X , mb_size , 38)\n",
    "    ks_tf39 = ks_2samp_tf(G_sample ,X , mb_size , 39)\n",
    "    ks_tf40 = ks_2samp_tf(G_sample ,X , mb_size , 40)\n",
    "    ks_tf41 = ks_2samp_tf(G_sample ,X , mb_size , 41)\n",
    "    ks_tf42 = ks_2samp_tf(G_sample ,X , mb_size , 42)\n",
    "    ks_tf43 = ks_2samp_tf(G_sample ,X , mb_size , 43)\n",
    "    ks_tf44 = ks_2samp_tf(G_sample ,X , mb_size , 44)\n",
    "    stat_ks_total = tf.reduce_sum([ks_tf0 , ks_tf1 , ks_tf2 , ks_tf3 , ks_tf4, ks_tf5 , ks_tf6 , ks_tf7 , ks_tf8 , ks_tf9,\n",
    "                                   ks_tf10 , ks_tf11 , ks_tf12 , ks_tf13 , ks_tf14,ks_tf15 , ks_tf16 , ks_tf17 , ks_tf18 , ks_tf19,\n",
    "                                   ks_tf20 , ks_tf21 , ks_tf22 , ks_tf23 , ks_tf24,ks_tf25 , ks_tf26 , ks_tf27 , ks_tf28 , ks_tf29,\n",
    "                                   ks_tf30 , ks_tf31 , ks_tf32 , ks_tf33 , ks_tf34,ks_tf35 , ks_tf36 , ks_tf37 , ks_tf38 , ks_tf39,\n",
    "                                   ks_tf40 , ks_tf41 , ks_tf42 , ks_tf43 , ks_tf44] , name = \"STACK_KS\")\n",
    "    #stat_ks_total = stat_ks_total # , tf.float32 , name=\"total_ks_cast\")\n",
    "    return stat_ks_total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess = tf.InteractiveSession()\n",
    "# G_sample = np.random.normal(size= [1000,45])\n",
    "# G_sample =[tf.transpose(tf.nn.embedding_lookup(tf.transpose(G_sample), 0))]\n",
    "# X11 = np.random.normal(size= [1000,45])\n",
    "# index = 0\n",
    "# mb_size = 500\n",
    "# G_sample2 = ks_2samp_tf(G_sample ,X11 , mb_size , index)\n",
    "# G_sample2.eval()\n",
    "# tf.slice(G_sample , [0, index] , [1,-1] ).eval()\n",
    "# tf.slice(G_sample , [0, index] , [1,-1] ).eval()\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_sample , ks= G(X , z)\n",
    "\n",
    "D_real = D( X )\n",
    "D_fake = D( G_sample )\n",
    "\n",
    "\n",
    "#Z = tf.reduce_sum(tf.exp(-D_real)) + tf.reduce_sum(tf.exp(-D_fake))\n",
    "# D_loss = -tf.reduce_mean(log(D_real) + log(1 - D_fake))\n",
    "# G_loss = 0.5 * tf.reduce_mean((log(D_fake) - log(1 - D_fake))**2)\n",
    "\n",
    "\n",
    "\n",
    "fake_logit = D_fake\n",
    "real_logit = D_real\n",
    "#D_loss = - (tf.reduce_mean(log(real_logit ) + log(1 - fake_logit )))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "1 . wgan loss는 0으로 갈수록 좋음 그러므로 0으로 가면서 ks 통계량 값을(45) 를 그대로 주는 방식으로 해보기 \n",
    "    물론 너무 크게 학습이 될 우려가 아주 심하지만.... 음....\n",
    "    결국 그 자체 값의 가중치 주기\n",
    "    HINGE LOSS 방식으로 진행.\n",
    "\n",
    "2. Boundart\n",
    "\"\"\"\n",
    "\n",
    "### Hinge Loss\n",
    "# real_loss = tf.reduce_mean( tf.nn.relu(1.0 - real_logit) )\n",
    "# fake_loss = tf.reduce_mean( tf.nn.relu(1.0 + fake_logit) )\n",
    "# D_loss = real_loss + fake_loss\n",
    "\n",
    "# Gp , gradient_check = gradient_penalty(X, G_sample , D )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########### Boundary loss\n",
    "D_real2 = tf.nn.sigmoid(real_logit)\n",
    "D_fake2 = tf.nn.sigmoid(fake_logit)\n",
    "G_loss = 0.5 * tf.reduce_mean((log(D_fake2) - log(1 - D_fake2))**2) + ks_2samp_tf_total(ks , X , mb_size)\n",
    "D_loss = -tf.reduce_mean(log(D_real2) + log(1 - D_fake2)) \n",
    "\n",
    "\n",
    "q75_loss = percentile(Hint_q25 , G_sample , q=75.0) * 0.01\n",
    "q25_loss = percentile(Hint_q75 , G_sample , q=25.0) * 0.01\n",
    "\n",
    "mean_loss = mse_mean(Hint_mean , G_sample)\n",
    "std_loss  = mse_std( Hint_std , G_sample)\n",
    "\n",
    "#G_recon_loss = \n",
    "# + ks_tf\n",
    "# ks_tf = tf.cast(ks_tf , tf.float32)\n",
    "# stat_ks_total = tf.reduce_sum([ks_tf0 , ks_tf1 , ks_tf2 , ks_tf3 , ks_tf4,\n",
    "#                      ks_tf5 , ks_tf6 , ks_tf7 , ks_tf8 , ks_tf9,\n",
    "#                      ks_tf10 , ks_tf11 , ks_tf12 , ks_tf13 , ks_tf14,\n",
    "#                      ks_tf15 , ks_tf16 , ks_tf17 , ks_tf18 , ks_tf19,\n",
    "#                      ks_tf20 , ks_tf21 , ks_tf22 , ks_tf23 , ks_tf24,\n",
    "#                      ks_tf25 , ks_tf26 , ks_tf27 , ks_tf28 , ks_tf29,\n",
    "#                      ks_tf30 , ks_tf31 , ks_tf32 , ks_tf33 , ks_tf34,\n",
    "#                      ks_tf35 , ks_tf36 , ks_tf37 , ks_tf38 , ks_tf39,\n",
    "#                      ks_tf40 , ks_tf41 , ks_tf42 , ks_tf43 , ks_tf44] , name = \"STACK_KS\")\n",
    "\n",
    "# #stat_ks_total = tf.reduce_sum(stack_ks, name=\"total_ks\")\n",
    "#stat_ks_total = ks_2samp_tf_total(G_sample ,X , mb_size )\n",
    "#stat_ks_total = tf.cast(ks_2samp_tf(G_sample ,X , mb_size , 0) , tf.float32)\n",
    "#stat_ks_total = ks_2samp_tf(G_sample ,X , mb_size , 0) \n",
    "\n",
    "#G_recon_loss = q75_loss + q25_loss + mean_loss + std_loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ind = tf.constant([0, 2])\n",
    "\n",
    "result = tf.transpose(tf.nn.embedding_lookup(tf.transpose(a), ind))\n",
    "result.eval(session = sess)\n",
    "a = np.random.normal(loc=0 ,scale= 1 ,size = [1000, 5])\n",
    "b = np.random.normal(loc=5 ,scale= 2 ,size = [1000, 5])\n",
    "c = np.concatenate([a,b])\n",
    "print(c)\n",
    "c = tf.searchsorted(c , a  , side='right') / len(a)\n",
    "d = tf.searchsorted(c , b , side='right') / len(b)\n",
    "e = tf.reduce_max(tf.abs(c-d) , axis=0)\n",
    "e.eval(session = sess)\n",
    "c.eval(session = sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "sess = tf.Session(config = config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "G_sample2 = sample_data(data=data , n=mb_size)\n",
    "X2 = sample_data(data=data , n=mb_size)\n",
    "print(ks_2samp_tf(G_sample2 ,X2 , mb_size , 0).eval(session = sess ))\n",
    "print(ks_2samp_tf(G_sample2 ,X2 , mb_size , 1).eval(session = sess ))\n",
    "print(ks_2samp_tf(G_sample2 ,X2 , mb_size , 8).eval(session = sess ))\n",
    "\n",
    "G_sample2 = sample_data(data=data.values , n=mb_size)[:,0]\n",
    "X2 = sample_data(data=data.values , n=mb_size)[:,0]\n",
    "\n",
    "data1 = tf.contrib.framework.sort(G_sample2, axis = 0 )\n",
    "data2 = tf.contrib.framework.sort(X2, axis = 0 )\n",
    "data_all = tf.concat([data1,data2] , axis = 0)\n",
    "data_all.eval(session = sess)\n",
    "cdf1 = tf.searchsorted(data1,data_all,side='right' )/ ( mb_size)\n",
    "cdf2 = tf.searchsorted(data2,data_all,side='right')/ ( mb_size)\n",
    "cdf1.eval(session = sess) - cdf2.eval(session = sess)\n",
    "tf.reduce_max(tf.abs(cdf1-cdf2), axis = 0).eval(session = sess)\n",
    "cdf2 = tf.searchsorted(data2,data_all,side='right')/ ( mb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Generator/GW1:0' shape=(64, 128) dtype=float32_ref>, <tf.Variable 'Generator/GB1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'Generator/GW2:0' shape=(128, 64) dtype=float32_ref>, <tf.Variable 'Generator/GB2:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'Generator/GW3:0' shape=(64, 32) dtype=float32_ref>, <tf.Variable 'Generator/GB3:0' shape=(32,) dtype=float32_ref>, <tf.Variable 'Generator/GW4:0' shape=(32, 45) dtype=float32_ref>, <tf.Variable 'Generator/GB4:0' shape=(45,) dtype=float32_ref>] \n",
      " [<tf.Variable 'Discriminator/DW1:0' shape=(45, 128) dtype=float32_ref>, <tf.Variable 'Discriminator/DB1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'Discriminator/DW2:0' shape=(128, 64) dtype=float32_ref>, <tf.Variable 'Discriminator/DB2:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'Discriminator/DW3:0' shape=(64, 32) dtype=float32_ref>, <tf.Variable 'Discriminator/DB3:0' shape=(32,) dtype=float32_ref>, <tf.Variable 'Discriminator/DW4:0' shape=(32, 16) dtype=float32_ref>, <tf.Variable 'Discriminator/DB4:0' shape=(16,) dtype=float32_ref>, <tf.Variable 'Discriminator/DW5:0' shape=(16, 1) dtype=float32_ref>, <tf.Variable 'Discriminator/DB5:0' shape=(1,) dtype=float32_ref>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "#fake_loss = -tf.reduce_mean(fake)\n",
    "#D_fake = tf.nn.sigmoid(fake_logit)\n",
    "#G_loss = 0.5 * tf.reduce_mean((log(D_fake) - log(1 - D_fake))**2)\n",
    "\n",
    "\n",
    "####################### 새로 배운 것 #########################################\n",
    "\n",
    "t_vars = tf.trainable_variables()\n",
    "\n",
    "d_vars = [var for var in t_vars if 'Discriminator' in var.name]\n",
    "g_vars = [var for var in t_vars if 'Generator' in var.name]\n",
    "#total_vars =  g_vars + d_vars\n",
    "\n",
    "print(g_vars, \"\\n\",d_vars)\n",
    "\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "    G_solver = tf.train.AdamOptimizer(lr).minimize(G_loss, var_list=g_vars)\n",
    "    D_solver = tf.train.AdamOptimizer(lr).minimize(D_loss, var_list=d_vars)\n",
    "    #G_recon_solver = tf.train.AdamOptimizer(lr).minimize(G_recon_loss, var_list=g_vars)\n",
    "    \n",
    "    \n",
    "\n",
    "# theta_G = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"Generator\")\n",
    "# theta_D = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"Discriminator\")\n",
    "\n",
    "\n",
    "# with tf.variable_scope(tf.get_variable_scope(),reuse=False): \n",
    "#     trainerD = tf.train.AdamOptimizer().minimize(D_loss, var_list=d_vars) \n",
    "#     trainerG = tf.train.AdamOptimizer().minimize(G_loss, var_list=g_vars)\n",
    "\n",
    "# D_solver = (tf.train.AdamOptimizer(learning_rate=lr)\n",
    "#             .minimize(D_loss, var_list=theta_D))\n",
    "\n",
    "# G_solver = (tf.train.AdamOptimizer(learning_rate=lr)\n",
    "#             .minimize(G_loss, var_list=theta_G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _strip_consts( graph_def):\n",
    "        from IPython.display import clear_output, Image, display, HTML\n",
    "        \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "        strip_def = tf.GraphDef()\n",
    "        for n0 in graph_def.node:\n",
    "            n = strip_def.node.add() \n",
    "            n.MergeFrom(n0)\n",
    "            if n.op == 'Const':\n",
    "                tensor = n.attr['value'].tensor\n",
    "                size = len(tensor.tensor_content)\n",
    "        return strip_def\n",
    "\n",
    "\n",
    "def _show_graph( graph_def):\n",
    "    from IPython.display import clear_output, Image, display, HTML\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = _strip_consts(graph_def)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "        function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "        }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "        <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:100%;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n",
    "\n",
    "def tensorboard():\n",
    "    _show_graph(tf.get_default_graph().as_graph_def())\n",
    "    \n",
    "    \n",
    "tensorboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler(feature_range= (0,1))\n",
    "\n",
    "x_plot = pd.DataFrame(sample_data(data = data.values , n=mb_size) , columns = col)\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "mean = np.expand_dims(np.mean(data,axis = 0) ,axis = 0)\n",
    "\n",
    "std = np.expand_dims(np.std(data,axis = 0) ,axis = 0)\n",
    "\n",
    "q75 = np.expand_dims(np.quantile(data , axis = 0 , q=0.75),axis = 0)\n",
    "\n",
    "q25 = np.expand_dims(np.quantile(data, axis = 0 , q=0.25),axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "def Check_KS(ks_output , Total_ks_graph , Margin , row , ncol) : \n",
    "    total = sess.run(G_sample, feed_dict={z: sample_z(mb_size, z_dim)})\n",
    "    total = scaler.inverse_transform(total)\n",
    "    g_plot = pd.DataFrame(total , columns = col )\n",
    "    g_plot[fac_var] = g_plot[fac_var].round(0)\n",
    "    Value = []\n",
    "    for label in col :\n",
    "        sample = g_plot[label]\n",
    "        real   = x_plot[label]\n",
    "        ks , p = stats.ks_2samp(real.values , sample.values)\n",
    "        Value.append(ks)\n",
    "    Total_KS = round( np.sum(Value) , 2)\n",
    "    ks_2 = [iteration] + Value\n",
    "    ks_3 = pd.DataFrame([ks_2], columns = [\"iter\"] + col)\n",
    "    ks_output = ks_output.append(ks_3)\n",
    "    fig , ax = plt.subplots(figsize=(26,13))\n",
    "    fig.subplots_adjust(top = 0.95 , left = 0.03 , bottom = 0.05 , right = 0.99)\n",
    "    updown = 0\n",
    "    for name in col : \n",
    "        if updown % 2 == 0 :\n",
    "            param , space=\"bottom\" , \"  \"\n",
    "        else : \n",
    "            param , space =\"top\" , \"   \"\n",
    "        ax.plot(ks_output.iter , ks_output[[name]], label = name)\n",
    "        ax.text(iteration , ks_output.loc[ks_output[\"iter\"]==iteration , [name]].values , space + name ,\n",
    "                verticalalignment = param)\n",
    "        updown +=1\n",
    "    ax.set_title(\"KS [{}]\".format(Total_KS) , fontsize = 30 )\n",
    "    ax.set_xlabel(\"iteration\")\n",
    "    ax.set_ylabel(\"KS\")    \n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0 + box.height * 0.1, box.width, box.height * 0.9])\n",
    "    ax.set_title(\"KS Measure {}[<1] , Max : {},  Minimum : {}\".format(Total_KS , len(col), Total_ks_graph.ks.min() ) , fontsize = 30)\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True, shadow=True, ncol=15 , fontsize= 10)\n",
    "    ax.text(iteration+1 , 0.05 , \"   0.05\", verticalalignment = param)\n",
    "    ax.axhline(0.05, linewidth=4, color='r')\n",
    "    plt.savefig(\"./V0_SN_KS_{}.png\".format(path_v))\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    Total Graph\n",
    "    \"\"\"\n",
    "    Total_ks_graph_ap = pd.DataFrame({\"iter\":[iteration] , \"ks\" :[Total_KS]})\n",
    "    Total_ks_graph    = Total_ks_graph.append(Total_ks_graph_ap)\n",
    "    #Total_ks_graph    = Total_ks_graph[Total_ks_graph.iter>0]\n",
    "    fig , ax = plt.subplots(figsize = (26,13))\n",
    "    fig.subplots_adjust(top = 0.95 , left = 0.03 , bottom = 0.04 , right = 0.99)\n",
    "    ax.plot(Total_ks_graph.iter , Total_ks_graph.ks , linestyle =\"-\" , marker =\".\" , linewidth = 3, markersize = 12)\n",
    "    ax.set_title(\"Iter : {} , KS : {} , Max : {},  Minimum : {}\".format(iteration-1 ,  Total_KS , len(col) , Total_ks_graph.ks.min() ), fontsize= 30)\n",
    "    plt.savefig(\"./V0_Total_SN_KS_{}.png\".format(path_v))\n",
    "    plt.show()\n",
    "    if Margin > Total_KS : \n",
    "        show_plot(row , ncol )\n",
    "        print(\"=======UPDate============\")\n",
    "        print(\"Margin : {} ===> {}\".format(Margin , Total_KS))\n",
    "        Margin = Total_KS\n",
    "        total = sess.run(G_sample, feed_dict={z: sample_z( 5000, z_dim)})\n",
    "        total = scaler.inverse_transform(total)\n",
    "        g_plot = pd.DataFrame(total , columns = col )\n",
    "        g_plot[fac_var] = g_plot[fac_var].round(0)\n",
    "        g_plot.to_csv(\"./V0_SN_Generated_{}.csv\".format(path_v) , index = False)\n",
    "        \n",
    "        saver.save(sess , model_dir + \"./V0_SN_MODEL_{}\".format(path_v))\n",
    "    clear_output(wait= True)\n",
    "    return ks_output , Total_ks_graph , Margin , Total_KS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def show_plot(row , ncol ) : \n",
    "    \n",
    "    fig , axes = plt.subplots(row , ncol , figsize = (26,13))\n",
    "    fig.subplots_adjust(hspace = 0.2 , wspace= 0.14 , top = 0.92 , left = 0.03 , bottom = 0.04 , right = 0.99)\n",
    "    total = sess.run(G_sample, feed_dict={z: sample_z(mb_size, z_dim)})\n",
    "    try : \n",
    "        total = total[~np.isnan(total).any(axis=1)]\n",
    "        total = scaler.inverse_transform(total)\n",
    "        g_plot = pd.DataFrame(total , columns = col )\n",
    "        g_plot[fac_var] = g_plot[fac_var].round(0)\n",
    "        \"\"\"\n",
    "        좀 더 쉬운 분포로 만들어서 학습시킨 후 다시 원래값으로 (factor 변수이기 때문에 가능하다 생각함.)\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        col2 = 0\n",
    "        error = []\n",
    "        for j in range(row) :\n",
    "            for k in range(ncol) :\n",
    "                try :\n",
    "                    label = col[col2]\n",
    "                    sample = g_plot.loc[: , label]\n",
    "                    sample.name = \"Gene\"\n",
    "                    real_0 = x_plot.loc[: , label]\n",
    "                    real_0.name =\"Real\"\n",
    "                    ks , p = stats.ks_2samp(real_0.values , sample.values)\n",
    "                    error.append(ks)\n",
    "                    col2 += 1\n",
    "                    if label in fac_var : \n",
    "                        sns.distplot( sample , ax=axes[j , k], norm_hist =True , kde=False , hist_kws ={\"color\":\"r\" , \"label\" :\"Gene\", \"rwidth\":0.75})\n",
    "                        sns.distplot(real_0 , ax=axes[j , k],norm_hist =True, kde=False , hist_kws ={\"color\":\"g\" , \"label\" :\"Real\", \"rwidth\":0.75})\n",
    "                        axes[j , k].legend(fontsize = 10)\n",
    "                    elif label in num_var : \n",
    "                        sns.distplot(  sample , ax=axes[j , k] ,\n",
    "                                     kde_kws={\"color\": \"r\", \"lw\": 2, \"label\": \"Gene\" , \"shade\" : True} , hist =False , rug = False) #   \n",
    "                        sns.distplot(  real_0 , ax=axes[j , k] ,\n",
    "                                     kde_kws={\"color\": \"g\", \"lw\": 2, \"label\": \"Real\", \"shade\" : True } , hist =False , rug = False) # \n",
    "                        axes[j , k].legend(fontsize = 10 )\n",
    "                    axes[j , k].set_title( label , loc =\"left\" , fontsize= 10 )\n",
    "                except IndexError as e : \n",
    "                    axes[j , k].axis(\"off\")\n",
    "        \n",
    "        KS_DIF = round(np.sum(error),2)\n",
    "        plt.suptitle('SNGAN Iteration {} ,  {} , D_loss : {} , G_loss : {} KS : {}'.format(i,res , dloss,gloss,KS_DIF) , fontsize= 30)\n",
    "        plt.savefig(\"./visualization_SN_{}.png\".format(path_v))\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "\n",
    "        fig , ax = plt.subplots(figsize = (26,13))\n",
    "        fig.subplots_adjust(top = 0.95 , left = 0.03 , bottom = 0.04 , right = 0.99)\n",
    "\n",
    "        ax.plot(output.iter , output.dloss , label =\"dloss\" , linestyle =\"-\" , marker =\".\" , linewidth = 4, markersize = 12)\n",
    "        ax.plot(output.iter , output.gloss , label =\"gloss\" , linestyle =\"-\" , marker =\".\" , linewidth = 4, markersize = 12)\n",
    "        ax.set_title(\"Iter : {} , Res : {}(>6) , Dloss : {} , Gloss : {}(>2) stat loss : {}\".format(iteration-1 ,  res , dloss, gloss , 0), fontsize= 30)\n",
    "        ax.set_ylim(-5, 15)\n",
    "        box = ax.get_position()\n",
    "        ax.set_position([box.x0, box.y0 + box.height * 0.1, box.width, box.height * 0.9])\n",
    "        ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True, shadow=True, ncol=4 , fontsize= 20)\n",
    "        plt.savefig(\"./loss_log_SN_{}.png\".format(path_v))\n",
    "        plt.show()\n",
    "    \n",
    "        return print(\"시각화\")\n",
    "\n",
    "    except Exception as e : \n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 10087; D_loss: 0.6782; G_loss: 1.026\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "model_dir = \"./Model_Save/{}\".format(path_v)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "\n",
    "config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "sess = tf.Session(config = config) # config = config\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "i = 0\n",
    "ks_output = pd.DataFrame([[0]+[1.]*len(col)], columns = [\"iter\"]+col)\n",
    "\n",
    "iteration , dloss ,  gloss = 0 , 0, 0\n",
    "output = pd.DataFrame({\"iter\" : [iteration] , \"dloss\" : [dloss] , \"gloss\" : [gloss] })\n",
    "ks_init = len(col)\n",
    "Total_ks_graph = pd.DataFrame({\"iter\":[iteration] , \"ks\" :[ks_init]})\n",
    "\n",
    "Margin = 10\n",
    "ROW , COL = 7,7\n",
    "\n",
    "print(\"START\")\n",
    "res =\"Boundary Loss Replay Memory\"\n",
    "\n",
    "# for i in range(20000):\n",
    "#     X_mb = sample_data(data = data , n=mb_size)\n",
    "#     z_mb = sample_z(mb_size, z_dim)\n",
    "    \n",
    "#     _, G_recon_loss_curr = sess.run([G_recon_solver, G_recon_loss ],  # , stat_ks_total # , ks_tf1\n",
    "#                                     feed_dict={X: X_mb , z: z_mb ,\n",
    "# #                                                Hint_mean : mean , Hint_std : std , \n",
    "# #                                                Hint_q25 : q25 , Hint_q75 : q75  \n",
    "#                                               })\n",
    "\n",
    "#     if i % 1000 == 0:\n",
    "#         clear_output(wait= True)\n",
    "#         show_plot(row = ROW , ncol= COL)\n",
    "#         print('Iter-{}; Pretrained G loss: {:.4}'.format(i, G_recon_loss_curr))\n",
    "#         #print(\"Q75 :{} ,Q25 : {} , Mean : {} , Std : {}\".format( q75_loss1 , q25_loss1 , mean_loss1 , std_loss1))\n",
    "#         #print(\"Tensorflow KS : {}\".format(ks_tf1))\n",
    "\n",
    "\n",
    "for i in range(1000000):\n",
    "    X_mb = sample_data(data = data , n=mb_size)\n",
    "    z_mb = sample_z(mb_size, z_dim)\n",
    "    \n",
    "    \n",
    "    if i < 200 : \n",
    "        for _ in range(10) :\n",
    "            _, dloss = sess.run(\n",
    "                [D_solver, D_loss], feed_dict={X: X_mb, z: z_mb})\n",
    "    else : \n",
    "        \n",
    "        if (Total_KS > Total_ks_graph.ks.min()) & (Total_ks_graph.ks.min() < 9 ): \n",
    "            print(\" Experience Memory 사용하여 학습 다시 시키기 \")\n",
    "            chkpt_fname = tf.train.latest_checkpoint( model_dir )\n",
    "            saver.restore(sess, chkpt_fname)\n",
    "            z_test_value = sample_z(m=5000 , n=z_dim)\n",
    "            good_replay_v = sess.run(G_sample, feed_dict={z : z_test_value})\n",
    "            \n",
    "            for _ in range(3) :\n",
    "                _, experience_loss = sess.run(\n",
    "                    [D_solver, D_loss], feed_dict={X: good_replay_v , z: z_mb })\n",
    "            \n",
    "        for _ in range(10) :\n",
    "            _, dloss = sess.run(\n",
    "                [D_solver, D_loss], feed_dict={X: X_mb, z: z_mb})\n",
    "            \n",
    "    \n",
    "    for _ in range(5) :\n",
    "        _, gloss  = sess.run(\n",
    "                [G_solver, G_loss ], feed_dict={X: X_mb, z: z_mb})\n",
    "\n",
    "    \n",
    "    iteration +=1\n",
    "    \n",
    "#     D_limit  = 1.0 \n",
    "#     G_limit  = 1.0 \n",
    "    \n",
    "        \n",
    "#     dcount = 0 \n",
    "#     while dloss > D_limit  : \n",
    "#         dcount += 1\n",
    "#         _, dloss = sess.run([D_solver, D_loss], feed_dict={X: X_mb, z: z_mb})\n",
    "#         if dcount > 2000 :\n",
    "#             print(\"{}th  Dicriminator Loss : {}\".format(i, dloss) )\n",
    "#             break\n",
    "#         if dloss < D_limit : \n",
    "#             print(\"D 탈출\")\n",
    "            \n",
    "#     gcount = 0 \n",
    "#     while gloss >  G_limit :\n",
    "#         gcount += 1 \n",
    "#         _, gloss  = sess.run([G_solver, G_loss ], feed_dict={X: X_mb, z: z_mb})\n",
    "        \n",
    "#         if gcount > 1000 :\n",
    "#             print(\" {}th  KS Loss : {}\".format(i, gloss) )\n",
    "#             break\n",
    "    \n",
    "#     ks_count = 0 \n",
    "#     if i > 1000 : \n",
    "#         print(\"G_Limit : {}\".format(Total_ks_graph.ks.min()))\n",
    "#         ks_count += 1 \n",
    "#         Stat_Loss = Total_KS\n",
    "#         while Stat_Loss >  Total_ks_graph.ks.min() :\n",
    "#             _, Stat_Loss = sess.run([G_recon_solver, G_recon_loss  ],feed_dict={X: X_mb , z: z_mb})\n",
    "#         if ks_count > 1000 : \n",
    "#             print(\"{}th Fail KS Loss : {}\".format(i, Stat_Loss) )\n",
    "#             break\n",
    "        \n",
    "        \n",
    "            \n",
    "    dloss = round( np.float64(dloss),4)\n",
    "    gloss = round( np.float64(gloss),4)   \n",
    "    output1 = pd.DataFrame({\"iter\" : [iteration] , \"dloss\" : [dloss] , \"gloss\" : [gloss] })\n",
    "    output  = output.append(output1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ks_output , Total_ks_graph , Margin  , Total_KS = Check_KS(ks_output , Total_ks_graph , Margin , ROW , COL)\n",
    "    print('Iter: {}; D_loss: {:.4}; G_loss: {:.4}'\n",
    "              .format(i, dloss, gloss))\n",
    "    if (i % 100 == 0) & (Margin < 10.0) :\n",
    "        clear_output(wait= True)\n",
    "        print('Iter: {}; D_loss: {:.4}; G_loss: {:.4}'\n",
    "              .format(i, dloss, gloss))\n",
    "        show_plot(row = ROW , ncol= COL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실시간 KS 그림을 찍으면서 기준 KS를 통과하면 저장하는 방식으로 진행\n",
    "\n",
    "## Update\n",
    "### Tensorflow 에 KS Loss 심음 점점 0으로 가까이가게\n",
    "\n",
    "## 일단 searchsort가 미분을 할 수 있는 method가 아닌 것 같다.\n",
    "## 그래도 하나 얻은 점이 있다. \n",
    "> ## 특정한 것에 대해서 좀 더 recontrinction loss 로 활용 가능할 듯 \n",
    "* ### EWS_A_K_D1M232000_OPR  \n",
    "* ### EWS_A_K_D90232200_OPR\n",
    "* ### EWS_A_K_D10231000_OPR\n",
    "* ### EWS_A_K_D10232000_OPR\n",
    "* ### EWS_C_N_P32002600\n",
    "* ### Etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
