{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [IDEA 제공한 사이트](https://medium.com/jungle-book/towards-data-set-augmentation-with-gans-9dd64e9628e6)\n",
    "\n",
    "## Modification Version - 2\n",
    "* [google wasserstein dist 추가](https://github.com/google/wasserstein-dist/blob/master/train.py)\n",
    "* variable_scope 추가\n",
    "* 수정한 Loss 방식 문제 확인 후 다시 원래대로\n",
    "* tf.nn.moments 를 활용하여 평균, 분산 구해서 통계량값이 맞도록 처리\n",
    "* dist 시각화 및 total dist를 잴 때 생성된 값과 여러번의 샘플링 된 것과의 거리들의 평균과 표준편차를 시각화 \n",
    "    * cross validation 처럼 하고 싶었음. \n",
    "    * 평균도 낮으면서(분포간 거리도 가까우면서), 다양한 샘플을 넣었을 때도 거리차이가 작기를 기대하면서\n",
    "    \n",
    "## Modification Version - 3\n",
    "* 변수 추가\n",
    "* 아직 문제점 변수가 많아졌을 때 손으래 해줘야하는 작업들이 있음\n",
    "    * 어떤 변수가 Count라서 Integer로 해줘야할지? \n",
    "    * 변수별처리가 수동적임 처음 지정된 type에 굉장히 의존하게 됨.\n",
    "* Col, Total Stat Loss 다시 추가 \n",
    "    * Col Stat Loss에서 문제점은 Categoricla Onehot 된 것에 대한 평균 분산을 구하게 됨. ( 거기서 Loss를 nan으로 발생시킬우려가 있는 듯함)\n",
    "* **추가** Categorical Loss에 Ratio를 추가\n",
    "    * 각 Categorical의 도수 비율이 유사한 비율로 나오게 제한해서 생성시키는 것이 목적\n",
    "* Prelu 추가\n",
    "    * 아직 테스트 중 초반에는 파라미터도 같이 학습해야해서 잘 안될 것이라고 생각이 들긴함.\n",
    "* 시각화\n",
    "    * bins를 추가해서 Count, Categorical 변수들이 극명하게 잘 보일 수 있도록 시각화.\n",
    "    \n",
    "## Modification Version - 4\n",
    "* Correlation Loss 수정 예정\n",
    "    * Frobenius Norm 으로 변경 예정  [URL](http://mathworld.wolfram.com/FrobeniusNorm.html)\n",
    "        * 좀 더 이론적임\n",
    "        * 흔히 Regularization으로 사용한다고 함.\n",
    "            * 기대하는 바는 상관성을 벗어나지 않으면서 생성할 수 있게 강제 할 수 있을 듯 \n",
    "        * https://stackoverflow.com/questions/43917456/matrix-norm-in-tensorflow\n",
    "        * 생성된 G의 Corr - R의 Corr 에서 빼면 2배가 되기때문에 한쪽만 생성할 수 있게 matrix_band_part를 사용\n",
    "            * tf.matrix_band_part(input, 0, -1) ==> Upper triangular part.\n",
    "            * tf.matrix_band_part(input, -1, 0) ==> Lower triangular part.\n",
    "            * tf.matrix_band_part(input, 0, 0) ==> Diagonal.\n",
    "    * 3차 모멘텀 추가 구현 [URL](https://en.wikipedia.org/wiki/Moment_(mathematics))\n",
    "        * 모든 모멘텀이 같다면, 같은 분포다.\n",
    "        * Numeric 변수에 대해서 시행\n",
    "        * Skewness 역할을 한다고 함.\n",
    "    * `신기한 점`\n",
    "        * 현재 Numeric변수들에 대해서 처리했는데, 원래 잘 안되던 `state` 에서 고르게 생성되는 것을 확인\n",
    "## Modification Version - 5\n",
    "* Categorical Loss에 추가적으로 Cosine Similarity Loss 추가함.\n",
    "    * 도수가 위치하는 차원이 근처에 있게?!\n",
    "   * 저 빈도자체에서 크게 벗어나지 않을 것이다라고 생각해서 \n",
    "    * 각 빈도를 평균으로 삼고 그러면 integer 가 되서 미분 가능하게 하기 위해 `reparameterization trick`을 적용시킨 다음에, 가깝게 매칭시키는 생각?!\n",
    " > 현재 확인하면 이 Loss는 거의 0가 안되고 있음 \n",
    "\n",
    "## Modification Version - 7\n",
    "* Modification Version - 5 에서 좀 더 확장해서 생각을 해본 것이 빈도수를 평균이라 가정하고 그 주변에서 발생할거라고 생각\n",
    "* 그리고 그 것들에 대한 KL Divergence를 구해서 Loss 떨어트리기\n",
    " \n",
    "## Modification Version - 8\n",
    " * 새로운 데이터 `Pokemon`에 접목 \n",
    "     * 일단 쌍봉인 것도 있고, `total` 변수같이 다른 변수의 합으로 파생되서 나오는 것들에 대해서 확인할 필요가 있어서 시도\n",
    " * **Missing 처리**\n",
    "     * Categorical 에 대해서 결측치 처리 하나의 변수를 추가해서 함\n",
    " * **-1 , 1 너무 극단적인 값에 쏠리는 현상 발생**\n",
    "     * 발생 배경은 추측컨데, node를 늘리면서 시작 된 것 같음\n",
    "         * 기본 구조가 activation을 identity 와 prelu를 쓰다보니, 계속 값이 상승하는 현상이 발생함.\n",
    "             * 억제 시도 \n",
    "                 * node의 수를 줄인다.\n",
    "                      * 하지만 문제는 결국 onehot 포함해서 1500차원이 되는데, 그때 극단치로 나오게 된다.\n",
    "                      * 과연 이 문제가 학습을 하면 없어질 문제일 것인지?\n",
    "                 * batch normalization\n",
    "                 * tf.nn.relu6 , tf.nn.relu 같이 activation을 꺼버리는 것을 통해서 제한을 줘버린다?\n",
    " * **tensorboard**에 지금 현재 상황 체크 가능하게 분포를 표시할 수 있다는 사실 발견!!\n",
    "\n",
    " > 결국 중간에는 Batch Normalization으로 잘 끌고가는데, 결국 마지막에서 fully connected 들어가기전에 값이 굉장히 커지는 것 같다.\n",
    " 이 부분을 해결하기위해 relu를 -1 , 2 까지로하는 것으로 변형해서 음수는 살아 있으면서 반영되게 함\n",
    " \n",
    " ## Modification Version - 10\n",
    " * [layer function](http://220.67.120.131:8888/notebooks/SR/Project/IITP/Class%3D1.ipynb)을 다시 사용\n",
    "     * Init 함수에 넣어 놓음 \n",
    "     * Spectral Normalization Generator 추가해서 시도!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets , sys\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sys.path.append('/home/advice/Python/SR/Custom/')\n",
    "from jupyter_tensorboard import *\n",
    "from utility import *\n",
    "from Init import *\n",
    "from Activations import *\n",
    "from wasserstein import Wasserstein\n",
    "from IPython.display import clear_output\n",
    "import os , re\n",
    "import collections\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "usecol = ['type1', 'type2', 'ability1',\n",
    "       'ability2', 'abilityH', 'hp', 'attack', 'defense', 'spattack',\n",
    "       'spdefense', 'speed', 'total', 'weight', 'height',\n",
    "       'class', 'percent-male', 'percent-female', 'pre-evolution',\n",
    "       'egg-group1', 'egg-group2']\n",
    "\n",
    "pokemon = pd.read_csv(\"./../Data/Pokemon/pokemon.csv\" , usecols = usecol)\n",
    "#\n",
    "## .str.replace('lbs.', '').astype(float)\n",
    "pokemon[\"weight(kg)\"] = \\\n",
    "pokemon[\"weight\"].map(lambda x : float(re.sub('lbs.', '' , x )) * 0.453592)\n",
    "pokemon[\"height(cm)\"] = \\\n",
    "pokemon[\"height\"].map(lambda x : round( 2.54 * (float(re.sub(r'[^A-Za-z0-9]+', ' ' , x ).split(\" \")[0])*12 + float(re.sub(r'[^A-Za-z0-9]+', ' ' , x ).split(\" \")[1])  ) , 1 ))\n",
    "\n",
    "pokemon.drop([\"height\" , \"weight\" ,  ], axis = 1 , inplace= True)\n",
    "## https://www.w3resource.com/python-exercises/python-basic-exercise-59.php\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pokemon[[\"percent-male\", \"percent-female\"]].dropna(axis = 0).drop_duplicates()\n",
    "b = 1- a \n",
    "c = pd.merge(a,b, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling\n",
    "if os.path.isfile(\"pokemon_profiling.html\")  :\n",
    "    pass\n",
    "else :\n",
    "    profile = pokemon.profile_report(title='pokemon Profiling Report')\n",
    "    profile.to_file(output_file=\"pokemon_profiling.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pokemon.shape)\n",
    "#pokemon.dropna(axis = 0 , subset=[\"percent-male\", \"percent-female\"] , inplace= True )\n",
    "### 1 - percent-female =  percent_male\n",
    "## 치환만 해주면 되는 거라서! \n",
    "pokemon = pokemon.fillna(\"NULL\")\n",
    "pokemon[[\"percent-male\", \"percent-female\"]] = pokemon[[\"percent-male\", \"percent-female\"]].astype(object)\n",
    "replace_gender = pokemon[[\"percent-male\", \"percent-female\"]].drop_duplicates()\n",
    "c = c.astype(object)\n",
    "replace_gender = pd.merge(c,replace_gender, how='outer').values.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_gender = dict(zip(replace_gender[:,0] , replace_gender[:,1]))\n",
    "replace_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon.drop([\"percent-male\" , \"total\"], axis = 1 , inplace= True)\n",
    "print(pokemon.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pokemon[[\"type1\" , \"type2\" ]].groupby([pokemon[\"ability1\"] , pokemon[\"ability2\"] , pokemon[\"abilityH\"]]).count()\n",
    "pokemon.pivot_table(values='abilityH', index =['type1' ],columns= [\"type2\"] ,aggfunc='count' , fill_value= 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Real = pokemon.copy()\n",
    "Real[\"percent-female\"] = Real[\"percent-female\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "39 + 52 + 43 + 60 + 50 + 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Real.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Real[\"percent-female\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/21271581/selecting-pandas-columns-by-dtype\n",
    "\n",
    "```\n",
    "## all float \n",
    "df.select_dtypes(include=['floating'])\n",
    "## all integer\n",
    "df.select_dtypes(include=['integer'])\n",
    "## all numeric\n",
    "df.select_dtypes(include=['number'])\n",
    "## select\n",
    "[col for col in df.columns.tolist() if df[col].dtype not in ['object','<M8[ns]']]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Real.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PostProcessing\n",
    "\n",
    "\n",
    "* Numeric\n",
    "    * Inverse Transform을 진행해서 원래 분포의 범위로 변환시켜주기\n",
    "    * **integer**\n",
    "        * count variable 같은 경우 round로 정수형으로 처리해주기?\n",
    "    * **numeric**\n",
    "        * 그대로 사용해도 가능 할 듯\n",
    "* Categorical\n",
    "    * softmax로 나오므로 argmax 처리하고, 분포를 보여줄때는 수치형으로 보여주고 사용할 때는 변환하는 방식\n",
    "    * 일단 그림을 그릴 때는 그대로 사용하면 될 듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Real.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_name = Real.select_dtypes(include= [\"float\"]).columns.tolist()\n",
    "float_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Real.select_dtypes(include= [\"number\"]).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Int_name = Real.select_dtypes(include= [\"integer\"]).columns.tolist()\n",
    "object_name = Real.select_dtypes(exclude= [\"number\"]).columns.tolist()\n",
    "numeric_name = Real.select_dtypes(include= [\"number\"]).columns.tolist()\n",
    "print(\"OBJECT : \", object_name)\n",
    "print(\"NUMERIC : \" ,numeric_name)\n",
    "print(\"Int : \",Int_name)\n",
    "print(\"float : \", float_name )\n",
    "assert len(Int_name) + len(float_name) == len(numeric_name) , \"개수가 일치 하지 않음 체크 필요함!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Real = Real[numeric_name + object_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = Real.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_length = 0 \n",
    "info = {}\n",
    "for i in object_name :\n",
    "    value = Real[i].nunique()\n",
    "    info[i] = value\n",
    "    total_length += value\n",
    "    \n",
    "total_length += len(numeric_name)\n",
    "print(total_length)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Real.values\n",
    "def sample_data(n= len(data) , data = None) : \n",
    "    output = data[list(np.random.choice(len(data) , n))]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = np.shape(data)[1] \n",
    "latent_dim = 100\n",
    "\n",
    "tf.reset_default_graph()\n",
    "batch_size = tf.placeholder(tf.int64, name=\"Batchsize\")\n",
    "X = tf.placeholder(tf.float32,[None , total_length  ], name= \"X\")\n",
    "Z = tf.placeholder(tf.float32,[None, latent_dim ] , name = \"Z\")\n",
    "phase = tf.placeholder(tf.bool, name='phase')\n",
    "\n",
    "\n",
    "def parametric_relu(_x):\n",
    "    \"\"\"\n",
    "    maximum을 제한을 4정도로 줌 \n",
    "    \"\"\"\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],\n",
    "                       initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=tf.float32)\n",
    "    pos = tf.minimum(tf.nn.relu(_x) , 4.0)\n",
    "    neg = alphas * (_x - abs(_x)) * 0.5\n",
    "    return pos + neg\n",
    "\n",
    "def dense(x, size, scope):\n",
    "    return tf.contrib.layers.fully_connected(x, size, \n",
    "                                             activation_fn=None,\n",
    "                                             scope=scope)\n",
    "\n",
    "def dense_batch_relu(input_ , output_ , activation , phase, scope):\n",
    "    with tf.variable_scope(scope):\n",
    "        h1 = tf.contrib.layers.fully_connected(input_ , output_ , \n",
    "                                               activation_fn=None,\n",
    "                                               scope='dense')\n",
    "        h2 = tf.contrib.layers.batch_norm(h1, \n",
    "                                          center=True, scale=True, \n",
    "                                          is_training=phase,\n",
    "                                          scope='bn')\n",
    "        if activation :\n",
    "            output = activation(h2)\n",
    "        else :\n",
    "            output =h2\n",
    "        return output\n",
    "# 출처: https://creamyforest.tistory.com/48 [Dohyun's Blog]\n",
    "def Relu2(tensor):\n",
    "    return tf.minimum(tf.maximum(tensor,-2.0) , 2.0)\n",
    "\n",
    "\n",
    "def nalu_v2(input_layer, num_outputs):\n",
    "    \"\"\" Neural Arithmetic Logic Unit tesnorflow layer\n",
    "    Arguments:\n",
    "    input_layer - A Tensor representing previous layer\n",
    "    num_outputs - number of ouput units \n",
    "    Returns:\n",
    "    A tensor representing the output of NALU\n",
    "    \"\"\"\n",
    "\n",
    "    shape = (int(input_layer.shape[-1]), num_outputs)\n",
    "\n",
    "    # define variables\n",
    "    W_hat = tf.Variable(tf.truncated_normal(shape, stddev=0.02))\n",
    "    M_hat = tf.Variable(tf.truncated_normal(shape, stddev=0.02))\n",
    "    G = tf.Variable(tf.truncated_normal(shape, stddev=0.02))\n",
    "\n",
    "    # operations according to paper\n",
    "    W = tf.tanh(W_hat) * tf.sigmoid(M_hat)\n",
    "    m = tf.exp(tf.matmul(tf.log(tf.abs(input_layer) + 1e-7), W))\n",
    "    g = tf.sigmoid(tf.matmul(input_layer, G))\n",
    "    a = tf.matmul(input_layer, W)\n",
    "    out = g * a + (1 - g) * m\n",
    "\n",
    "    return out\n",
    "\n",
    "def Encoder(X ,  is_training = True , reuse = False  , hsize = [750 , 600 , 500 , 400 , latent_dim ]) :\n",
    "    with tf.variable_scope(\"BiGAN/Encoder\",reuse=reuse): \n",
    "        h = layer(X , X.shape[1] , hsize[0] , \"layer1\" , parametric_relu , \n",
    "                   usebias = True ,  final = False , SN = False ,  Type = \"Batch_Normalization\" , phase = is_training\n",
    "                  )\n",
    "        h = layer(h , h.shape[1] , hsize[1] , \"layer2\" , parametric_relu , \n",
    "                   usebias = True ,  final = False , SN = False ,  Type = \"Batch_Normalization\" , phase = is_training\n",
    "                  )\n",
    "        h = layer(h , h.shape[1] , hsize[2] , \"layer3\" , parametric_relu , \n",
    "                   usebias = True ,  final = False , SN = False ,  Type = \"Batch_Normalization\" , phase = is_training\n",
    "                  )\n",
    "        h = layer(h , h.shape[1] , hsize[3] , \"layer4\" , parametric_relu , \n",
    "                   usebias = True ,  final = False , SN = False ,  Type = \"Batch_Normalization\" , phase = is_training\n",
    "                  )\n",
    "        out = tf.layers.dense(h, hsize[4], activation=None, use_bias=False )\n",
    "        return out\n",
    "        \n",
    "        \n",
    "\n",
    "def generator(Z , batch_size , hsize=[ 250, 300 , 350 , 400 , 600],reuse=False):\n",
    "    with tf.variable_scope(\"GAN/Generator\",reuse=reuse):\n",
    "        h1 = tf.layers.dense(Z   , 200 , activation = belu , use_bias= True )\n",
    "        #h1 = dense_batch_relu(h1 , 200 , belu , phase, \"trans\")\n",
    "        tf.summary.histogram(\"G_Transform_z\", h1)\n",
    "        h2 = layer(h1 , h1.shape[1] , hsize[0] , \"layer1\" , belu , True , False , SN = False ,  Type = \"Batch_Normalization\" , phase = phase )\n",
    "        tf.summary.histogram(\"G_Layer1\", h1)\n",
    "        #h2 = dense_batch_relu(h1  ,hsize[1],  Relu2 , phase ,'layer1')\n",
    "#         h2 = layer(h1 , h1.shape[1] , hsize[1] , \"layer2\" , belu , True , False , SN = False ,  Type = \"Batch_Normalization\" ,phase = phase)\n",
    "#         tf.summary.histogram(\"G_Layer2\", h2)\n",
    "        #h3 = dense_batch_relu(h2  ,hsize[2],  belu , phase ,'layer2')\n",
    "        h4 = layer(h2 , h2.shape[1] , hsize[2] , \"layer3\" , belu , True , False , SN = False ,  Type = \"Batch_Normalization\" ,phase = phase )\n",
    "        tf.summary.histogram(\"G_Layer3\", h4)\n",
    "#         h4 = layer(h3 , h3.shape[1] , hsize[3] , name =  \"layer4\" , activation = belu  ,\n",
    "#                    usebias = True ,  final = False , SN = False ,  Type = \"Batch_Normalization\" , phase = phase)\n",
    "#         tf.summary.histogram(\"G_Layer4\", h4)\n",
    "#       \n",
    "        #h5 = dense_batch_relu(h4  ,hsize[4],  belu , phase ,'layer4')\n",
    "        h5 = layer(h4 , h4.shape[1] , hsize[4] , \"layer5\" , belu , \n",
    "                   usebias = True ,  final = False , SN = False ,  Type = \"Batch_Normalization\" , phase = phase\n",
    "                  )\n",
    "        tf.summary.histogram(\"G_Layer5\", h5)\n",
    "#         out = nalu_v2(h5 ,total_length )\n",
    "#         print(out)\n",
    "        out = tf.layers.dense(h5,  total_length , activation = None , use_bias= True )\n",
    "        #out = nalu(out , total_length , epsilon=1e-7, name=\"nalu_total\", reuse=reuse)\n",
    "        tf.summary.histogram(\"G_Final\", out)\n",
    "        num = tf.slice(out , [0,0] , [batch_size,  len(numeric_name) ] )  # \n",
    "        tf.summary.histogram(\"G_Numeric\", num)\n",
    "        first = tf_sqnl(num)\n",
    "        ## Categorcial \n",
    "        NUM = 0\n",
    "        for i in object_name :\n",
    "            cat = tf.slice(out , [0,len(numeric_name) + NUM ] , [batch_size , info.get(i)] ) # \n",
    "            tf.summary.histogram(\"G_\" + i , cat )\n",
    "            Species = tf.nn.softmax(cat)\n",
    "            NUM += info.get(i)\n",
    "            first = tf.concat([first , Species], axis = 1 , name = i[0] )\n",
    "        #out = tf.layers.dense(num ,input_dim , activation = tf_sqnl , use_bias=False , name = 'Numerical' )  # activation=None \n",
    "    return first\n",
    "\n",
    "\n",
    "def discriminator(X, latent_code , hsize=[750 , 500 , 300 , 200, 100],reuse=True):\n",
    "    with tf.variable_scope(\"BiGAN/Discriminator\",reuse=reuse):\n",
    "        latent_fc = layer(latent_code  , latent_code.shape[1] , X.shape[1], name =  \"latent_layer1\" , activation = tf.nn.leaky_relu ,\n",
    "                          usebias = True ,  final = False , SN = False ,  Type = \"Batch_Normalization\" , phase = phase)\n",
    "        X = tf.concat([X , latent_fc] , axis = 1)\n",
    "        h1 = layer(X  , X.shape[1] , hsize[0] , name =  \"D_layer1\" , activation = tf.nn.leaky_relu ,\n",
    "               usebias = True ,  final = False , SN = True ,  Type = \"Batch_Normalization\" , phase = phase)\n",
    "        h2 = layer(h1  , h1.shape[1] , hsize[1] , name =  \"D_layer2\" , activation = tf.nn.leaky_relu ,\n",
    "               usebias = True ,  final = False , SN = True ,  Type = \"Batch_Normalization\" , phase = phase)\n",
    "        #h2 = tf.layers.dense(h1,hsize[1],activation=tf.nn.leaky_relu , name = \"D2\" )\n",
    "        #h2 = minibatch(h2 , name = \"D2\")\n",
    "        h2 = layer(h2  , h2.shape[1] , hsize[2] , name =  \"D_layer3\" , activation =  tf.nn.leaky_relu ,\n",
    "               usebias = True ,  final = False , SN = True ,  Type = \"Batch_Normalization\" , phase = phase)\n",
    "        #h2 = tf.layers.dense(h2,hsize[2],activation=tf.nn.leaky_relu , name = \"D3\" )\n",
    "        #h2 = minibatch(h2 , name = \"D3\")\n",
    "        h2 = layer(h2  , h2.shape[1] , hsize[3] , name =  \"D_layer4\" , activation = tf.nn.leaky_relu ,\n",
    "               usebias = True ,  final = False , SN = True ,  Type = \"Batch_Normalization\" , phase = phase)\n",
    "        #h2 = minibatch(h2 , name = \"D4\")\n",
    "        h2 = layer(h2  , h2.shape[1] , hsize[4] , name =  \"D_layer5\" , activation = tf.nn.leaky_relu ,\n",
    "               usebias = True ,  final = False , SN = True ,  Type = \"Batch_Normalization\" , phase = phase)        \n",
    "        h3 = layer(h2  , h2.shape[1] , 100 , name =  \"D_layer6\" , activation = tf.nn.leaky_relu ,\n",
    "               usebias = True ,  final = False , SN = True ,  Type = \"Batch_Normalization\" , phase = phase)\n",
    "        h3 = minibatch(h3 , num_kernels = 50 ,  bs = BATCH_SIZE ,name = \"Minibatch_Discrimination\")\n",
    "        #h3 = tf.layers.dense(h2,10 ,activation=tf.nn.leaky_relu , name = \"D4\" )\n",
    "        #h3 = minibatch(h3 , name = \"D3\")\n",
    "        out = tf.layers.dense(h3,1) # activation=None OUTPUT\n",
    "    return out, h3\n",
    "\n",
    "def sample_Z(m , n ):\n",
    "    Uniform = np.random.uniform(-1., 1., size=[m , n])\n",
    "    #Normal = np.random.normal(loc = 0. ,  scale = 1. , size=[m , n])\n",
    "    return Uniform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500000\n",
    "BATCH_SIZE = 100\n",
    "data_len = np.shape(data)[0]\n",
    "batch_iter = int(data_len / BATCH_SIZE)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, Z)).batch(batch_size , drop_remainder=True ).repeat().shuffle(1000)\n",
    "iter = dataset.make_initializable_iterator()\n",
    "x , z = iter.get_next()\n",
    "\n",
    "print(batch_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_sample = generator(z , batch_size)\n",
    "latent = Encoder(x , reuse = False)\n",
    "r_logits, r_rep = discriminator(x, latent , reuse=False)\n",
    "f_logits, g_rep = discriminator(G_sample, z , reuse=True)\n",
    "## \n",
    "Recon_latent = Encoder(x , is_training=False , reuse = True)\n",
    "G_sample_DATA = generator(Recon_latent , batch_size , reuse = True )\n",
    "\n",
    "# e = tf.random_uniform([BATCH_SIZE , 1] , 0 , 1)\n",
    "# x_hat =  e * x + (1-e) * G_sample\n",
    "# grad  = tf.gradients( discriminator(x_hat,reuse=True ), x_hat)[0]\n",
    "\n",
    "#grad_norm = tf.norm( tf.layers.flatten(grad) , axis = 1)\n",
    "#LP = 10 * tf.reduce_mean( tf.square( tf.maximum(0.0 ,grad_norm - 1.0)))\n",
    "\n",
    "\n",
    "# slopes = tf.sqrt(1e-8 + tf.reduce_sum(tf.square(grad), axis=[1]))\n",
    "# gradient_penalty = 5 * tf.reduce_mean((slopes - 1.) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moment(x = None , nth = 3) :\n",
    "    \"\"\"Column 별 3차 모멘트 계산\"\"\"\n",
    "    std = tf.math.reduce_std(x , axis = 0) \n",
    "    std = tf.clip_by_value(std , 0.001 , 20.0 ) \n",
    "    standard = tf.div(x - tf.reduce_mean(x, axis = 0) ,  std ) + 1e-10\n",
    "    pow_3 = tf.pow(standard ,nth )\n",
    "    three_moment = tf.reduce_mean(pow_3 , axis = 0)\n",
    "    return three_moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake_logit = (f_logits - tf.reduce_mean(r_logits))\n",
    "# real_logit = (r_logits - tf.reduce_mean(f_logits))\n",
    "# real_logit = tf.sigmoid( real_logit )\n",
    "# fake_logit = tf.sigmoid( fake_logit )\n",
    "# disc_loss = - tf.reduce_mean( log(real_logit)) - tf.reduce_mean( log(1-fake_logit ))\n",
    "# gen_loss = - tf.reduce_mean( log( fake_logit )) - tf.reduce_mean( log(1- real_logit ))\n",
    "\n",
    "\n",
    "# disc_loss = tf.reduce_mean(\n",
    "#     tf.nn.sigmoid_cross_entropy_with_logits(logits=r_logits,labels=tf.ones_like(r_logits)) + \n",
    "#     tf.nn.sigmoid_cross_entropy_with_logits(logits=f_logits,labels=tf.zeros_like(f_logits)) , name = \"discriminator_loss\")\n",
    "# gen_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=f_logits,labels=tf.ones_like(f_logits)), name= \"generator_loss\")\n",
    "\n",
    "## wgan Loss 변경\n",
    "### 이유 : 이전에는 이미지다보니 2D 전체를 하는 것이 맞지만, 테이블 데이터 같은 경우에는 열별로 비교해야할듯하다.\n",
    "### 이유2 : 전체적인 것도 고려할 수 있게하기! \n",
    "e = tf.random_uniform([BATCH_SIZE , 1] , 0 , 1)\n",
    "x_hat =  e * x + (1-e) * G_sample\n",
    "latent_x_hat =  e * z + (1-e) * latent\n",
    "grad  = tf.gradients( discriminator(x_hat, latent_x_hat , reuse=True ), x_hat)[0]\n",
    "slopes = tf.sqrt(1e-8 + tf.reduce_sum(tf.square(grad), axis=[1]))\n",
    "gradient_penalty = 5 * tf.reduce_mean((slopes - 1.) ** 2)\n",
    "\n",
    "fake_logit = (f_logits - tf.reduce_mean(r_logits))\n",
    "real_logit = (r_logits - tf.reduce_mean(f_logits))\n",
    "real_logit = tf.sigmoid( real_logit )\n",
    "fake_logit = tf.sigmoid( fake_logit )\n",
    "\n",
    "with tf.variable_scope(\"Discriminator_Loss\") :        \n",
    "#     total_disc_loss = tf.reduce_mean(f_logits) - tf.reduce_mean(r_logits)     \n",
    "#     col_disc_loss = tf.reduce_sum(tf.reduce_mean(f_logits , axis = 0 ) - tf.reduce_mean(r_logits , axis = 0  ))\n",
    "#     ratio = 1.0\n",
    "#     disc_loss = ratio * total_disc_loss + (1-ratio) * col_disc_loss \n",
    "    disc_loss = - tf.reduce_mean( log(real_logit)) - tf.reduce_mean( log(1-fake_logit ))\n",
    "    disc_loss += gradient_penalty\n",
    "\n",
    "## 기존 wgan loss\n",
    "#disc_loss = tf.reduce_mean(f_logits  ) - tf.reduce_mean(r_logits  ) + gradient_penalty\n",
    "#gen_loss  = - tf.reduce_mean(f_logits)\n",
    "\n",
    "\n",
    "## Numeric 변수에 대한 Correlation Loss 추가\n",
    "### 이유는 상관관계가 유사하게 나와야 하기 때문에 규제를 주는 역할을 기대할 수 있다!\n",
    "### Generator Loss에 넣어서 Generator에서만 Weight 조절할수 있게! \n",
    "def tf_cov(x):\n",
    "    mean_x = tf.reduce_mean(x, axis=0, keep_dims=True)\n",
    "    mx = tf.matmul(tf.transpose(mean_x), mean_x)\n",
    "    vx = tf.matmul(tf.transpose(x), x)/tf.cast(tf.shape(x)[0], tf.float32)\n",
    "    cov_xx = vx - mx\n",
    "    return cov_xx\n",
    "def kl_divergence_gaussians(q_mu, q_sigma, p_mu, p_sigma) :\n",
    "    r = q_mu - p_mu\n",
    "    return tf.reduce_sum( log(p_sigma) - log(q_sigma) - .5 * (1. - (q_sigma**2 + r**2) / p_sigma**2), axis=-1)\n",
    "\n",
    "def Differ_Round(x) :\n",
    "    ## https://stackoverflow.com/questions/46596636/differentiable-round-function-in-tensorflow\n",
    "    differentiable_round = tf.maximum(x-0.499,0)\n",
    "    differentiable_round = differentiable_round * 10000\n",
    "    differentiable_round = tf.minimum(differentiable_round, 1) + 1e-20\n",
    "    return differentiable_round\n",
    "\n",
    "with tf.variable_scope(\"Generator_Loss\"):\n",
    "    with tf.variable_scope(\"Original_Loss\"):\n",
    "        gen_loss = - tf.reduce_mean( log( fake_logit )) - tf.reduce_mean( log(1- real_logit ))\n",
    "        #gen_loss  = - tf.reduce_mean(f_logits)\n",
    "        # -tf.reduce_sum(tf.reduce_mean(f_logits , axis = 0 ))\n",
    "    with tf.variable_scope(\"Corr_Loss\"):\n",
    "        num_G = tf.slice(G_sample , [0,0] , [batch_size,  len(numeric_name) ] )  # \n",
    "        num_R = tf.slice(x , [0,0] , [batch_size,  len(numeric_name) ] )  # \n",
    "        matrix = tf_cov(num_G) - tf_cov(num_R)\n",
    "        Upper_triangular = tf.matrix_band_part(matrix, 0, -1)\n",
    "        corr_loss = tf.reduce_sum(tf.square( Upper_triangular ))        \n",
    "    tf.summary.scalar(\"Corr_loss\", corr_loss)\n",
    "    gen_loss += 1 * corr_loss\n",
    "    with tf.variable_scope(\"wasserstein_Loss\"):\n",
    "        num_G = tf.slice(G_sample , [0,0] , [batch_size,  len(numeric_name) ] )  # \n",
    "        cat_G = tf.slice(G_sample , [0, len(numeric_name)] , [batch_size,  -1 ] )  # \n",
    "        Round_G = Differ_Round(cat_G)\n",
    "        Processing_G = tf.concat([num_G , Round_G] , axis = 1 )\n",
    "        wasserstein = Wasserstein(Processing_G  , x , BATCH_SIZE)\n",
    "        loss = wasserstein.dist(C=0.1, nsteps=10)\n",
    "    tf.summary.scalar(\"wasserstein_loss\", loss)\n",
    "    gen_loss += loss\n",
    "    with tf.variable_scope(\"Stat\"):\n",
    "        Gcol_mean , Gcol_var = tf.nn.moments(G_sample , axes = [0])\n",
    "        Xcol_mean , Xcol_var = tf.nn.moments(x , axes = [0])\n",
    "        G_mean , G_var = tf.nn.moments(G_sample , axes = [0,1])\n",
    "        X_mean , X_var = tf.nn.moments(x , axes = [0,1])\n",
    "        Colstat = tf.reduce_mean(tf.square(Gcol_mean - Xcol_mean)) \\\n",
    "        + tf.reduce_mean(tf.square( tf.sqrt(Gcol_var + 1e-10) - tf.sqrt(Xcol_var + 1e-10 )))\n",
    "        Totalstat = tf.reduce_mean(tf.square(G_mean - X_mean)) \\\n",
    "        + tf.reduce_mean(tf.square( tf.sqrt(G_var + 1e-10 ) - tf.sqrt(X_var + 1e-10 )))\n",
    "        ## 추가 빈도 비율이 유사하게 되지 않을까!?\n",
    "    with tf.variable_scope(\"Categorical_Ratio\"):\n",
    "        cat_G = tf.slice(G_sample , [0, len(numeric_name)] , [batch_size,  -1 ] )  # \n",
    "        cat_R = tf.slice(x , [0,  len(numeric_name)] , [batch_size,  -1] )  # \n",
    "        Round_G = tf.reduce_sum(Differ_Round(cat_G) , axis = 0 )\n",
    "        Round_R = tf.reduce_sum(Differ_Round(cat_R) , axis = 0 )\n",
    "        D = tf.shape(cat_R)[1]\n",
    "        q_sigma = tf.ones(D) * 2.0\n",
    "        p_sigma = tf.ones(D) * 2.0\n",
    "        ## 너무 Loss가 폭발해서 sum -> mean으로 수정 \n",
    "        KL_LOSS = tf.reduce_mean(tf.distributions.kl_divergence(\n",
    "            tf.distributions.Normal(loc=Round_G, scale=q_sigma),\n",
    "            tf.distributions.Normal(loc=Round_R, scale=p_sigma)), axis=-1) * 0.5 \n",
    "        \n",
    "        \"\"\"\n",
    "        제한은 주지만, 너무 Loss가 크게 의존하지는 않게,\n",
    "        \"\"\"\n",
    "        Cat_Ratio = tf.reduce_mean( tf.square(Round_G - Round_R) ) * 0.5\n",
    "        normalize_a = tf.nn.l2_normalize(Round_G,0)        \n",
    "        normalize_b = tf.nn.l2_normalize(Round_R,0)\n",
    "        cos_similarity=tf.reduce_sum(tf.multiply(normalize_a,normalize_b))\n",
    "        Similarity_Loss = 1 * cos_similarity\n",
    "    tf.summary.scalar(\"Categorical_Ratio_loss\", Cat_Ratio)\n",
    "    tf.summary.scalar(\"Colstat_loss\", Colstat)\n",
    "    tf.summary.scalar(\"Totalstat_loss\", Totalstat)\n",
    "    tf.summary.scalar(\"Similarity_loss\", Similarity_Loss)\n",
    "    tf.summary.scalar(\"Categorical_KL_loss\", KL_LOSS)\n",
    "    gen_loss +=  Totalstat + Cat_Ratio + Similarity_Loss + KL_LOSS \n",
    "    with tf.variable_scope(\"Moment\"):\n",
    "        num_G = tf.slice(G_sample , [0,0] , [batch_size,  len(numeric_name) ] )  # \n",
    "        num_R = tf.slice(x , [0,0] , [batch_size,  len(numeric_name) ] )  # \n",
    "        Diff_3 = moment(num_G,3) - moment(num_R,3)\n",
    "        three_moment_diff = tf.reduce_sum(tf.square(Diff_3)) * 0.1\n",
    "        Diff_4 = moment(num_G,4) - moment(num_R,4)\n",
    "        ## 너무 강하게 자극을 받는 듯 수정 필요 \n",
    "        #four_moment_diff = tf.reduce_sum(tf.square(Diff_4))\n",
    "        four_moment_diff = tf.reduce_mean(tf.square(Diff_4)) * 0.1\n",
    "    tf.summary.scalar(\"3th_moment\", three_moment_diff)\n",
    "    tf.summary.scalar(\"4th_moment\", four_moment_diff)\n",
    "    gen_loss += three_moment_diff + four_moment_diff\n",
    "    # Colstat +\n",
    "    \n",
    "Recon = tf.reduce_mean(tf.square(z- Recon_latent))\n",
    "gen_loss += Recon\n",
    "\n",
    "## BATCH NORMALIZATION LOSS \n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "if update_ops != [] : \n",
    "    updates = tf.group(*update_ops)\n",
    "    gen_loss = control_flow_ops.with_dependencies([updates], gen_loss)\n",
    "\n",
    "with tf.variable_scope(\"Loss\"):\n",
    "    tf.summary.scalar(\"Dloss\", disc_loss)\n",
    "    tf.summary.scalar(\"Gloss\", gen_loss)\n",
    "\n",
    "t_vars = tf.trainable_variables()\n",
    "\n",
    "for var in t_vars :\n",
    "    tf.summary.histogram(var.op.name, var)\n",
    "\n",
    "global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)\n",
    "gen_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"GAN/Generator\") + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='BiGAN/Encoder')\n",
    "disc_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"BiGAN/Discriminator\")\n",
    "lr = 0.0005\n",
    "learning_rate = tf.train.exponential_decay(lr, global_step, decay_steps=100, decay_rate=0.9998, staircase=True , )\n",
    "\n",
    "\n",
    "tf.summary.scalar(\"learning_rate\", learning_rate)\n",
    "with tf.variable_scope(\"Optimizer\"):\n",
    "    gen_step = tf.train.AdamOptimizer(learning_rate=learning_rate ,\n",
    "                                      beta1=0.9, beta2 = 0.99 ).minimize(gen_loss,var_list = gen_vars) # G Train step\n",
    "    disc_step = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(disc_loss,var_list = disc_vars) # D Train step\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tensorboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "config=tf.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "merged_summary = tf.summary.merge_all()\n",
    "sess = tf.Session(config= config)\n",
    "saver = tf.train.Saver()\n",
    "suumary_dir = \"/home/advice/Python/SR/board/Gan_Pokemon_bigan_2\"\n",
    "img_dir = \"./Gan_Pokemon_bigan_2\"\n",
    "try :\n",
    "    if tf.gfile.Exists(suumary_dir):\n",
    "        tf.gfile.DeleteRecursively(suumary_dir)\n",
    "        tf.gfile.MakeDirs(suumary_dir)\n",
    "        tf.gfile.DeleteRecursively(img_dir)\n",
    "        tf.gfile.MakeDirs(img_dir)\n",
    "except Exception as e :\n",
    "    tf.gfile.MakeDirs(img_dir)\n",
    "tf.gfile.MakeDirs(img_dir)\n",
    "    \n",
    "writer = tf.summary.FileWriter(suumary_dir)\n",
    "writer.add_graph(sess.graph )\n",
    "tf.global_variables_initializer().run(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False\n",
    "if load_model == True :\n",
    "    print(\"학습된 모델 사용하기\")\n",
    "    save_file = \"./savemodel/gan.meta\"\n",
    "    saver = tf.train.import_meta_graph(save_file)\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('./savemodel/'))\n",
    "else :\n",
    "    print(\"처음부터 학습시키기\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = data.copy()\n",
    "transform = original.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "onehot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
    "##\n",
    "TOTAL = data[:,:len(numeric_name)]\n",
    "onehot_bin = []\n",
    "value2code = {}\n",
    "code2value = {}\n",
    "for i in range(len(numeric_name) , np.shape(data)[1]) :\n",
    "    print(feature[i])\n",
    "    label_encoder.fit(data[:,i])\n",
    "    keys = label_encoder.classes_.tolist()\n",
    "    value2code[feature[i]] = dict(zip(keys , np.arange(len(keys))))\n",
    "    code2value[feature[i]] = dict(zip(np.arange(len(keys)), keys))\n",
    "    encoding = label_encoder.transform(data[:,i])\n",
    "    onehot_encoder.fit(np.array(encoding).reshape(-1,1))\n",
    "    cat = onehot_encoder.transform(np.array(encoding).reshape(-1,1))\n",
    "    onehot_bin.append(TOTAL.shape[1])\n",
    "    before = TOTAL.shape[1]\n",
    "    TOTAL = np.concatenate((TOTAL , cat) , axis = 1 )\n",
    "    print(\"{} 위치[{}] : {}:{}\".format(feature[i], i , before  , TOTAL.shape[1] +1 ))\n",
    "    transform[:,i] = label_encoder.transform(transform[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= TOTAL\n",
    "del TOTAL\n",
    "onehot_bin += [data.shape[1]]\n",
    "print(onehot_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(np.argmax(data[:,995:1002] , axis = 1)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = sample_data(n= data.shape[0] , data = transform ).astype(float) # len(data)\n",
    "x_plot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_plot2 = x_plot[:,:len(numeric_name)].copy()\n",
    "# for idx , i in enumerate(onehot_bin[:-1]) :\n",
    "#     x_plot2 = np.concatenate( (x_plot2 ,\n",
    "#                                np.argmax(x_plot[:, onehot_bin[idx] : onehot_bin[idx+1] ] , 1 )[:,np.newaxis] ) , axis = 1 )\n",
    "# x_plot = x_plot2\n",
    "# del x_plot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Numeric NAME : {}\".format(numeric_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance as was_dist\n",
    "nd_steps = 1\n",
    "ng_steps = 1\n",
    "save_model = True\n",
    "scaler = MinMaxScaler(feature_range=(-1. ,1.))\n",
    "## 표준화로 해보기!\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#scaler = StandardScaler()\n",
    "data[:,:len(numeric_name)] = scaler.fit_transform(data[:,:len(numeric_name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Onehot_shape = data[: , len(numeric_name) : ].shape\n",
    "true_idx = data[: , len(numeric_name) : ] > 0 \n",
    "random_init = np.random.uniform(low = 0.0001 , high = 0.001 , size = Onehot_shape)\n",
    "data[: , len(numeric_name) : ] =\\\n",
    "data[: , len(numeric_name) : ] * true_idx +random_init * (~true_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## epsilon 추가\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gan_Vis(g_plot , x_plot , total_dist , i , dloss ,gloss  ) : \n",
    "    clear_output(wait= True)\n",
    "    fig, ax = plt.subplots(4,5, figsize=(18, 12))\n",
    "    fig.subplots_adjust(hspace = 0.35 , wspace= 0.14 , top = 0.92 , left = 0.03 , bottom = 0.04 , right = 0.99)\n",
    "    axx = ax.flatten()\n",
    "    num = 0\n",
    "\n",
    "    ## total 변수 \n",
    "    total_var = 0\n",
    "    g_total_var = 0\n",
    "    \n",
    "    for n , name in enumerate(feature) :\n",
    "    \n",
    "        \"\"\"\n",
    "        bin count 비교\n",
    "        \"\"\"\n",
    "        if name in object_name  :\n",
    "            print(name)\n",
    "            print(\"gene -----------\")\n",
    "            print(np.bincount( np.array(g_plot[:,n]).astype(int) ) )\n",
    "            print(\"true -----------\")\n",
    "            print(np.bincount( np.array(x_plot[:,n]).astype(int) ) )\n",
    "            print(\"-----------------\")\n",
    "        \"\"\"\n",
    "        Visualization\n",
    "        \"\"\"\n",
    "        ## object \n",
    "        if name in object_name  :\n",
    "            binsize = len(np.unique(x_plot[:,n].astype(float) ))\n",
    "            if name == \"percent-female\" :\n",
    "                Real_MALE = [code2value[name][letter] for letter in x_plot[:,n].astype(float) ]\n",
    "                Gene_MALE = [code2value[name][letter] for letter in g_plot[:,n].astype(float) ]\n",
    "            sns.distplot(x_plot[:,n].astype(float) , kde = False ,  \n",
    "                         hist_kws ={\"color\":\"r\", \"label\" : \"Real\"} ,  \n",
    "                         bins = binsize ,\n",
    "                         rug = True , ax = axx[num])\n",
    "            sns.distplot(g_plot[:,n],  kde = False ,  \n",
    "                         hist_kws ={\"color\":\"g\" , \"label\" : \"Gene\"} ,\n",
    "                         bins = binsize , \n",
    "                         rug = True   , ax = axx[num])\n",
    "        ## numeric\n",
    "        else :\n",
    "            ## numeric_int\n",
    "            if name in Int_name  : \n",
    "                Real  = x_plot[:,n].astype(float).round(0)\n",
    "                Fake  = g_plot[:,n].round(0)\n",
    "                binsize= len(np.unique(Real))\n",
    "                if name in ['hp', 'attack', 'defense', 'spattack', 'spdefense', 'speed'] :\n",
    "                    total_var += Real\n",
    "                    g_total_var += Fake\n",
    "                sns.distplot(Real  , kde_kws ={\"color\": \"r\", \"label\" : \"Real\"} , bins = binsize ,\n",
    "                             hist_kws ={\"color\":\"r\"}, ax = axx[num])\n",
    "                sns.distplot(Fake , kde_kws ={\"color\": \"g\", \"label\" : \"Gene\"} , bins = binsize ,\n",
    "                             hist_kws ={\"color\":\"g\"}, ax = axx[num])\n",
    "            ## numeric_float\n",
    "            elif name in float_name :\n",
    "                Real  = x_plot[:,n].astype(float)\n",
    "                Fake  = g_plot[:,n]\n",
    "                ## 사후 전처리 \n",
    "                sns.distplot(Real  , kde_kws ={\"color\": \"r\", \"label\" : \"Real\"} , hist_kws ={\"color\":\"r\"}, ax = axx[num])\n",
    "                sns.distplot(Fake , kde_kws ={\"color\": \"g\", \"label\" : \"Gene\"} , hist_kws ={\"color\":\"g\"}, ax = axx[num])\n",
    "        \n",
    "        axx[num].set_title(\"{}\".format(name))\n",
    "        num +=1\n",
    "    for name_2 in [\"total\", \"percent-male\"] :\n",
    "        if name_2 == \"total\" :\n",
    "            sns.distplot(total_var  , kde_kws ={\"color\": \"r\", \"label\" : \"Real\"} , hist_kws ={\"color\":\"r\"}, ax = axx[num])\n",
    "            sns.distplot(g_total_var , kde_kws ={\"color\": \"g\", \"label\" : \"Gene\"} , hist_kws ={\"color\":\"g\"}, ax = axx[num])\n",
    "        elif name_2 == \"percent-male\" :\n",
    "            Real_ = [replace_gender[letter] for letter in Real_MALE ]\n",
    "            Gene_ = [replace_gender[letter] for letter in Gene_MALE ]\n",
    "            \n",
    "            \n",
    "            Real_ = [ male if male == \"NULL\" else str(1-male.astype(float)) for male in Real_ ]\n",
    "            \n",
    "            Gene_ = [ male if male == \"NULL\" else str(1-male.astype(float)) for male in Gene_ ]\n",
    "            print(\"Gene----\")\n",
    "            print(collections.Counter(Gene_))\n",
    "            print(\"Real----\")\n",
    "            print(collections.Counter(Real_))\n",
    "            label_encoder.fit(Real_)\n",
    "            R_Male = label_encoder.transform(Real_ )\n",
    "            G_Male = label_encoder.transform(Gene_ )\n",
    "            binsize = len(np.unique(R_Male.astype(float) ))\n",
    "            sns.distplot(R_Male.astype(float) , kde = False ,  \n",
    "                         hist_kws ={\"color\":\"r\", \"label\" : \"Real\"} ,  \n",
    "                         bins = binsize ,\n",
    "                         rug = True , ax = axx[num])\n",
    "            sns.distplot(G_Male ,  kde = False ,  \n",
    "                         hist_kws ={\"color\":\"g\" , \"label\" : \"Gene\"} ,\n",
    "                         bins = binsize , \n",
    "                         rug = True   , ax = axx[num])\n",
    "        axx[num].set_title(\"{}\".format(name_2))\n",
    "        num +=1\n",
    "    \n",
    "    plt.suptitle('Iteration %d , D_loss : %.4f , G_loss : %.4f , Dist : %.4f'%(i,dloss,gloss, total_dist) )\n",
    "    \n",
    "    img_dir_path = img_dir + '/iteration_%d.png'%i\n",
    "    plt.savefig(img_dir_path)\n",
    "    plt.close()\n",
    "    return img_dir_path\n",
    "\n",
    "\n",
    "\n",
    "def Iter_History(Total_ws = None , i = None ,save_dir = './Pokemon_Dist_v5.png' ) :    \n",
    "    fig , ax = plt.subplots(figsize = (26,13))\n",
    "    fig.subplots_adjust(top = 0.95 , left = 0.03 , bottom = 0.04 , right = 0.99)\n",
    "    ax.plot(Total_ws[\"iter\"] , Total_ws[\"ws\"] , linestyle =\"-\" , marker =\".\" , linewidth = 3, markersize = 12)\n",
    "    ax.plot(Total_ws[\"iter\"] , Total_ws[\"ws\"]-1.96*Total_ws[\"wstd\"] , linestyle =\"dashed\"  , linewidth = 1)\n",
    "    ax.plot(Total_ws[\"iter\"] , Total_ws[\"ws\"]+1.96*Total_ws[\"wstd\"] , linestyle =\"dashed\"  , linewidth = 1)\n",
    "    try :\n",
    "        ax.set_title(\"EPOCH : {} , WS : {:.3f}[{:.3f}]\".format(i , Total_ws[\"ws\"].tail(1).values[0] , Total_ws.ws.min()) , fontsize = 30)\n",
    "        ax.axhline(y = Total_ws[\"ws\"].min() , color='r', linestyle='-')\n",
    "        minws = Total_ws[\"ws\"].min()\n",
    "        miniter = Total_ws.loc[Total_ws[\"ws\"] == minws , \"iter\"].values[0]\n",
    "        print(miniter , minws )\n",
    "        ax.text(Total_ws.iter.max()/2 , minws  , \"EPOCH : {} , Min : {:.2f}\".format(miniter  , minws ) , \n",
    "                fontsize=30, va='center', ha='center', backgroundcolor='w')\n",
    "        plt.savefig(save_dir)\n",
    "        plt.show()\n",
    "    except Exception as e :\n",
    "        print(e)\n",
    "        pass\n",
    "    \n",
    "def Columns_Dist(ws_output , total_dist , Total_ws , save_dir , i ) : \n",
    "\tfig , ax = plt.subplots(figsize=(26,13))\n",
    "\tfig.subplots_adjust(top = 0.95 , left = 0.03 , bottom = 0.05 , right = 0.99)\n",
    "\tupdown = 0\n",
    "\tfor name in feature : \n",
    "\t    if updown % 2 == 0 :\n",
    "\t        param , space=\"bottom\" , \"  \"\n",
    "\t    else : \n",
    "\t        param , space =\"top\" , \"   \"\n",
    "\t    ax.plot(ws_output.iter , ws_output[[name]], label = name)\n",
    "\t    ax.text(i , ws_output.loc[ws_output[\"iter\"]==i , [name]].values , space + name ,\n",
    "\t            verticalalignment = param)\n",
    "\t    updown +=1\n",
    "\tax.set_title(\"WS [{}]\".format(total_dist) , fontsize = 30 )\n",
    "\tax.set_xlabel(\"Epoch\")\n",
    "\tax.set_ylabel(\"WS\")    \n",
    "\tbox = ax.get_position()\n",
    "\tax.set_position([box.x0, box.y0 + box.height * 0.1, box.width, box.height * 0.9])\n",
    "\tax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True, shadow=True, ncol=15 , fontsize= 10)\n",
    "\tax.text(i + 1 , 0.05 , \"   0.05\", verticalalignment = param)\n",
    "\tax.axhline(0.05, linewidth=4, color='r')\n",
    "\tax.set_title(\"EPOCH : {} , WS : {:.2f}[{:.2f}]\".format(i  , total_dist , Total_ws.ws.min()  ) , fontsize = 30)\n",
    "\tplt.savefig( save_dir )\n",
    "\tplt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() :\n",
    "    imgs = []\n",
    "    Total_ws = pd.DataFrame({\"iter\":[0] , \"ws\" :[0], \"wstd\" : [0]})\n",
    "    ws_output = pd.DataFrame([[0]+[1.]*len(feature)], columns = [\"iter\"]+feature)\n",
    "    for i in range(EPOCHS) : \n",
    "        z_batch = sample_Z(data_len , latent_dim )\n",
    "        sess.run(iter.initializer, feed_dict={ X : data , \n",
    "                                              batch_size: BATCH_SIZE , \n",
    "                                              Z : z_batch  ,\n",
    "                                              phase : 1\n",
    "                                             })\n",
    "        #for _ in range( batch_iter)  :\n",
    "        nd_steps = 5\n",
    "        for _ in range(nd_steps):\n",
    "            _, dloss  = sess.run([disc_step, disc_loss] , feed_dict={global_step : i ,\n",
    "                                                                     batch_size : BATCH_SIZE ,\n",
    "                                                                     phase : 1\n",
    "                                                                    })\n",
    "        #writer.add_summary(summary, global_step=i)\n",
    "        ng_steps = 1\n",
    "        for _ in range(ng_steps):\n",
    "            _, gloss = sess.run([gen_step, gen_loss], feed_dict={global_step : i , \n",
    "                                                                 batch_size : BATCH_SIZE ,\n",
    "                                                                 phase : 1\n",
    "                                                                })\n",
    "        ### 채찍을 가하기\n",
    "        if (i > 100)  and ( i % 20 == 0 ) : \n",
    "            setting = 50 if dloss < 50 else 2000 if dloss > 100 else 1000\n",
    "            print(\"D : {:.2f} , G : {:.2f}\".format(dloss , gloss))\n",
    "            goal = 1.0 if dloss < 20 else dloss / 5\n",
    "            iter_d = 0\n",
    "            while dloss > goal :\n",
    "                _, dloss  = sess.run([disc_step, disc_loss] , \n",
    "                                     feed_dict={global_step : i ,\n",
    "                                                batch_size : BATCH_SIZE ,\n",
    "                                                phase : 1\n",
    "                                               })\n",
    "                \n",
    "                iter_d += 1\n",
    "                ## 너무 과하게는 하지 않기 \n",
    "                if iter_d > setting :\n",
    "                    break\n",
    "            \n",
    "            iter_g = 0\n",
    "            setting = 500 if gloss < 50 else 1000 if gloss > 100 else 750\n",
    "            goal = 1.0 if gloss < 20 else gloss / 5\n",
    "            while gloss > goal :\n",
    "                _, gloss = sess.run([gen_step, gen_loss], \n",
    "                                    feed_dict={\n",
    "                                        global_step : i , \n",
    "                                        batch_size : BATCH_SIZE ,\n",
    "                                        phase : 1 \n",
    "                                              })\n",
    "                iter_g += 1\n",
    "                ## 너무 과하게는 하지 않기 \n",
    "                if iter_g > setting :\n",
    "                    break\n",
    "        \n",
    "            print(\"After : D : {:.2f} , G : {:.2f}\".format(dloss , gloss))    \n",
    "        #writer.add_summary(summary, global_step = i)\n",
    "        \"\"\"\n",
    "        Distance Measure\n",
    "        \"\"\"\n",
    "        generate_n = data.shape[0]\n",
    "        z_batch = sample_Z(generate_n  , latent_dim )\n",
    "        ## 샘플개수만큼 여기서는 다 사용해서 x : data가 가능했음 \n",
    "        g_plot = sess.run(G_sample_DATA , feed_dict = {x : data , z : z_batch , batch_size : generate_n , phase : 0})\n",
    "        g_plot[:,:len(numeric_name)] = scaler.inverse_transform(g_plot[:,:len(numeric_name)])\n",
    "        g_numeric = g_plot[:,:len(numeric_name)]\n",
    "        for idx , _ in enumerate(onehot_bin[:-1]) :\n",
    "            cat = np.argmax(g_plot[:, onehot_bin[idx] : onehot_bin[idx+1] ], 1 )\n",
    "            g_numeric = np.concatenate((g_numeric ,cat[:,np.newaxis] ) , axis = 1 )\n",
    "        g_plot = g_numeric\n",
    "        del g_numeric\n",
    "        total_dist = 0 \n",
    "        dist_set = []\n",
    "        for n , name in enumerate(feature) :\n",
    "            try :\n",
    "                DIST = was_dist(x_plot[:,n], g_plot[:,n])\n",
    "                total_dist += DIST\n",
    "                dist_set.append(DIST)\n",
    "            except Exception as e :\n",
    "                total_dist += 500\n",
    "                dist_set.append( 500 )\n",
    "                pass\n",
    "        ws_2 = [i] + dist_set\n",
    "        ws_3 = pd.DataFrame([ws_2], columns = [\"iter\"] + feature )\n",
    "        ws_output = ws_output.append(ws_3)\n",
    "        \n",
    "        print(\"Iter : {} , {} > {} ?!\".format(i , Total_ws.ws.min() , total_dist))\n",
    "        if i % 25 == 0 :\n",
    "            clear_output(wait= True)\n",
    "        if ( (Total_ws.ws.min() > total_dist) and (i> 100)  ) or ( (i % 500 == 0) and (i>0) ) or (i == 1) :\n",
    "            try :\n",
    "                _ , _ , summary_str = sess.run([disc_loss , gen_loss , merged_summary] , \n",
    "                                                       feed_dict = {global_step : i ,\n",
    "                                                                    batch_size : BATCH_SIZE ,\n",
    "                                                                    phase : 1\n",
    "                                                                   })\n",
    "                writer.add_summary(summary_str , global_step = i)\n",
    "            except Exception as e :\n",
    "                pass \n",
    "\n",
    "            if save_model == True :\n",
    "                saver.save(sess, './savemodel/gan')\n",
    "            clear_output(wait= True)\n",
    "            imgs.append(Gan_Vis(g_plot, x_plot, total_dist , i , dloss ,gloss  ))\n",
    "            ws_output = ws_output[ws_output.iter > 0 ]\n",
    "            Columns_Dist(ws_output , total_dist , Total_ws , save_dir = './Gan_Pokemon_Int_bigan_2.png' , i = i  )\n",
    "            ## 쓸모없는 것 제거 \n",
    "            total_dist = []\n",
    "            for _ in range(10) :\n",
    "                sampling = sample_data(n= generate_n , data = transform).astype(float) # len(data)\n",
    "                dist = 0\n",
    "                for col in range(len(feature)) :\n",
    "                    try :\n",
    "                        dist += was_dist(sampling[:,col], g_plot[:,col])\n",
    "                    except Exception as e :\n",
    "                        ## Penalty\n",
    "                        dist += 100\n",
    "                        pass\n",
    "                total_dist.append(dist)\n",
    "            total_std = np.std(total_dist)\n",
    "            total_dist = np.mean(total_dist)\n",
    "            iter_ws = pd.DataFrame({\"iter\":[i] , \"ws\" :[total_dist] , \"wstd\" : [total_std]})\n",
    "            Total_ws    = Total_ws.append(iter_ws)\n",
    "            Total_ws = Total_ws[Total_ws.iter > 0 ]\n",
    "            Iter_History(Total_ws , i , save_dir = './Gan_Pokemon_bigan_2.png')\n",
    "    return imgs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\" :\n",
    "    imgs = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "img2gif = 'convert -delay 30 -loop 0 %s {}/simulation.gif'.format(img_dir)\n",
    "os.system(img2gif % ' '.join(imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (100 > 110 ) or (100 % 100 == 0) :\n",
    "    print(\"Pass?\")\n",
    "else :\n",
    "    print(\"Not Pass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
