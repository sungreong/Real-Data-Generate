{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import kurtosis\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "from tensorflow.contrib.distributions import percentile as tf_percent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./../data/credit44_sc.csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot(samples):\n",
    "#     fig = plt.figure(figsize=(4, 4))\n",
    "#     gs = gridspec.GridSpec(4, 4)\n",
    "#     gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "#     for i, sample in enumerate(samples):\n",
    "#         ax = plt.subplot(gs[i])\n",
    "#         plt.axis('off')\n",
    "#         ax.set_xticklabels([])\n",
    "#         ax.set_yticklabels([])\n",
    "#         ax.set_aspect('equal')\n",
    "#         plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "#     return fig\n",
    "\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "\n",
    "def log(x):\n",
    "    return tf.log(x + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_size = 1000\n",
    "X_dim = np.shape(data)[1]\n",
    "z_dim = 25\n",
    "h_dim = 36\n",
    "lr = 1e-4\n",
    "d_steps = 3\n",
    "information = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mb_size = 32\n",
    "X_dim = np.shape(data)[1]\n",
    "z_dim = 64\n",
    "h_dim = 128\n",
    "lr = 1e-3\n",
    "d_steps = 3\n",
    "\n",
    "나쁘지 않게 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, X_dim])\n",
    "z = tf.placeholder(tf.float32, shape=[None, z_dim])\n",
    "c = tf.placeholder(tf.float32, shape=[None, information])\n",
    "\n",
    "def sample_z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "\n",
    "\n",
    "def sample_c(m):\n",
    "    return np.random.multinomial(1, information*[0.2], size=m)\n",
    "\n",
    "\n",
    "def D(X, hsize=None  ,reuse=tf.AUTO_REUSE ):\n",
    "    with tf.variable_scope( \"Discriminator\" ,reuse=reuse  ):\n",
    "        h = tf.layers.dense(X,hsize[0], activation=tf.nn.relu , kernel_initializer= tf.contrib.layers.xavier_initializer() )\n",
    "        for i in np.arange( len(hsize)-1 )  : \n",
    "            h = tf.layers.dense(h,hsize[i+1],activation=tf.nn.relu ,\n",
    "                                name=\"total_w_\"+str(i), kernel_initializer= tf.contrib.layers.xavier_initializer() )\n",
    "        output  = tf.layers.dense(h,1 ,activation= tf.nn.sigmoid  ) # \n",
    "    return output  ## 1 이 맞을려나....\n",
    "\n",
    "\n",
    "def G( Z , c , hsize = None , reuse = tf.AUTO_REUSE) : \n",
    "    with tf.variable_scope(\"Generator\" , reuse = reuse) : \n",
    "        Z = tf.concat(axis=1, values=[Z, c])\n",
    "        h = tf.layers.dense(Z,hsize[0], activation=tf.nn.relu , kernel_initializer= tf.contrib.layers.xavier_initializer())\n",
    "        for i in np.arange( len(hsize)-1 )  : \n",
    "            h = tf.layers.dense(h,hsize[i+1],activation=tf.nn.relu ,\n",
    "                                name=\"relu_w_\"+str(i) , kernel_initializer= tf.contrib.layers.xavier_initializer()) #  , kernel_initializer = tf.variance_scaling_initializer()\n",
    "        G_prob = tf.layers.dense(h, X_dim , activation=tf.nn.sigmoid )\n",
    "    return G_prob\n",
    "\n",
    "\n",
    "def Q(X , hsize = None , reuse = tf.AUTO_REUSE ):\n",
    "    with tf.variable_scope(\"info\" , reuse = reuse) : \n",
    "        h = tf.layers.dense(X,hsize[0], activation=tf.nn.relu , kernel_initializer= tf.contrib.layers.xavier_initializer())\n",
    "        for i in np.arange( len(hsize)-1 )  : \n",
    "            h = tf.layers.dense(h,hsize[i+1],activation=tf.nn.relu ,\n",
    "                                name=\"relu_w_\"+str(i) , kernel_initializer= tf.contrib.layers.xavier_initializer()) #  , kernel_initializer = tf.variance_scaling_initializer()\n",
    "        Q_prob = tf.layers.dense(h, information , activation=tf.nn.softmax )\n",
    "    return Q_prob\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsize =[h_dim] * 2\n",
    "G_sample = G(z,c, hsize)\n",
    "\n",
    "D_real = D(X , hsize)\n",
    "D_fake = D(G_sample , hsize)\n",
    "\n",
    "# 기존 loss D가 nan\n",
    "D_loss = -tf.reduce_mean(tf.log(D_real + 1e-8) + tf.log(1 - D_fake + 1e-8))\n",
    "G_loss = -tf.reduce_mean(tf.log(D_fake + 1e-8))\n",
    "\n",
    "# softmax gan 착\n",
    "# D_target = 1./mb_size\n",
    "# G_target = 1./(mb_size*2)\n",
    "\n",
    "# Z = tf.reduce_sum(tf.exp(-D_real)) + tf.reduce_sum(tf.exp(-D_fake))\n",
    "\n",
    "# D_loss = tf.reduce_sum(D_target * D_real) + log(Z)\n",
    "# G_loss = tf.reduce_sum(G_target * D_real) + tf.reduce_sum(G_target * D_fake) + log(Z)\n",
    "\n",
    "\n",
    "\n",
    "Q_c_given_x = Q(G_sample , hsize )\n",
    "cross_ent = tf.reduce_mean(-tf.reduce_sum(tf.log(Q_c_given_x + 1e-8) * c, 1))\n",
    "Q_loss = cross_ent\n",
    "\n",
    "theta_G = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"Generator\")\n",
    "theta_D = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"Discriminator\")\n",
    "theta_Q = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"info\")\n",
    "\n",
    "D_solver = (tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            .minimize(D_loss, var_list=theta_D))\n",
    "\n",
    "G_solver = (tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            .minimize(G_loss, var_list=theta_G))\n",
    "Q_solver = tf.train.AdamOptimizer().minimize(Q_loss, var_list=theta_G + theta_Q)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연속형 variable : SCORE_ORG , count : 395 \n",
      "연속형 variable : SCORE_NK0200_000 , count : 445 \n",
      "연속형 variable : SCORE_RK0400_700 , count : 294 \n",
      "factor variable : CNT_CONTACT_POS, count : 8 \n",
      "factor variable : CNT_ENG, count : 7 \n",
      "factor variable : DAYS_CONTACT_POS, count : 11 \n",
      "factor variable : DAYS_CALL_PAYMENT, count : 11 \n",
      "factor variable : EWS_C_N_P27000100, count : 14 \n",
      "연속형 variable : EWS_A_K_D1M232000_OPR , count : 2384 \n",
      "factor variable : EWS_C_N_P42000200, count : 14 \n",
      "연속형 variable : EWS_C_N_P32003000 , count : 31 \n",
      "연속형 variable : AGE , count : 54 \n",
      "연속형 variable : CNT_Contact , count : 29 \n",
      "연속형 variable : EWS_A_K_D1M23200C_OPR , count : 2384 \n",
      "연속형 variable : EWS_C_K_D10310000_OPR , count : 52 \n",
      "연속형 variable : EWS_A_K_D90232200_OPR , count : 2490 \n",
      "연속형 variable : D_N_CA0000603 , count : 4894 \n",
      "연속형 variable : EWS_C_N_P32002600 , count : 22 \n",
      "연속형 variable : EWS_D_N_P43004000 , count : 399 \n",
      "factor variable : C_N_PS0001777, count : 11 \n",
      "factor variable : A_K_D10220000_OPR, count : 9 \n",
      "연속형 variable : SCORE_RK0400_000 , count : 424 \n",
      "연속형 variable : D_K_D10310000_OPR , count : 52 \n",
      "연속형 variable : D_K_L2Z000034 , count : 400 \n",
      "연속형 variable : D_K_L20283000 , count : 402 \n",
      "연속형 variable : EWS_D_K_L20283000 , count : 402 \n",
      "연속형 variable : EWS_D_N_P43004500 , count : 344 \n",
      "연속형 variable : D_K_L2Z000035 , count : 402 \n",
      "연속형 variable : EWS_A_K_D10231000_OPR , count : 2778 \n",
      "factor variable : DAYS_CONTACT, count : 7 \n",
      "연속형 variable : EWS_D_K_L2Z000035 , count : 402 \n",
      "연속형 variable : SC0000059 , count : 448 \n",
      "연속형 variable : D_N_L24003800 , count : 401 \n",
      "factor variable : MOB, count : 14 \n",
      "연속형 variable : SC0000063 , count : 445 \n",
      "연속형 variable : EWS_A_K_D10232000_OPR , count : 3270 \n",
      "연속형 variable : TF_N_CRT000021 , count : 5156 \n",
      "factor variable : EWS_C_K_D10220000_OPR, count : 9 \n",
      "연속형 variable : SC0000055 , count : 376 \n",
      "연속형 variable : SC0000049 , count : 308 \n",
      "factor variable : EWS_C_K_D10210D00_OPR, count : 11 \n",
      "연속형 variable : D_N_P21010500 , count : 170 \n",
      "연속형 variable : EWS_C_K_D10110000_OPR , count : 53 \n",
      "연속형 variable : EWS_A_N_L22002000 , count : 6369 \n",
      "factor variable : target, count : 2 \n",
      "binary target\n"
     ]
    }
   ],
   "source": [
    "fac_var = []\n",
    "num_var = []\n",
    "for i in list(data) : \n",
    "    if data[i].nunique() < 15 : \n",
    "        print(\"factor variable : {}, count : {} \".format(i, data[i].nunique()))\n",
    "        fac_var.append(i)\n",
    "        if data[i].nunique() == 2 :\n",
    "            print(\"binary\" , i)\n",
    "    else : \n",
    "        print(\"연속형 variable : {} , count : {} \".format(i, data[i].nunique()))\n",
    "        num_var.append(i)\n",
    "        \n",
    "data = data.loc[:,fac_var + num_var]\n",
    "\n",
    "col = list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def sample_data(data = None , n= len(data)) : \n",
    "    output = data[list(np.random.choice(len(data) , n))]\n",
    "    return output\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_plot = pd.DataFrame(sample_data(data = data.values , n=mb_size) , columns = col)\n",
    "data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot( row , ncol ) : \n",
    "    \n",
    "    fig , axes = plt.subplots(row , ncol , figsize = (26,13))\n",
    "    fig.subplots_adjust(hspace = 0.2 , wspace= 0.14 , top = 0.92 , left = 0.03 , bottom = 0.04 , right = 0.99)\n",
    "    \n",
    "    idx = np.random.randint(0, information)\n",
    "    c_noise = np.zeros([mb_size, information])\n",
    "    c_noise[range(mb_size), idx] = 1\n",
    "    \n",
    "    total = sess.run(G_sample, feed_dict={z: sample_z(mb_size, z_dim) , c: c_noise})\n",
    "    try : \n",
    "        total = total[~np.isnan(total).any(axis=1)]\n",
    "        total = scaler.inverse_transform(total)\n",
    "        g_plot = pd.DataFrame(total , columns = col )\n",
    "        g_plot[fac_var] = g_plot[fac_var].round(0)\n",
    "        \"\"\"\n",
    "        좀 더 쉬운 분포로 만들어서 학습시킨 후 다시 원래값으로 (factor 변수이기 때문에 가능하다 생각함.)\n",
    "        \"\"\"\n",
    "        #g_plot['DAYS_CONTACT_POS'].replace(to_replace=[1.0], value=7777.0, inplace=True)\n",
    "        #g_plot['DAYS_CONTACT_POS'].mask(g_plot['DAYS_CONTACT_POS'] > 0.5 , 7777.0, inplace=True)\n",
    "        #g_plot['DAYS_CALL_PAYMENT'].replace(to_replace=[1.0], value=7777.0, inplace=True)\n",
    "        STAT4 = pd.read_csv(\"./describe.csv\")\n",
    "        SKEW = pd.DataFrame(g_plot.skew() , columns = [\"skew\"]).T\n",
    "        KURT = pd.DataFrame(g_plot.kurt() , columns = [\"kurt\"]).T\n",
    "        STAT2 = pd.concat([SKEW , KURT])\n",
    "        STAT3 =pd.concat([g_plot.describe(), STAT2])\n",
    "        stat4 = pd.concat([STAT4 , STAT3])\n",
    "        # .rename( [STAT3.index.tolist()*2] )\n",
    "        stat4.index = STAT3.index.tolist()*2\n",
    "        stat4.to_csv(\"./real_fake_describe.csv\")\n",
    "        print(\"통계량 값 저장\")\n",
    "        \n",
    "        col2 = 0\n",
    "\n",
    "        for j in range(row) :\n",
    "            for k in range(ncol) :\n",
    "                try :\n",
    "                    label = col[col2]\n",
    "                    sample = g_plot.loc[: , label]\n",
    "                    sample.name = \"Gene\"\n",
    "                    real_0 = x_plot.loc[: , label]\n",
    "                    real_0.name =\"Real\"\n",
    "                    col2 += 1\n",
    "                    if label in fac_var : \n",
    "                        sns.distplot( sample , ax=axes[j , k], norm_hist =True , kde=False , hist_kws ={\"color\":\"r\" , \"label\" :\"Gene\", \"rwidth\":0.75})\n",
    "                        sns.distplot(real_0 , ax=axes[j , k],norm_hist =True, kde=False , hist_kws ={\"color\":\"g\" , \"label\" :\"Real\", \"rwidth\":0.75})\n",
    "                        axes[j , k].legend(fontsize = 10)\n",
    "                    elif label in num_var : \n",
    "                        sns.distplot(  sample , ax=axes[j , k] ,\n",
    "                                     kde_kws={\"color\": \"r\", \"lw\": 2, \"label\": \"Gene\" , \"shade\" : True} , hist =False , rug = False) #   \n",
    "                        sns.distplot(  real_0 , ax=axes[j , k] ,\n",
    "                                     kde_kws={\"color\": \"g\", \"lw\": 2, \"label\": \"Real\", \"shade\" : True } , hist =False , rug = False) # \n",
    "                        axes[j , k].legend(fontsize = 10 )\n",
    "                    axes[j , k].set_title( label , loc =\"left\" , fontsize= 10 )\n",
    "                except IndexError as e : \n",
    "                    axes[j , k].axis(\"off\")\n",
    "\n",
    "        plt.suptitle('infogan Iteration {} , Residual_loss : {} , D_loss : {} , G_loss : {}'.format(i,res , dloss,gloss) , fontsize= 30)\n",
    "        plt.savefig(\"./visualization_infogan.png\")\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "        fig , ax = plt.subplots(figsize = (26,13))\n",
    "        fig.subplots_adjust(top = 0.95 , left = 0.03 , bottom = 0.04 , right = 0.99)\n",
    "\n",
    "        ax.plot(output.iter , output.dloss , label =\"dloss\" , linestyle =\"-\" , marker =\".\" , linewidth = 4, markersize = 12)\n",
    "        ax.plot(output.iter , output.gloss , label =\"gloss\" , linestyle =\"-\" , marker =\".\" , linewidth = 4, markersize = 12)\n",
    "        ax.plot(output.iter , output.measure , label =\"measure\" , linestyle =\"-\" , marker =\".\" , linewidth = 4, markersize = 12)\n",
    "#         ax.plot(output.iter , output.dfac , label =\"D_FAC\" , linestyle =\"-\" , marker =\".\" , linewidth = 4, markersize = 12)\n",
    "#         ax.plot(output.iter , output.dnum , label =\"D_NUM\" , linestyle =\"-\" , marker =\".\" , linewidth = 4, markersize = 12)\n",
    "#         ax.plot(output.iter , output.dtotal , label =\"D_TOTAL\" , linestyle =\"-\" , marker =\".\" , linewidth = 4, markersize = 12)\n",
    "        ax.set_title(\"Iter : {} , measure : {}(~0) , Dloss : {} , Gloss : {}(>2) stat loss : {}\".format(iteration-1 ,  measure , dloss, gloss , 0), fontsize= 30)\n",
    "        ax.set_ylim(-5, 15)\n",
    "        box = ax.get_position()\n",
    "        ax.set_position([box.x0, box.y0 + box.height * 0.1, box.width, box.height * 0.9])\n",
    "        ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True, shadow=True, ncol=4 , fontsize= 20)\n",
    "        plt.savefig(\"./iteration_log_infogan.png\")\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e : \n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 480; D_loss: 0.8485; G_loss: 1.257\n",
      "통계량 값 저장\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "i = 0\n",
    "\n",
    "iteration , dloss ,  gloss , measure = 0 , 0, 0 , 0\n",
    "output = pd.DataFrame({\"iter\" : [iteration] , \"dloss\" : [dloss] , \"gloss\" : [gloss]})\n",
    "\n",
    "for i in range(1000000):\n",
    "    X_mb = sample_data(data = data , n=mb_size)\n",
    "    z_mb = sample_z(mb_size, z_dim)\n",
    "    c_noise = sample_c(mb_size)\n",
    "    \n",
    "\n",
    "#     samples = sess.run(G_sample,\n",
    "#                            feed_dict={z: z_mb , c: c_noise})\n",
    "    \n",
    "    _, dloss = sess.run([D_solver, D_loss],\n",
    "                              feed_dict={X: X_mb, z: z_mb, c: c_noise})\n",
    "    _, gloss = sess.run([G_solver, G_loss],feed_dict={z : z_mb, c: c_noise , X: X_mb})\n",
    "    \n",
    "    sess.run([Q_solver], feed_dict={z : z_mb, c: c_noise})\n",
    "    iteration +=1\n",
    "    \n",
    "#     if i < 50 : \n",
    "#         D_limit = 5 \n",
    "#         G_limit = 5 \n",
    "#     elif (i >= 50) & (i < 200) :\n",
    "#         D_limit = 3.5\n",
    "#         G_limit = 3.5\n",
    "#     else : \n",
    "#         D_limit = 1.5\n",
    "#         G_limit = 1.5\n",
    "#     dcount = 0 \n",
    "#     while dloss > D_limit  : \n",
    "#         dcount += 1\n",
    "#         _, dloss = sess.run([D_solver, D_loss], feed_dict={X: X_mb, z: z_mb, k: k_curr})\n",
    "#         if dcount > 2000 :\n",
    "#             print(\"{}th  Dicriminator Loss : {}\".format(i, dloss) )\n",
    "#             break\n",
    "#     gcount = 0 \n",
    "#     while gloss > G_limit :\n",
    "#         gcount += 1 \n",
    "#         _, gloss = sess.run([G_solver, G_loss], feed_dict={X: X_mb, z: z_mb})\n",
    "#         if gcount > 2000 :\n",
    "#             print(\" {}th  Generate Loss : {}\".format(i, gloss) )\n",
    "#             break\n",
    "    \n",
    "\n",
    "    \n",
    "    output1 = pd.DataFrame({\"iter\" : [iteration] , \"dloss\" : [dloss] , \"gloss\" : [gloss] , \"measure\" : [measure]})\n",
    "    output  = output.append(output1)\n",
    "    if i % 10 == 0:\n",
    "        \n",
    "        clear_output(wait= True)\n",
    "        dloss = round( np.float64(dloss),4)\n",
    "        gloss = round( np.float64(gloss),4)\n",
    "        print('Iter: {}; D_loss: {:.4}; G_loss: {:.4}'\n",
    "              .format(i, dloss, gloss))\n",
    "        res =\"infogan\"\n",
    "        show_plot( row = 7 , ncol=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
