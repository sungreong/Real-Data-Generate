{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import kurtosis\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "from tensorflow.contrib.distributions import percentile as tf_percent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./../data/credit44_sc.csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "factor  개수[13] \n",
      "\n",
      " ['CNT_CONTACT_POS', 'CNT_ENG', 'DAYS_CONTACT_POS', 'DAYS_CALL_PAYMENT', 'EWS_C_N_P27000100', 'EWS_C_N_P42000200', 'C_N_PS0001777', 'A_K_D10220000_OPR', 'DAYS_CONTACT', 'MOB', 'EWS_C_K_D10220000_OPR', 'EWS_C_K_D10210D00_OPR', 'target'] \n",
      "\n",
      "numeric 개수[32] \n",
      "\n",
      " ['SCORE_ORG', 'SCORE_NK0200_000', 'SCORE_RK0400_700', 'EWS_A_K_D1M232000_OPR', 'EWS_C_N_P32003000', 'AGE', 'CNT_Contact', 'EWS_A_K_D1M23200C_OPR', 'EWS_C_K_D10310000_OPR', 'EWS_A_K_D90232200_OPR', 'D_N_CA0000603', 'EWS_C_N_P32002600', 'EWS_D_N_P43004000', 'SCORE_RK0400_000', 'D_K_D10310000_OPR', 'D_K_L2Z000034', 'D_K_L20283000', 'EWS_D_K_L20283000', 'EWS_D_N_P43004500', 'D_K_L2Z000035', 'EWS_A_K_D10231000_OPR', 'EWS_D_K_L2Z000035', 'SC0000059', 'D_N_L24003800', 'SC0000063', 'EWS_A_K_D10232000_OPR', 'TF_N_CRT000021', 'SC0000055', 'SC0000049', 'D_N_P21010500', 'EWS_C_K_D10110000_OPR', 'EWS_A_N_L22002000'] \n"
     ]
    }
   ],
   "source": [
    "fac_var = []\n",
    "num_var = []\n",
    "for i in list(data) : \n",
    "    if data[i].nunique() < 15 : \n",
    "\n",
    "        fac_var.append(i)\n",
    "\n",
    "    else : \n",
    "\n",
    "        num_var.append(i)\n",
    "        \n",
    "data = data.loc[:,fac_var + num_var]\n",
    "\n",
    "col = list(data)\n",
    "print(\"factor  개수[{}] \\n\\n {} \\n\".format(len(fac_var) ,fac_var))\n",
    "print(\"numeric 개수[{}] \\n\\n {} \".format(len(num_var) , num_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(x):\n",
    "    return tf.log(x + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_size = 1000\n",
    "X_dim = np.shape(data)[1]\n",
    "z_dim = 64\n",
    "h_dim = 128\n",
    "lr = 1e-4\n",
    "d_steps = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mb_size = 32\n",
    "X_dim = np.shape(data)[1]\n",
    "z_dim = 64\n",
    "h_dim = 128\n",
    "lr = 1e-3\n",
    "d_steps = 3\n",
    "\n",
    "나쁘지 않게 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, X_dim])\n",
    "z = tf.placeholder(tf.float32, shape=[None, z_dim])\n",
    "\n",
    "\n",
    "scope = [\"GAN/map\",\n",
    "         \"GAN/fac\" , \n",
    "         \"GAN/num\" , \n",
    "         \"DIS/fac\" , \n",
    "         \"DIS/num\"]\n",
    "\n",
    "\n",
    "def sample_z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "\n",
    "def mapping_net( Z , hsize = None , reuse = tf.AUTO_REUSE) : \n",
    "    with tf.variable_scope( scope[0] , reuse = reuse) : \n",
    "        h = tf.layers.dense(Z,hsize[0], activation=tf.nn.elu )\n",
    "        h = tf.layers.dense(h,hsize[0], activation=tf.nn.leaky_relu)\n",
    "        #h = slim.dropout(h, keep_prob=0.9  , scope='dropout_map_1st')\n",
    "        for i in np.arange( len(hsize)-1 )  : \n",
    "            h = tf.layers.dense(h,hsize[i+1],activation=tf.nn.elu ,\n",
    "                                name=\"map_w_\"+str(i) ) #  , kernel_initializer = tf.variance_scaling_initializer()\n",
    "            h = tf.contrib.layers.batch_norm(h , decay=0.1 , renorm_clipping= (0 , 1) ,activation_fn = tf.nn.selu   )\n",
    "        return h\n",
    "\n",
    "def G_fac( Z , hsize = None , reuse = tf.AUTO_REUSE) : \n",
    "    with tf.variable_scope(scope[1] , reuse = reuse) : \n",
    "        h = tf.layers.dense(Z,hsize[0], activation=tf.nn.relu , kernel_initializer= tf.contrib.layers.xavier_initializer())\n",
    "        for i in np.arange( len(hsize)-1 )  : \n",
    "            h = tf.layers.dense(h,hsize[i+1],activation=tf.nn.relu ,\n",
    "                                name=\"Fac_w_\"+str(i) , kernel_initializer= tf.contrib.layers.xavier_initializer()) #  , kernel_initializer = tf.variance_scaling_initializer()\n",
    "        G_prob = tf.layers.dense(h, len(fac_var) , activation=tf.nn.sigmoid )\n",
    "    return G_prob\n",
    "    \n",
    "def G_num( Z , hsize = None , reuse = tf.AUTO_REUSE) : \n",
    "    with tf.variable_scope( scope[2] , reuse = reuse) : \n",
    "        h = tf.layers.dense(Z,hsize[0], activation=tf.nn.relu , kernel_initializer= tf.contrib.layers.xavier_initializer())\n",
    "        for i in np.arange( len(hsize)-1 )  : \n",
    "            h = tf.layers.dense(h,hsize[i+1],activation=tf.nn.selu ,\n",
    "                                name=\"Num_w_\"+str(i) , kernel_initializer= tf.contrib.layers.xavier_initializer()) #  , kernel_initializer = tf.variance_scaling_initializer()\n",
    "        G_prob = tf.layers.dense(h, len(num_var) )\n",
    "    return G_prob  \n",
    "    \n",
    "\n",
    "def D_fac(X, hsize=None  ,reuse=tf.AUTO_REUSE ):\n",
    "    with tf.variable_scope(scope[3] ,reuse=reuse  ):\n",
    "        h = tf.layers.dense(X,hsize[0], activation=tf.nn.relu , kernel_initializer= tf.contrib.layers.xavier_initializer() )\n",
    "        for i in np.arange( len(hsize)-1 )  : \n",
    "            h = tf.layers.dense(h,hsize[i+1],activation=tf.nn.relu ,\n",
    "                                name=\"D_fac_w_\"+str(i), kernel_initializer= tf.contrib.layers.xavier_initializer() )\n",
    "        out = tf.layers.dense(h,1 ,activation=tf.nn.sigmoid ) # \n",
    "    return out\n",
    "\n",
    "\n",
    "def D_num(X, hsize=None  ,reuse=tf.AUTO_REUSE ):\n",
    "    with tf.variable_scope(scope[4] ,reuse=reuse  ):\n",
    "        h = tf.layers.dense(X,hsize[0], activation=tf.nn.relu , kernel_initializer= tf.contrib.layers.xavier_initializer() )\n",
    "        for i in np.arange( len(hsize)-1 )  : \n",
    "            h = tf.layers.dense(h,hsize[i+1],activation=tf.nn.relu ,\n",
    "                                name=\"D_num_w_\"+str(i), kernel_initializer= tf.contrib.layers.xavier_initializer() )\n",
    "        out = tf.layers.dense(h,1 ,activation=tf.nn.sigmoid ) # \n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsize =[h_dim] * 1\n",
    "hsize2 = [z_dim] * 5\n",
    "W = mapping_net(z , hsize2)\n",
    "G_sample = G(W, hsize)\n",
    "\n",
    "D_real = D(X , hsize)\n",
    "D_fake = D(G_sample , hsize)\n",
    "\n",
    "\n",
    "Z = tf.reduce_sum(tf.exp(-D_real)) + tf.reduce_sum(tf.exp(-D_fake))\n",
    "\n",
    "D_loss = -tf.reduce_mean(log(D_real) + log(1 - D_fake))\n",
    "G_loss = 0.5 * tf.reduce_mean((log(D_fake) - log(1 - D_fake))**2)\n",
    "\n",
    "theta_M = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"Map\")\n",
    "theta_G = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"Generator\")\n",
    "theta_D = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,scope=\"Discriminator\")\n",
    "\n",
    "\n",
    "\n",
    "D_solver = (tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            .minimize(D_loss, var_list=theta_D))\n",
    "\n",
    "gen_vars = []\n",
    "gen_vars.append(theta_M)\n",
    "gen_vars.append(theta_G)\n",
    "G_solver = (tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            .minimize(G_loss, var_list=gen_vars ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Map/dense/kernel:0' shape=(64, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'Map/dense/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'Map/dense_1/kernel:0' shape=(64, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'Map/dense_1/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'Map/map_w_0/kernel:0' shape=(64, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'Map/map_w_0/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'Map/BatchNorm/beta:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'Map/BatchNorm/moving_mean:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'Map/BatchNorm/moving_variance:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'Map/map_w_1/kernel:0' shape=(64, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'Map/map_w_1/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'Map/BatchNorm_1/beta:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'Map/BatchNorm_1/moving_mean:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'Map/BatchNorm_1/moving_variance:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'Map/map_w_2/kernel:0' shape=(64, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'Map/map_w_2/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'Map/BatchNorm_2/beta:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'Map/BatchNorm_2/moving_mean:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'Map/BatchNorm_2/moving_variance:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'Map/map_w_3/kernel:0' shape=(64, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'Map/map_w_3/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'Map/BatchNorm_3/beta:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'Map/BatchNorm_3/moving_mean:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'Map/BatchNorm_3/moving_variance:0' shape=(64,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def sample_data(data = None , n= len(data)) : \n",
    "    output = data[list(np.random.choice(len(data) , n))]\n",
    "    return output\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_plot = pd.DataFrame(sample_data(data = data.values , n=mb_size) , columns = col)\n",
    "data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot( row , ncol ) : \n",
    "    \n",
    "    fig , axes = plt.subplots(row , ncol , figsize = (26,13))\n",
    "    fig.subplots_adjust(hspace = 0.2 , wspace= 0.14 , top = 0.92 , left = 0.03 , bottom = 0.04 , right = 0.99)\n",
    "    total = sess.run(G_sample, feed_dict={z: sample_z(mb_size, z_dim)})\n",
    "    try : \n",
    "        total = total[~np.isnan(total).any(axis=1)]\n",
    "        total = scaler.inverse_transform(total)\n",
    "        g_plot = pd.DataFrame(total , columns = col )\n",
    "        g_plot[fac_var] = g_plot[fac_var].round(0)\n",
    "        \"\"\"\n",
    "        좀 더 쉬운 분포로 만들어서 학습시킨 후 다시 원래값으로 (factor 변수이기 때문에 가능하다 생각함.)\n",
    "        \"\"\"\n",
    "        #g_plot['DAYS_CONTACT_POS'].replace(to_replace=[1.0], value=7777.0, inplace=True)\n",
    "        #g_plot['DAYS_CONTACT_POS'].mask(g_plot['DAYS_CONTACT_POS'] > 0.5 , 7777.0, inplace=True)\n",
    "        #g_plot['DAYS_CALL_PAYMENT'].replace(to_replace=[1.0], value=7777.0, inplace=True)\n",
    "        STAT4 = pd.read_csv(\"./describe.csv\")\n",
    "        SKEW = pd.DataFrame(g_plot.skew() , columns = [\"skew\"]).T\n",
    "        KURT = pd.DataFrame(g_plot.kurt() , columns = [\"kurt\"]).T\n",
    "        STAT2 = pd.concat([SKEW , KURT])\n",
    "        STAT3 =pd.concat([g_plot.describe(), STAT2])\n",
    "        stat4 = pd.concat([STAT4 , STAT3])\n",
    "        # .rename( [STAT3.index.tolist()*2] )\n",
    "        stat4.index = STAT3.index.tolist()*2\n",
    "        stat4.to_csv(\"./real_fake_describe.csv\")\n",
    "        print(\"통계량 값 저장\")\n",
    "        \n",
    "        col2 = 0\n",
    "\n",
    "        for j in range(row) :\n",
    "            for k in range(ncol) :\n",
    "                try :\n",
    "                    label = col[col2]\n",
    "                    sample = g_plot.loc[: , label]\n",
    "                    sample.name = \"Gene\"\n",
    "                    real_0 = x_plot.loc[: , label]\n",
    "                    real_0.name =\"Real\"\n",
    "                    col2 += 1\n",
    "                    if label in fac_var : \n",
    "                        sns.distplot( sample , ax=axes[j , k], norm_hist =True , kde=False , hist_kws ={\"color\":\"r\" , \"label\" :\"Gene\", \"rwidth\":0.75})\n",
    "                        sns.distplot(real_0 , ax=axes[j , k],norm_hist =True, kde=False , hist_kws ={\"color\":\"g\" , \"label\" :\"Real\", \"rwidth\":0.75})\n",
    "                        axes[j , k].legend(fontsize = 10)\n",
    "                    elif label in num_var : \n",
    "                        sns.distplot(  sample , ax=axes[j , k] ,\n",
    "                                     kde_kws={\"color\": \"r\", \"lw\": 2, \"label\": \"Gene\" , \"shade\" : True} , hist =False , rug = False) #   \n",
    "                        sns.distplot(  real_0 , ax=axes[j , k] ,\n",
    "                                     kde_kws={\"color\": \"g\", \"lw\": 2, \"label\": \"Real\", \"shade\" : True } , hist =False , rug = False) # \n",
    "                        axes[j , k].legend(fontsize = 10 )\n",
    "                    axes[j , k].set_title( label , loc =\"left\" , fontsize= 10 )\n",
    "                except IndexError as e : \n",
    "                    axes[j , k].axis(\"off\")\n",
    "\n",
    "        plt.suptitle('Boundary Iteration {} , Residual_loss : {} , D_loss : {} , G_loss : {}'.format(i,res , dloss,gloss) , fontsize= 30)\n",
    "        plt.savefig(\"./visualization_boundary_Map.png\")\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "        fig , ax = plt.subplots(figsize = (26,13))\n",
    "        fig.subplots_adjust(top = 0.95 , left = 0.03 , bottom = 0.04 , right = 0.99)\n",
    "\n",
    "        ax.plot(output.iter , output.dloss , label =\"dloss\" , linestyle =\"-\" , marker =\".\" , linewidth = 4, markersize = 12)\n",
    "        ax.plot(output.iter , output.gloss , label =\"gloss\" , linestyle =\"-\" , marker =\".\" , linewidth = 4, markersize = 12)\n",
    "#         ax.plot(output.iter , output.stat , label =\"stat\" , linestyle =\"-\" , marker =\".\" , linewidth = 4, markersize = 12)\n",
    "#         ax.plot(output.iter , output.dfac , label =\"D_FAC\" , linestyle =\"-\" , marker =\".\" , linewidth = 4, markersize = 12)\n",
    "#         ax.plot(output.iter , output.dnum , label =\"D_NUM\" , linestyle =\"-\" , marker =\".\" , linewidth = 4, markersize = 12)\n",
    "#         ax.plot(output.iter , output.dtotal , label =\"D_TOTAL\" , linestyle =\"-\" , marker =\".\" , linewidth = 4, markersize = 12)\n",
    "        ax.set_title(\"Iter : {} , Res : {}(>6) , Dloss : {} , Gloss : {}(>2) stat loss : {}\".format(iteration-1 ,  res , dloss, gloss , 0), fontsize= 30)\n",
    "        ax.set_ylim(-5, 15)\n",
    "        box = ax.get_position()\n",
    "        ax.set_position([box.x0, box.y0 + box.height * 0.1, box.width, box.height * 0.9])\n",
    "        ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True, shadow=True, ncol=4 , fontsize= 20)\n",
    "        plt.savefig(\"./iteration_plot_bondary_Map.png\")\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e : \n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 4150; D_loss: 0.9894; G_loss: 0.2621\n",
      "통계량 값 저장\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "i = 0\n",
    "\n",
    "iteration , dloss ,  gloss = 0 , 0, 0\n",
    "output = pd.DataFrame({\"iter\" : [iteration] , \"dloss\" : [dloss] , \"gloss\" : [gloss] })\n",
    "\n",
    "for i in range(1000000):\n",
    "    X_mb = sample_data(data = data , n=mb_size)\n",
    "    z_mb = sample_z(mb_size, z_dim)\n",
    "\n",
    "    _, dloss = sess.run(\n",
    "        [D_solver, D_loss], feed_dict={X: X_mb, z: z_mb})\n",
    "\n",
    "    _, gloss = sess.run(\n",
    "        [G_solver, G_loss], feed_dict={X: X_mb, z: z_mb})\n",
    "    iteration +=1\n",
    "    if i < 100 : \n",
    "        D_limit = 1.5\n",
    "        G_limit = 1.5\n",
    "    elif (i >= 100) & (i < 200) :\n",
    "        D_limit = 1\n",
    "        G_limit = 1\n",
    "    else : \n",
    "        D_limit = 1\n",
    "        G_limit = 1\n",
    "    dcount = 0 \n",
    "    while dloss > D_limit  : \n",
    "        dcount += 1\n",
    "        _, dloss = sess.run([D_solver, D_loss], feed_dict={X: X_mb, z: z_mb})\n",
    "        if dcount > 2000 :\n",
    "            print(\"{}th  Dicriminator Loss : {}\".format(i, dloss) )\n",
    "            break\n",
    "    gcount = 0 \n",
    "    while gloss > G_limit :\n",
    "        gcount += 1 \n",
    "        _, gloss = sess.run([G_solver, G_loss], feed_dict={X: X_mb, z: z_mb})\n",
    "        if gcount > 2000 :\n",
    "            print(\" {}th  Generate Loss : {}\".format(i, gloss) )\n",
    "            break\n",
    "        \n",
    "    output1 = pd.DataFrame({\"iter\" : [iteration] , \"dloss\" : [dloss] , \"gloss\" : [gloss] })\n",
    "    output  = output.append(output1)\n",
    "    if i % 10 == 0:\n",
    "        clear_output(wait= True)\n",
    "        dloss = round( np.float64(dloss),4)\n",
    "        gloss = round( np.float64(gloss),4)\n",
    "        print('Iter: {}; D_loss: {:.4}; G_loss: {:.4}'\n",
    "              .format(i, dloss, gloss))\n",
    "        res =\"Gloss Optimal = 0.5\"\n",
    "        show_plot( row = 7 , ncol=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
